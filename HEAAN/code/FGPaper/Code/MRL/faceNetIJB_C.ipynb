{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/byalavar/miniconda3/envs/train/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# def unzip_file(zip_file_path, extract_to):\n",
    "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(extract_to)\n",
    "\n",
    "# # Example usage\n",
    "# zip_file_path = '/home/csgrad/byalavar/FHE/HEAAN/FG_Round_2/face-samples.zip'\n",
    "# extract_to_directory = '/home/csgrad/byalavar/FHE/HEAAN/FG_Round_2/face-samples'\n",
    "\n",
    "# unzip_file(zip_file_path, extract_to_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "\n",
    "# # Function to extract information from the image file name and grandparent folder names\n",
    "# def extract_info(file_path):\n",
    "#     parts = file_path.split(os.sep)\n",
    "#     filename = parts[-1].split('.')[0]  # Get the filename without extension\n",
    "#     #gender = parts[-2]\n",
    "#     ethnicity = parts[-3].split('_')[0]\n",
    "#     gender = parts[-3].split('_')[1]\n",
    "#     print(filename,gender,ethnicity)\n",
    "#     return filename, gender, ethnicity,file_path\n",
    "\n",
    "# # Function to process a directory and create a CSV file\n",
    "# def process_directory(directory_path, output_csv):\n",
    "#     with open(output_csv, 'w', newline='') as csvfile:\n",
    "#         fieldnames = ['Name', 'Gender', 'Ethnicity','Path']\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#         writer.writeheader()\n",
    "\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in files:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 name, gender, ethnicity,path = extract_info(file_path)\n",
    "#                 writer.writerow({'Name': name, 'Gender': gender, 'Ethnicity': ethnicity,'Path':path})\n",
    "\n",
    "# # Example usage\n",
    "# dataset_directory = '/home/csgrad/byalavar/FHE/HEAAN/FG_Round_2/face-samples'\n",
    "# output_csv_file = '/home/csgrad/byalavar/FHE/HEAAN/FG_Round_2/bfw.csv'\n",
    "\n",
    "# process_directory(dataset_directory, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://hydranets-data.s3.eu-west-3.amazonaws.com/UTKFace.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip UTKFace.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.gender_mapping = {'males': 0, 'females': 1}\n",
    "        self.ethnicity_mapping = {'indian': 0, 'white': 1, 'asian': 2, 'black': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = \"/home/csgrad/byalavar/FHE/HEAAN/FG_Round_2/IJB_C/data/\" + str(self.data.iloc[idx]['TEMPLATE_ID']) + \".jpg\"\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        #print(img_path)\n",
    "        gender = self.data.iloc[idx]['GENDER']\n",
    "        ethnicity = self.data.iloc[idx]['SKIN_COLOUR']\n",
    "        age = self.data.iloc[idx]['AGE']\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = {\n",
    "            'age': age,\n",
    "            'gender': gender,\n",
    "            'ethnicity': ethnicity\n",
    "        \n",
    "        }\n",
    "        return image,label\n",
    "\n",
    "# Example usage:\n",
    "csv_file_path = 'IJB_C/labels.csv'  # Replace with the actual path to your CSV file\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor(),          # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "\n",
    "dataset = CustomDataset(csv_file_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24155, 64152, 11879])\n"
     ]
    }
   ],
   "source": [
    "temp = pd.read_csv('IJB_C/labels.csv')\n",
    "#plt.hist(temp['Ag'], bins=2, color='blue', edgecolor='black')\n",
    "label_counts = torch.bincount(torch.tensor(temp['AGE']))\n",
    "print(label_counts)\n",
    "total_samples = label_counts.sum().float()\n",
    "class_weights = total_samples / (label_counts + 1e-7)\n",
    "ageWeights =class_weights/ class_weights.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2933, 0.1104, 0.5963])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ageWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('IJB_C/labels.csv')\n",
    "#plt.hist(temp['Ag'], bins=2, color='blue', edgecolor='black')\n",
    "label_counts = torch.bincount(torch.tensor(temp['GENDER']))\n",
    "total_samples = label_counts.sum().float()\n",
    "class_weights = total_samples / (label_counts + 1e-7)\n",
    "genderWeights =class_weights/ class_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('IJB_C/labels.csv')\n",
    "#plt.hist(temp['Ag'], bins=2, color='blue', edgecolor='black')\n",
    "label_counts = torch.bincount(torch.tensor(temp['SKIN_COLOUR']))\n",
    "total_samples = label_counts.sum().float()\n",
    "class_weights = total_samples / (label_counts + 1e-7)\n",
    "ethnWeights =class_weights/ class_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2933, 0.1104, 0.5963])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ageWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders for training and testing sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': tensor([1, 1, 1, 1, 1, 2, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'gender': tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0]), 'ethnicity': tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "tempIter = iter(train_dataloader)\n",
    "images,labels = next(tempIter)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "def generate_C(C_range, m):\n",
    "    \"\"\" Randomly generates m coefficients for the PolyProtect mapping.\n",
    "\n",
    "    **Inputs:**\n",
    "\n",
    "    C_range : integer\n",
    "        The absolute min/max values of the coefficients range.\n",
    "\n",
    "    m : int\n",
    "        The number of coefficients to generate.\n",
    "\n",
    "    **Outputs:**\n",
    "\n",
    "    C : 1D numpy array of integers\n",
    "        Array of m coefficients.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate coefficient range (excluding 0):\n",
    "    neg_range = numpy.arange(-1 * C_range, 0)\n",
    "    pos_range = numpy.arange(1, C_range + 1)\n",
    "    whole_range = numpy.concatenate([neg_range, pos_range])\n",
    "\n",
    "    # Randomly generate m unique coefficients:\n",
    "    C = numpy.random.permutation(whole_range)[0 : m] # randomly permute the whole range and pick the first few m values\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def generate_E(m):\n",
    "    \"\"\" Randomly generates m exponents for the PolyProtect mapping.\n",
    "\n",
    "    **Inputs:**\n",
    "\n",
    "    m : int\n",
    "        The number of exponents to generate.\n",
    "\n",
    "    **Outputs:**\n",
    "\n",
    "    E : 1D numpy array of integers\n",
    "        Array of m exponents.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly generate m unique exponents:\n",
    "    E = numpy.random.permutation(range(1, m + 1))[0 : m] # permute the integers in the range [1, m]\n",
    "\n",
    "    return E\n",
    "\n",
    "\n",
    "def polyprotect(overlap, V):\n",
    "    \"\"\" Maps an embedding to a PolyProtected template.\n",
    "\n",
    "    **Inputs:**\n",
    "\n",
    "    overlap : int\n",
    "        The amount of overlap between sets of embedding elements used to generate each PolyProtected element (0, 1, 2, 3, or 4).\n",
    "\n",
    "    V : torch.Tensor\n",
    "        The embedding as a PyTorch tensor.\n",
    "\n",
    "    **Outputs:**\n",
    "\n",
    "    P : torch.Tensor\n",
    "        The PolyProtected template as a PyTorch tensor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    C = torch.tensor([-42, -35, 31, 4], dtype=torch.float32, device=V.device)\n",
    "    E = torch.tensor([3, 2, 1, 4], dtype=torch.float32, device=V.device)\n",
    "\n",
    "    if C.shape[0] != E.shape[0]:\n",
    "        print(\"Number of coefficients and exponents must be the same.\")\n",
    "        return None\n",
    "    #print(\"here\")\n",
    "    m = C.shape[0] # number of embedding elements used to generate each PolyProtected element\n",
    "    step_size = m - overlap\n",
    "    decimal_remainder, integer = math.modf((V.shape[0] - m) / step_size)\n",
    "    if decimal_remainder > 0:\n",
    "        padding = math.ceil((1 - decimal_remainder) * step_size)\n",
    "    else:\n",
    "        padding = 0\n",
    "   # print(\"here1\")\n",
    "    # Pad V by \"padding\" zeros at the end\n",
    "    V = torch.cat((V, torch.zeros(padding, device=V.device)), dim=0)\n",
    "\n",
    "    starting_indices = torch.arange(0, V.shape[0] - m + 1, step_size)\n",
    "    #print(\"here2\")\n",
    "    P = torch.zeros(len(starting_indices), device=V.device)\n",
    "    \n",
    "    for storage_ind, ind in enumerate(starting_indices):\n",
    "        \n",
    "        final_ind = ind + m\n",
    "        crnt_word = V[ind:final_ind]\n",
    "        P[storage_ind] = torch.sum(C * (crnt_word ** E))\n",
    "    #print(\"here3\")\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class faceAnalytics(nn.Module):\n",
    "\n",
    "    def __init__(self,var):\n",
    "        super().__init__()\n",
    "        self.var = var\n",
    "        self.layer1=nn.Linear(var,256)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.layer2=nn.Linear(256,128)\n",
    "        #self.layer3=nn.Linear(1024,512)\n",
    "        self.layer4=nn.Linear(128,64)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        self.genderOut=nn.Linear(64,2)\n",
    "        self.ageOut=nn.Linear(64,3)\n",
    "        self.ethnicityOut = nn.Linear(64,2)\n",
    "\n",
    "        # self.maxVal = 0\n",
    "        # self.min=0\n",
    "        \n",
    "    \n",
    "    def writeResult(self,result):\n",
    "       output_directory=\"\"\n",
    "       file_name = \"resultAge.txt\"\n",
    "\n",
    "       with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in result:\n",
    "            file.write(f\"{value}\\n\")\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        #print(\"Input\",x[0])\n",
    "        x=self.layer1(x)\n",
    "        #print(x[0])\n",
    "        #x=nn.functional.relu(x)\n",
    "        x=self.dropout1(x)\n",
    "        #x=nn.functional.relu(x)\n",
    "        x=self.layer2(x)\n",
    "        x=nn.functional.relu(x)\n",
    "        #self.writeResult(x[0])\n",
    "       # x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        #print(torch.max(torch.abs(x)))\n",
    "        x=nn.functional.relu(x)\n",
    "        \n",
    "        x=self.dropout2(x)\n",
    "        gender=self.genderOut(x)\n",
    "        \n",
    "        age=self.ageOut(x)\n",
    "\n",
    "        ethn = self.ethnicityOut(x)\n",
    "        #self.writeResult(age[0])\n",
    "        #print(gender)\n",
    "        return gender,age,ethn\n",
    "    \n",
    "        \n",
    "    \n",
    "    #tempPT,genderLabelTensor,ageLabelTensor,ethnLabelTensor\n",
    "    def trainModel(self,tempPT,genderLabelTensor,ageLabelTensor,ethnLabelTensor,testLoader,device,episodes,resnet,genderWeights,ageWeights,ethnWeights):\n",
    "        \n",
    "        #maxVal = 0\n",
    "        learningRate=0.01\n",
    "\n",
    "\n",
    "\n",
    "        genderWeights=genderWeights.to(device)\n",
    "        ageWeights=ageWeights.to(device)\n",
    "        ethnWeights=ethnWeights.to(device)\n",
    "\n",
    "        gender_loss = nn.CrossEntropyLoss() \n",
    "        age_loss = nn.CrossEntropyLoss()\n",
    "        ethn_loss = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learningRate,weight_decay=0.0000001)\n",
    "        #ageLabelTensor = torch.where(ageLabelTensor == 6, torch.tensor(5), ageLabelTensor)\n",
    "\n",
    "        #processedTensor = torch.zeros((len(train_dataset),128)).to(device)\n",
    "        #print(processedTensor.shape)\n",
    " #processedTensor = torch.zeros((len(train_dataloader.dataset),128))\n",
    "        # ageLabelTensor = torch.zeros((len(train_dataset))).to(device)\n",
    "        # genderLabelTensor = torch.zeros((len(train_dataset))).to(device)\n",
    "        # ethnLabelTensor = torch.zeros((len(train_dataset))).to(device)\n",
    "        for e in range(0,episodes):\n",
    "         total_training_loss =0\n",
    "         tempAccGender=0\n",
    "         tempAccAge=0\n",
    "         tempAccEthn=0\n",
    "         count=0\n",
    "         totalGenderLoss=0\n",
    "         self.train()\n",
    "         \n",
    "      \n",
    "\n",
    "        #     inputs = data[\"image\"].to(device=device)\n",
    "        #     #inputs = inputs.type(torch.double)\n",
    "        #     age_label = data[\"age\"].to(device=device)\n",
    "        #     gender_label = data[\"gender\"].to(device=device)\n",
    "        #     ethn_label = data[\"ethn\"].to(device=device)\n",
    "         batchSize = 512\n",
    "\n",
    "         ageLabelTensor = ageLabelTensor.type(torch.LongTensor)\n",
    "         genderLabelTensor = genderLabelTensor.type(torch.LongTensor)\n",
    "         ethnLabelTensor = ethnLabelTensor.type(torch.LongTensor)    \n",
    "\n",
    "         while(count<tempPT.shape[0]):\n",
    "            if(count+batchSize<=tempPT.shape[0]):\n",
    "             inputs = tempPT[count:count+batchSize].to(device=device)\n",
    "             age_label = ageLabelTensor[count:count+batchSize].to(device=device)\n",
    "             gender_label = genderLabelTensor[count:count+batchSize].to(device=device)\n",
    "             ethn_label = ethnLabelTensor[count:count+batchSize].to(device=device)\n",
    "            else:\n",
    "               inputs = tempPT[count:].to(device=device)\n",
    "               age_label = ageLabelTensor[count:].to(device=device)\n",
    "               gender_label = genderLabelTensor[count:].to(device=device)\n",
    "               ethn_label = ethnLabelTensor[count:].to(device=device)\n",
    "            \n",
    "            \n",
    "    \n",
    "            gender,age,ethn = self(inputs)\n",
    "            # age=torch.squeeze(age)\n",
    "            # age=age.type(torch.float32)\n",
    "            #print(age.shape,age_label.shape)\n",
    "            #print(gender.shape,gender_label.shape)\n",
    "            # predictedGender = torch.argmax(gender,dim=1)\n",
    "            # predictedGender = predictedGender.type(torch.float32)\n",
    "            #print(gender)\n",
    "            loss =    gender_loss(gender,gender_label) + 5*age_loss(age,age_label) + 4*ethn_loss(ethn,ethn_label)\n",
    "            #totalGenderLoss = totalGenderLoss + loss.item()\n",
    "            loss.backward()\n",
    "            #print(\"Loss:\",loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_training_loss = total_training_loss+loss.item()*256\n",
    "\n",
    "            predictedGender = torch.argmax(gender,dim=1)\n",
    "            \n",
    "            \n",
    "            for j in range(0,predictedGender.shape[0]):\n",
    "                count=count+1\n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedGender[j].item()==gender_label[j]):\n",
    "                    tempAccGender=tempAccGender+1\n",
    "            \n",
    "            predictedAge = torch.argmax(age,dim=1)\n",
    "            \n",
    "            for j in range(0,predictedAge.shape[0]):\n",
    "               \n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedAge[j].item()==age_label[j]):\n",
    "                    tempAccAge=tempAccAge+1\n",
    "            \n",
    "            predictedEthn = torch.argmax(ethn,dim=1)\n",
    "            \n",
    "            for j in range(0,predictedEthn.shape[0]):\n",
    "               \n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedEthn[j].item()==ethn_label[j]):\n",
    "                    tempAccEthn=tempAccEthn+1\n",
    "         \n",
    "            genderAccuracy =  tempAccGender/count\n",
    "            ageAccuracy =  tempAccAge/count\n",
    "            ethnAccuracy =  tempAccEthn/count\n",
    "            #print(count)\n",
    "            #break\n",
    "         #break;\n",
    "         print(\"Training Gender Accuracy:\", genderAccuracy,\"Age:\", ageAccuracy,\"Ethn:\",ethnAccuracy)\n",
    "        #  print(\"\\n\")\n",
    "        #  print(\"Training Ethn Accuracy:\", ethnAccuracy,\"\\n\")\n",
    "        #  print(\"total training loss:\",total_training_loss/16595,\"\\n\")\n",
    "         #print(\"\\n\")\n",
    "         #print(\"max observed value: \", maxVal)\n",
    "        #  if(e%2==0):\n",
    "        #      self.test(testLoader,device)\n",
    "\n",
    "        # print(processedTensor.shape,processedTensor[0])\n",
    "        # print(ageLabelTensor.shape,ageLabelTensor[0])\n",
    "        # print(genderLabelTensor.shape,genderLabelTensor[0])\n",
    "        # print(ethnLabelTensor.shape,ethnLabelTensor[0])\n",
    "        #return processedTensor,genderLabelTensor,ageLabelTensor,ethnLabelTensor\n",
    "\n",
    "\n",
    "    def are_values_in_same_class(self,value1, value2):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(0, 2),(2,5),(5,13),(13,20),(20,40),(40,60),(60,80),(80,120)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        for class_range in class_ranges:\n",
    "            if class_range[0] <= value1 <= class_range[1] and class_range[0] <= value2 <= class_range[1]:\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def test(self,testLoader,resnet,device):\n",
    "\n",
    "        predGender = []\n",
    "        predAge = []\n",
    "        predEthn = []\n",
    "        labGender = []\n",
    "        labAge = []\n",
    "        labEthn = []\n",
    "        tempAccGender=0\n",
    "        tempAccAge=0\n",
    "        tempAccEthn=0\n",
    "        count=0\n",
    "\n",
    "        self.eval()\n",
    "         \n",
    "        for i,data in enumerate(testLoader):\n",
    "\n",
    "            inputs = data[0].to(device=device)\n",
    "            #inputs = inputs.type(torch.double)\n",
    "            age_label = data[1][\"age\"].to(device=device)\n",
    "            gender_label = data[1][\"gender\"].to(device=device)\n",
    "            ethn_label = data[1][\"ethnicity\"].to(device=device)\n",
    "            \n",
    "            \n",
    "            inputs=resnet(inputs)\n",
    "            #print(inputs.shape)\n",
    "            inputUpdated = torch.zeros((inputs.shape[0], 128),device=device)\n",
    "            for t in range(0,inputs.shape[0]):\n",
    "             #print(inputs[t].shape,\"inputs[t]\")   \n",
    "             inputUpdated[t] = polyprotect(0,inputs[t])\n",
    "            #print(\"here4\")\n",
    "            #print(inputUpdated[0])\n",
    "            gender,age,ethn = self(inputUpdated)\n",
    "            \n",
    "            predictedGender = torch.argmax(gender,dim=1)\n",
    "            for j in range(0,predictedGender.shape[0]):\n",
    "                count=count+1\n",
    "                predGender.append(predictedGender[j].item())\n",
    "                labGender.append(gender_label[j].item())\n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedGender[j].item()==gender_label[j]):\n",
    "                    tempAccGender=tempAccGender+1\n",
    "            \n",
    "            predictedAge = torch.argmax(age,dim=1)\n",
    "            for j in range(0,predictedAge.shape[0]):\n",
    "                predAge.append(predictedAge[j].item())\n",
    "                labAge.append(age_label[j].item())\n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedAge[j].item()==age_label[j]):\n",
    "                    tempAccAge=tempAccAge+1\n",
    "            \n",
    "            predictedEthn = torch.argmax(ethn,dim=1)\n",
    "            for j in range(0,predictedEthn.shape[0]):\n",
    "                predEthn.append(predictedEthn[j].item())\n",
    "                labEthn.append(ethn_label[j].item())\n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedEthn[j].item()==ethn_label[j]):\n",
    "                    tempAccEthn=tempAccEthn+1\n",
    "         \n",
    "        genderAccuracy =  tempAccGender/count\n",
    "        ageAccuracy =  tempAccAge/count\n",
    "        ethnAccuracy =  tempAccEthn/count\n",
    "        \n",
    "        print(\"Test Gender Accuracy:\", genderAccuracy,\"\\n\")\n",
    "        print(\"Test Age Accuracy:\", ageAccuracy,\"\\n\")\n",
    "        print(\"Test Ethn Accuracy:\", ethnAccuracy,\"\\n\")\n",
    "   \n",
    "         #print(\"\\n\")\n",
    "        return predGender,predAge,predEthn,labGender,labAge,labEthn\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "# count=0\n",
    "# for param in resnet.parameters():\n",
    "#     count=count+1\n",
    "#     if(count>350):\n",
    "#      param.requires_grad = True\n",
    "#     else:\n",
    "#      param.requires_grad = False\n",
    "\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gender Accuracy: 0.675188401457304 Age: 0.6418750311922943 Ethn: 0.8051479762439487\n",
      "Training Gender Accuracy: 0.7103483555422468 Age: 0.6623746069770924 Ethn: 0.8109746968108998\n",
      "Training Gender Accuracy: 0.7178345061635973 Age: 0.6652318211309078 Ethn: 0.8103009432549783\n",
      "Training Gender Accuracy: 0.7194315516294855 Age: 0.6672406048809703 Ethn: 0.8109247891400908\n",
      "Training Gender Accuracy: 0.7175350601387434 Age: 0.6677147277536557 Ethn: 0.8117732195438438\n"
     ]
    }
   ],
   "source": [
    "var = 128\n",
    "model=faceAnalytics(var)\n",
    "#model=torch.load(\"modelUsing0.pt\")\n",
    "model.to(device)\n",
    "processedTensor = torch.load(\"inputTempIJB.pt\")\n",
    "processedTensor = processedTensor.to(device)\n",
    "# #print(processedTensor.shape)\n",
    "genderLabelTensor= torch.load(\"genderTempIJB.pt\")\n",
    "genderLabelTensor = genderLabelTensor.to(device)\n",
    "ageLabelTensor = torch.load(\"ageTempIJB.pt\")\n",
    "ageLabelTensor = ageLabelTensor.to(device)\n",
    "ethnLabelTensor = torch.load(\"ethnTempIJB.pt\")\n",
    "ethnLabelTensor = ethnLabelTensor.to(device)\n",
    "model.trainModel(processedTensor,genderLabelTensor,ageLabelTensor,ethnLabelTensor,test_dataloader,device,5,resnet,torch.zeros((5)),torch.zeros((5)),torch.zeros((5)))\n",
    "\n",
    "#processedTensor,genderLabelTensor,ageLabelTensor,ethnLabelTensor = model.trainModel(train_dataloader,val_dataloader,device,1,resnet)\n",
    "# #torch.save(model,\"faTest14.pt\")\n",
    "# torch.save(processedTensor,\"inputUTK.pt\")\n",
    "# torch.save(genderLabelTensor,\"genderLabelUTK.pt\")\n",
    "# torch.save(ageLabelTensor,\"ageLabelUTK.pt\")\n",
    "# torch.save(ethnLabelTensor,\"ethnLabelUTK.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3638, 0.6362])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genderWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Gender Accuracy: 0.7221279568819243 \n",
      "\n",
      "Test Age Accuracy: 0.6808563728915061 \n",
      "\n",
      "Test Ethn Accuracy: 0.8198422996307017 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model=faceAnalytics()\n",
    "#model=torch.load(\"fa3CS.pt\")\n",
    "# model.to(device)\n",
    "#print(model)\n",
    "#(0.9170416197975253, 0.7315804274465691, 0.8307086614173228)\n",
    "predGender,predAge,predEthn,labGender,labAge,labEthn = model.test(test_dataloader,resnet,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81     12776\n",
      "           1       0.70      0.52      0.60      7262\n",
      "\n",
      "    accuracy                           0.74     20038\n",
      "   macro avg       0.73      0.70      0.70     20038\n",
      "weighted avg       0.74      0.74      0.73     20038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(labGender, predGender)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Extract code\n",
    "\n",
    "\n",
    "\n",
    "class faceAnalytics(nn.Module):\n",
    "\n",
    "    def __init__(self,var):\n",
    "        super().__init__()\n",
    "        self.var = var\n",
    "        self.layer1=nn.Linear(var,256)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.layer2=nn.Linear(256,128)\n",
    "        #self.layer3=nn.Linear(1024,512)\n",
    "        self.layer4=nn.Linear(128,64)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        self.genderOut=nn.Linear(64,2)\n",
    "        self.ageOut=nn.Linear(64,6)\n",
    "        self.ethnicityOut = nn.Linear(64,4)\n",
    "\n",
    "        # self.maxVal = 0\n",
    "        # self.min=0\n",
    "        \n",
    "    \n",
    "    def writeResult(self,result):\n",
    "       output_directory=\"\"\n",
    "       file_name = \"resultAge.txt\"\n",
    "\n",
    "       with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in result:\n",
    "            file.write(f\"{value}\\n\")\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        #print(\"Input\",x[0])\n",
    "        x=self.layer1(x)\n",
    "        #print(x[0])\n",
    "        #x=nn.functional.relu(x)\n",
    "        #x=self.dropout1(x)\n",
    "        #x=nn.functional.relu(x)\n",
    "        x=self.layer2(x)\n",
    "        \n",
    "        #self.writeResult(x[0])\n",
    "       # x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        #print(torch.max(torch.abs(x)))\n",
    "        x=nn.functional.relu(x)\n",
    "        \n",
    "        x=self.dropout2(x)\n",
    "        gender=self.genderOut(x)\n",
    "        \n",
    "        age=self.ageOut(x)\n",
    "\n",
    "        ethn = self.ethnicityOut(x)\n",
    "        #self.writeResult(age[0])\n",
    "        #print(gender)\n",
    "        return gender,age,ethn\n",
    "    \n",
    "        \n",
    "    \n",
    "    #tempPT,genderLabelTensor,ageLabelTensor,ethnLabelTensor\n",
    "    def trainModel(self,trainLoader,testLoader,device,episodes,resnet):\n",
    "        \n",
    "        #maxVal = 0\n",
    "        learningRate=0.005\n",
    "        gender_loss = nn.CrossEntropyLoss() \n",
    "        age_loss = nn.CrossEntropyLoss()\n",
    "        ethn_loss = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learningRate,weight_decay=0.003)\n",
    "        processedTensor = torch.zeros((len(trainLoader.dataset),self.var)).to(device)\n",
    "        print(processedTensor.shape)\n",
    " #processedTensor = torch.zeros((len(train_dataloader.dataset),128))\n",
    "        ageLabelTensor = torch.zeros((len(trainLoader.dataset))).to(device)\n",
    "        genderLabelTensor = torch.zeros((len(trainLoader.dataset))).to(device)\n",
    "        ethnLabelTensor = torch.zeros((len(trainLoader.dataset))).to(device)\n",
    "        for e in range(0,episodes):\n",
    "         total_training_loss =0\n",
    "         tempAccGender=0\n",
    "         tempAccAge=0\n",
    "         tempAccEthn=0\n",
    "         count=0\n",
    "         totalGenderLoss=0\n",
    "         self.train()\n",
    "         \n",
    "         for i,data in enumerate(trainLoader):\n",
    "\n",
    "            inputs = data[0].to(device=device)\n",
    "            #inputs = inputs.type(torch.double)\n",
    "            age_label = data[1][\"age\"].to(device=device)\n",
    "            gender_label = data[1][\"gender\"].to(device=device)\n",
    "            ethn_label = data[1][\"ethnicity\"].to(device=device)\n",
    "         #batchSize = 256\n",
    "\n",
    "        #  ageLabelTensor = ageLabelTensor.type(torch.LongTensor)\n",
    "        #  genderLabelTensor = genderLabelTensor.type(torch.LongTensor)\n",
    "        #  ethnLabelTensor = ethnLabelTensor.type(torch.LongTensor)    \n",
    "\n",
    "        #  while(count<tempPT.shape[0]):\n",
    "        #     if(count+batchSize<=tempPT.shape[0]):\n",
    "        #      inputs = tempPT[count:count+batchSize].to(device=device)\n",
    "        #      age_label = ageLabelTensor[count:count+batchSize].to(device=device)\n",
    "        #      gender_label = genderLabelTensor[count:count+batchSize].to(device=device)\n",
    "        #      ethn_label = ethnLabelTensor[count:count+batchSize].to(device=device)\n",
    "        #     else:\n",
    "        #        inputs = tempPT[count:].to(device=device)\n",
    "        #        age_label = ageLabelTensor[count:].to(device=device)\n",
    "        #        gender_label = genderLabelTensor[count:].to(device=device)\n",
    "        #        ethn_label = ethnLabelTensor[count:].to(device=device)\n",
    "            \n",
    "            \n",
    "            inputs=resnet(inputs)\n",
    "            # #print(inputs.shape)\n",
    "            inputUpdated = torch.zeros((inputs.shape[0], 128),device=device)\n",
    "            for t in range(0,inputs.shape[0]):\n",
    "             #print(inputs[t].shape,\"inputs[t]\")   \n",
    "             inputUpdated[t] = polyprotect(0,inputs[t])\n",
    "             #count = count +1\n",
    "            #print(\"here4\")\n",
    "             \n",
    "            #print(inputUpdated[0]) \n",
    "            #print(inputUpdated[1])\n",
    "            processedTensor[count:count+inputUpdated.shape[0]] = inputUpdated\n",
    "            ageLabelTensor[count:count+age_label.shape[0]] = age_label\n",
    "            genderLabelTensor[count:count+gender_label.shape[0]] = gender_label\n",
    "            ethnLabelTensor[count:count+ethn_label.shape[0]] = ethn_label\n",
    "            #continue\n",
    "            gender,age,ethn = self(inputUpdated)\n",
    "            # age=torch.squeeze(age)\n",
    "            # age=age.type(torch.float32)\n",
    "            #print(age.shape,age_label.shape)\n",
    "            #print(gender.shape,gender_label.shape)\n",
    "            # predictedGender = torch.argmax(gender,dim=1)\n",
    "            # predictedGender = predictedGender.type(torch.float32)\n",
    "            #print(gender)\n",
    "            loss = gender_loss(gender,gender_label) + 4*age_loss(age,age_label) + 4*ethn_loss(ethn,ethn_label)\n",
    "            #totalGenderLoss = totalGenderLoss + loss.item()\n",
    "            loss.backward()\n",
    "            #print(\"Loss:\",loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_training_loss = total_training_loss+loss.item()*256\n",
    "\n",
    "            predictedGender = torch.argmax(gender,dim=1)\n",
    "            \n",
    "            \n",
    "            for j in range(0,predictedGender.shape[0]):\n",
    "                count=count+1\n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedGender[j].item()==gender_label[j]):\n",
    "                    tempAccGender=tempAccGender+1\n",
    "            \n",
    "            predictedAge = torch.argmax(age,dim=1)\n",
    "            \n",
    "            for j in range(0,predictedAge.shape[0]):\n",
    "               \n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedAge[j].item()==age_label[j]):\n",
    "                    tempAccAge=tempAccAge+1\n",
    "            \n",
    "            predictedEthn = torch.argmax(ethn,dim=1)\n",
    "            \n",
    "            for j in range(0,predictedEthn.shape[0]):\n",
    "               \n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedEthn[j].item()==ethn_label[j]):\n",
    "                    tempAccEthn=tempAccEthn+1\n",
    "         \n",
    "            genderAccuracy =  tempAccGender/count\n",
    "            ageAccuracy =  tempAccAge/count\n",
    "            ethnAccuracy =  tempAccEthn/count\n",
    "            #print(count)\n",
    "            #break\n",
    "         break;\n",
    "         print(\"Training Gender Accuracy:\", genderAccuracy,\"Age:\", ageAccuracy,\"Ethn:\",ethnAccuracy)\n",
    "        #  print(\"\\n\")\n",
    "        #  print(\"Training Ethn Accuracy:\", ethnAccuracy,\"\\n\")\n",
    "        #  print(\"total training loss:\",total_training_loss/16595,\"\\n\")\n",
    "         #print(\"\\n\")\n",
    "         #print(\"max observed value: \", maxVal)\n",
    "        #  if(e%2==0):\n",
    "        #      self.test(testLoader,device)\n",
    "\n",
    "        # print(processedTensor.shape,processedTensor[0])\n",
    "        # print(ageLabelTensor.shape,ageLabelTensor[0])\n",
    "        # print(genderLabelTensor.shape,genderLabelTensor[0])\n",
    "        # print(ethnLabelTensor.shape,ethnLabelTensor[0])\n",
    "        return processedTensor,genderLabelTensor,ageLabelTensor,ethnLabelTensor\n",
    "\n",
    "\n",
    "    def are_values_in_same_class(self,value1, value2):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(0, 2),(2,5),(5,13),(13,20),(20,40),(40,60),(60,80),(80,120)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        for class_range in class_ranges:\n",
    "            if class_range[0] <= value1 <= class_range[1] and class_range[0] <= value2 <= class_range[1]:\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def test(self,testLoader,resnet,device):\n",
    "\n",
    "    \n",
    "        tempAccGender=0\n",
    "        tempAccAge=0\n",
    "        tempAccEthn=0\n",
    "        count=0\n",
    "\n",
    "        self.eval()\n",
    "         \n",
    "        for i,data in enumerate(testLoader):\n",
    "\n",
    "            inputs = data[\"image\"].to(device=device)\n",
    "            #inputs = inputs.type(torch.double)\n",
    "            age_label = data[\"age\"].to(device=device)\n",
    "            gender_label = data[\"gender\"].to(device=device)\n",
    "            ethn_label = data[\"ethnicity\"].to(device=device)\n",
    "            \n",
    "            \n",
    "            inputs=resnet(inputs)\n",
    "            #print(inputs.shape)\n",
    "            inputUpdated = torch.zeros((inputs.shape[0], 128),device=device)\n",
    "            for t in range(0,inputs.shape[0]):\n",
    "             #print(inputs[t].shape,\"inputs[t]\")   \n",
    "             inputUpdated[t] = polyprotect(0,inputs[t])\n",
    "            #print(\"here4\")\n",
    "            #print(inputUpdated[0])\n",
    "            gender,age,ethn = self(inputUpdated)\n",
    "            \n",
    "            predictedGender = torch.argmax(gender,dim=1)\n",
    "            for j in range(0,predictedGender.shape[0]):\n",
    "                count=count+1\n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedGender[j].item()==gender_label[j]):\n",
    "                    tempAccGender=tempAccGender+1\n",
    "            \n",
    "            predictedAge = torch.argmax(age,dim=1)\n",
    "            for j in range(0,predictedAge.shape[0]):\n",
    "               \n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedAge[j].item()==age_label[j]):\n",
    "                    tempAccAge=tempAccAge+1\n",
    "            \n",
    "            predictedEthn = torch.argmax(ethn,dim=1)\n",
    "            for j in range(0,predictedEthn.shape[0]):\n",
    "               \n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedEthn[j].item()==ethn_label[j]):\n",
    "                    tempAccEthn=tempAccEthn+1\n",
    "         \n",
    "        genderAccuracy =  tempAccGender/count\n",
    "        ageAccuracy =  tempAccAge/count\n",
    "        ethnAccuracy =  tempAccEthn/count\n",
    "        \n",
    "        print(\"Test Gender Accuracy:\", genderAccuracy,\"\\n\")\n",
    "        print(\"Test Age Accuracy:\", ageAccuracy,\"\\n\")\n",
    "        print(\"Test Ethn Accuracy:\", ethnAccuracy,\"\\n\")\n",
    "   \n",
    "         #print(\"\\n\")\n",
    "        return genderAccuracy,ageAccuracy, ethnAccuracy\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80148, 128])\n"
     ]
    }
   ],
   "source": [
    "var = 128\n",
    "model=faceAnalytics(var)\n",
    "model.to(device)\n",
    "processedTensor,genderLabelTensor,ageLabelTensor,ethnLabelTensor = model.trainModel(train_dataloader,test_dataloader,device,1,resnet)\n",
    "\n",
    "# torch.save(processedTensor,\"inputCS.pt\")\n",
    "# torch.save(genderLabelTensor,\"genderLabelCS.pt\")\n",
    "# torch.save(ageLabelTensor,\"ageLabelCS.pt\")\n",
    "# torch.save(ethnLabelTensor,\"ethnLabelCS.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(processedTensor,\"inputTempIJB.pt\")\n",
    "torch.save(genderLabelTensor,\"genderTempIJB.pt\")\n",
    "torch.save(ageLabelTensor,\"ageTempIJB.pt\")\n",
    "torch.save(ethnLabelTensor,\"ethnTempIJB.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80148, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temp = torch.load(\"/home/csgrad/byalavar/FHE/HEAAN/FG_Round_2/inputTempIJB1.pt\")\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'input1.txt'\n",
    "\n",
    "# Read data from the .txt file into a NumPy array\n",
    "data = np.loadtxt(file_path, delimiter=',')\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "tensor_data = torch.tensor(data).to(device)\n",
    "tensor_data = tensor_data.type(torch.float32)\n",
    "tensor_data = tensor_data.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=faceAnalytics()\n",
    "model=torch.load(\"modelUsing0.pt\")\n",
    "model.to(device)\n",
    "\n",
    "model.test(val_dataloader,device,tensor_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gender Accuracy: 0.8246142719382835 \n",
      "\n",
      "total training loss: 0.4848069211954667 \n",
      "\n",
      "\n",
      "\n",
      "Test Gender Accuracy: 0.890466816647919 \n",
      "\n",
      "Test Age Loss: 0.1393999291753608 \n",
      "\n",
      "\n",
      "\n",
      "Training Gender Accuracy: 0.8826542912246866 \n",
      "\n",
      "total training loss: 0.346408403792558 \n",
      "\n",
      "\n",
      "\n",
      "Training Gender Accuracy: 0.8980834136933462 \n",
      "\n",
      "total training loss: 0.3051145996663828 \n",
      "\n",
      "\n",
      "\n",
      "Test Gender Accuracy: 0.9147919010123734 \n",
      "\n",
      "Test Age Loss: 0.13645052078470202 \n",
      "\n",
      "\n",
      "\n",
      "Training Gender Accuracy: 0.9035679845708775 \n",
      "\n",
      "total training loss: 0.2846982805942261 \n",
      "\n",
      "\n",
      "\n",
      "Training Gender Accuracy: 0.9057377049180327 \n",
      "\n",
      "total training loss: 0.2736512154547268 \n",
      "\n",
      "\n",
      "\n",
      "Test Gender Accuracy: 0.921541057367829 \n",
      "\n",
      "Test Age Loss: 0.1309188806836329 \n",
      "\n",
      "\n",
      "\n",
      "Training Gender Accuracy: 0.9090525554484089 \n",
      "\n",
      "total training loss: 0.2645829449291867 \n",
      "\n",
      "\n",
      "\n",
      "Training Gender Accuracy: 0.9110414657666345 \n",
      "\n",
      "total training loss: 0.25597767612954947 \n",
      "\n",
      "\n",
      "\n",
      "Test Gender Accuracy: 0.9246344206974129 \n",
      "\n",
      "Test Age Loss: 0.1358024146076799 \n",
      "\n",
      "\n",
      "\n",
      "Training Gender Accuracy: 0.9098963355834137 \n",
      "\n",
      "total training loss: 0.2515466768794047 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=faceAnalytics()\n",
    "#model=torch.load(\"bestFaceAn.pt\")\n",
    "model.to(device)\n",
    "#model.test(val_dataloader,device) # batchSize = 128, learning rate = 0.05\n",
    "model.trainModel(train_dataloader,val_dataloader,device,8,resnet)\n",
    "torch.save(model,\"bestFaceAn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"model92.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "input1=[]\n",
    "for i,data in enumerate(val_dataloader):\n",
    "    \n",
    "    if(count==0):\n",
    "     inputs=resnet(data[\"image\"].to(device))\n",
    "\n",
    " \n",
    "     input1 = polyprotect(0,inputs[0])\n",
    "\n",
    "     break\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4430, -1.9958, -0.6684,  1.0280, -1.1776,  0.5820,  1.1919, -1.2120,\n",
       "         0.5127, -0.1822,  0.6574, -1.9295,  1.2562, -1.0405, -1.5425, -2.9240,\n",
       "         0.8135,  1.8771,  0.7530, -0.4737,  2.3692, -1.3006, -1.7764, -1.7158,\n",
       "        -1.2634,  0.7241,  1.4031, -2.1193,  0.3556,  0.8521, -2.6465, -1.6996,\n",
       "        -0.3830,  0.5889, -1.5234,  0.5707, -0.4224, -1.3535, -1.2482, -2.5428,\n",
       "        -0.5712, -0.3220,  0.3134,  1.0675, -2.0277, -0.0984,  0.9127, -0.8203,\n",
       "         0.5162,  0.8371,  0.2184,  1.7868,  0.1441,  2.2517,  0.9456,  0.2534,\n",
       "        -2.4333, -1.6649,  0.5323, -0.8450,  0.6767, -1.2998,  0.1268, -1.4707,\n",
       "        -1.2234, -3.1957, -0.3958,  2.0446, -1.4466,  1.2428, -0.1448,  1.3427,\n",
       "        -0.7067, -1.0655, -1.0468, -4.1246,  1.0056, -1.1438, -0.4887,  1.7819,\n",
       "        -0.3919, -1.5383,  3.3167,  0.0240, -0.7870, -2.7450, -2.3516, -0.2413,\n",
       "        -1.3294, -2.1587, -1.5163,  1.6668, -0.6842, -1.0367, -1.9168,  0.2893,\n",
       "        -1.6461, -0.0207, -0.1686,  1.3771,  2.9180,  1.0974, -1.5445, -1.6418,\n",
       "        -0.9539, -0.2443, -1.6943,  1.4373,  2.0447, -0.0315,  1.7296, -0.5938,\n",
       "        -0.6551,  0.2148,  0.5714,  2.0054, -1.2415,  2.9698,  0.6808,  0.2798,\n",
       "         0.3181,  0.2219,  0.2307,  0.2685,  0.3365,  0.8023, -2.3881,  1.5875],\n",
       "       device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 128])\n",
      "tensor([-0.0765, -0.1069, -0.0411,  0.0238,  0.0118,  0.0784, -0.0193, -0.0194,\n",
      "        -0.0132,  0.0487,  0.0409, -0.0441, -0.1003,  0.0050,  0.0179,  0.0378,\n",
      "        -0.0322,  0.0842, -0.0395, -0.0201,  0.0457, -0.0654, -0.0586,  0.0188,\n",
      "        -0.0772,  0.0069,  0.0861, -0.0244,  0.0486, -0.0309, -0.0071,  0.0655,\n",
      "        -0.1047,  0.0400,  0.0215, -0.0451,  0.0729, -0.0582, -0.0285,  0.0711,\n",
      "        -0.0588, -0.0468, -0.0742, -0.0929,  0.0281,  0.0036, -0.0665, -0.0713,\n",
      "        -0.0775,  0.0966,  0.0213,  0.0536, -0.0497, -0.0261,  0.0736, -0.0707,\n",
      "        -0.0635,  0.0039,  0.0356,  0.0208, -0.0432,  0.0132,  0.0090, -0.0508,\n",
      "         0.0649, -0.0474,  0.0852,  0.0400,  0.0783,  0.0792, -0.0682, -0.0400,\n",
      "         0.0074, -0.0067,  0.0327,  0.0179, -0.0077, -0.0110,  0.0008,  0.0626,\n",
      "         0.0268, -0.0395, -0.0766,  0.0686, -0.0601,  0.0466,  0.0655,  0.0477,\n",
      "        -0.0761,  0.0306,  0.0702, -0.0810,  0.0598,  0.0180, -0.0715, -0.0503,\n",
      "        -0.0228, -0.0253,  0.0304, -0.0354, -0.0314, -0.0703, -0.0203,  0.0615,\n",
      "         0.0830,  0.0707,  0.0716, -0.0595, -0.0409, -0.0479, -0.0437,  0.0757,\n",
      "        -0.0215, -0.0659,  0.0782, -0.0438,  0.0741, -0.1035, -0.0170,  0.0418,\n",
      "         0.0713, -0.0575,  0.0096, -0.0349, -0.0885,  0.0735,  0.0153, -0.0870],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(-0.2496, device='cuda:0', grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    print(param[0])\n",
    "    print(torch.dot(input1,param[1]))\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4430, -1.9958, -0.6684,  1.0280, -1.1776,  0.5820,  1.1919, -1.2120,\n",
       "         0.5127, -0.1822,  0.6574, -1.9295,  1.2562, -1.0405, -1.5425, -2.9240,\n",
       "         0.8135,  1.8771,  0.7530, -0.4737,  2.3692, -1.3006, -1.7764, -1.7158,\n",
       "        -1.2634,  0.7241,  1.4031, -2.1193,  0.3556,  0.8521, -2.6465, -1.6996,\n",
       "        -0.3830,  0.5889, -1.5234,  0.5707, -0.4224, -1.3535, -1.2482, -2.5428,\n",
       "        -0.5712, -0.3220,  0.3134,  1.0675, -2.0277, -0.0984,  0.9127, -0.8203,\n",
       "         0.5162,  0.8371,  0.2184,  1.7868,  0.1441,  2.2517,  0.9456,  0.2534,\n",
       "        -2.4333, -1.6649,  0.5323, -0.8450,  0.6767, -1.2998,  0.1268, -1.4707,\n",
       "        -1.2234, -3.1957, -0.3958,  2.0446, -1.4466,  1.2428, -0.1448,  1.3427,\n",
       "        -0.7067, -1.0655, -1.0468, -4.1246,  1.0056, -1.1438, -0.4887,  1.7819,\n",
       "        -0.3919, -1.5383,  3.3167,  0.0240, -0.7870, -2.7450, -2.3516, -0.2413,\n",
       "        -1.3294, -2.1587, -1.5163,  1.6668, -0.6842, -1.0367, -1.9168,  0.2893,\n",
       "        -1.6461, -0.0207, -0.1686,  1.3771,  2.9180,  1.0974, -1.5445, -1.6418,\n",
       "        -0.9539, -0.2443, -1.6943,  1.4373,  2.0447, -0.0315,  1.7296, -0.5938,\n",
       "        -0.6551,  0.2148,  0.5714,  2.0054, -1.2415,  2.9698,  0.6808,  0.2798,\n",
       "         0.3181,  0.2219,  0.2307,  0.2685,  0.3365,  0.8023, -2.3881,  1.5875],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory=\"\"\n",
    "file_name = \"input1.txt\"\n",
    "\n",
    "with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in input1:\n",
    "            file.write(f\"{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "test() missing 1 required positional argument: 'tensor_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/csgrad/byalavar/FHE/HEAAN/faceAnlayticsExp1.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-01.cse.buffalo.edu/home/csgrad/byalavar/FHE/HEAAN/faceAnlayticsExp1.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#model=torch.load(\"/home/csgrad/byalavar/HEAAN/bestFaceAn.pt\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btrail-01.cse.buffalo.edu/home/csgrad/byalavar/FHE/HEAAN/faceAnlayticsExp1.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mtest(val_dataloader,device)\n",
      "\u001b[0;31mTypeError\u001b[0m: test() missing 1 required positional argument: 'tensor_data'"
     ]
    }
   ],
   "source": [
    "#model=torch.load(\"/home/csgrad/byalavar/HEAAN/bestFaceAn.pt\")\n",
    "model.test(val_dataloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"modelUsing0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "Parameter containing:\n",
      "tensor([-0.0608, -0.0217,  0.0529, -0.0089,  0.0091,  0.0046, -0.0850,  0.0737,\n",
      "         0.0143,  0.0217, -0.0977,  0.0810, -0.0564,  0.0011,  0.0722, -0.0247,\n",
      "         0.0400, -0.0504, -0.0266,  0.0041,  0.0306, -0.0060, -0.0088, -0.0187,\n",
      "         0.0724, -0.0828, -0.0848, -0.0345, -0.0290,  0.0105,  0.0145, -0.0039,\n",
      "        -0.0052, -0.0191,  0.0698, -0.0620, -0.0487, -0.1063,  0.0467, -0.0002,\n",
      "         0.0537,  0.0358, -0.0329,  0.0654, -0.0006, -0.1011, -0.0192, -0.0640,\n",
      "        -0.0694,  0.0320, -0.1085,  0.0418, -0.0238, -0.0615, -0.0560,  0.0657,\n",
      "        -0.0512,  0.0451, -0.0391,  0.0244, -0.0777, -0.0340,  0.0475,  0.0886,\n",
      "         0.0003, -0.0109, -0.0577, -0.0619, -0.0323, -0.0008, -0.0176,  0.0066,\n",
      "         0.0504,  0.0278, -0.0135,  0.0303,  0.0313, -0.0311,  0.0308, -0.0487,\n",
      "         0.0603, -0.0842,  0.0063, -0.0263,  0.0686,  0.0789,  0.0931,  0.0091,\n",
      "        -0.0183, -0.0787,  0.0457,  0.0954,  0.0281, -0.0374,  0.0159, -0.0512,\n",
      "        -0.0433, -0.0511, -0.0834,  0.0953, -0.1000,  0.0523, -0.0116,  0.0425,\n",
      "        -0.0527, -0.0206, -0.0190, -0.0797,  0.0660, -0.0485, -0.0466,  0.0153,\n",
      "        -0.0169,  0.0452, -0.0746, -0.0683,  0.0565,  0.0540, -0.0384,  0.0310,\n",
      "         0.0139,  0.0437, -0.0087, -0.0292,  0.0351, -0.0614, -0.0722,  0.0297,\n",
      "         0.1136,  0.0672, -0.0866,  0.1015, -0.0465,  0.0330,  0.0159, -0.0102,\n",
      "         0.0414,  0.0236,  0.0637,  0.0466,  0.0528, -0.0651, -0.0702, -0.0829,\n",
      "        -0.0911,  0.0808, -0.0970,  0.0308,  0.1084,  0.0431,  0.0110,  0.0354,\n",
      "        -0.0004,  0.0313, -0.0509,  0.0891, -0.0619, -0.0549,  0.0235, -0.0453,\n",
      "         0.0305,  0.0650,  0.0026,  0.0656,  0.0059,  0.0531, -0.0560,  0.0448,\n",
      "         0.0648, -0.0330, -0.0089, -0.1149,  0.0918, -0.0469,  0.0221, -0.0289,\n",
      "         0.0285,  0.1076, -0.0770, -0.0513,  0.0834,  0.0107, -0.0912, -0.0662,\n",
      "        -0.0766,  0.0287,  0.0123, -0.0207, -0.0837, -0.0780,  0.0546,  0.0836,\n",
      "        -0.0027,  0.1223, -0.0599, -0.0164,  0.0038,  0.0645,  0.0602, -0.0260,\n",
      "        -0.1117, -0.0681, -0.0433,  0.0739,  0.0762,  0.0478,  0.0897, -0.0819,\n",
      "         0.0830, -0.0597,  0.0852,  0.0506,  0.0303,  0.0344, -0.0822, -0.0732,\n",
      "        -0.0964,  0.0137, -0.0595, -0.0760,  0.0394,  0.0083, -0.0690,  0.0529,\n",
      "         0.0407, -0.0937,  0.0200,  0.0386,  0.0058,  0.0301, -0.0113,  0.0929,\n",
      "         0.0309, -0.0483, -0.0190, -0.0249,  0.0614, -0.0806,  0.0442,  0.0468,\n",
      "         0.0455,  0.0116, -0.0451, -0.0917,  0.0322, -0.0705,  0.0507, -0.0204,\n",
      "         0.0007, -0.0027,  0.0428,  0.0796, -0.0316,  0.0552, -0.0604,  0.0575,\n",
      "         0.0360,  0.0643,  0.0433, -0.0074,  0.0539, -0.0003,  0.0321,  0.0552,\n",
      "         0.0537, -0.0608, -0.0301,  0.0042,  0.0934,  0.0286, -0.1056, -0.0474,\n",
      "        -0.0313, -0.0984,  0.0300,  0.0091, -0.0760,  0.0004, -0.0812,  0.0902,\n",
      "        -0.0741,  0.0695, -0.0525, -0.0741,  0.0278, -0.0640, -0.0199, -0.0633,\n",
      "         0.0149, -0.0275, -0.0856, -0.0580, -0.0747, -0.0139,  0.0040,  0.0315,\n",
      "        -0.0552, -0.0549,  0.0007, -0.0453, -0.0781,  0.0112,  0.0050,  0.0232,\n",
      "        -0.0420,  0.0694,  0.0666,  0.0664,  0.0244, -0.0461, -0.0278, -0.0601,\n",
      "         0.0799, -0.0605, -0.0106, -0.0346,  0.0085,  0.0739, -0.0433, -0.0053,\n",
      "         0.0755, -0.0756, -0.0710, -0.0266,  0.0126, -0.0906,  0.0843, -0.0450,\n",
      "        -0.0306,  0.0114, -0.0750,  0.0695,  0.0847,  0.0204,  0.0869,  0.0717,\n",
      "         0.0892,  0.0096, -0.0551, -0.0669, -0.0853, -0.0864,  0.1047, -0.0818,\n",
      "        -0.0770,  0.0931,  0.0221,  0.0545, -0.0748,  0.0771,  0.0588, -0.0495,\n",
      "         0.0075, -0.0071, -0.0681,  0.0002,  0.0290, -0.0568, -0.0345,  0.0315,\n",
      "         0.0363, -0.0928,  0.0599, -0.0570,  0.0604,  0.0524,  0.0424, -0.1010,\n",
      "        -0.1053, -0.0505, -0.0577,  0.0719,  0.0369,  0.0338,  0.0068,  0.0026,\n",
      "         0.0439,  0.0258, -0.0427,  0.0594, -0.0648, -0.0282, -0.0707, -0.0582,\n",
      "        -0.0210, -0.0880,  0.0366, -0.0133, -0.0476,  0.0635, -0.0333,  0.0534,\n",
      "        -0.0450,  0.0781, -0.0265, -0.0237,  0.0189, -0.0033, -0.0879,  0.0066,\n",
      "        -0.0582,  0.0313, -0.0012, -0.0256,  0.0776, -0.0801,  0.0883, -0.0772,\n",
      "        -0.0547,  0.0284, -0.0085, -0.0172, -0.0248, -0.0611, -0.0257, -0.0427,\n",
      "        -0.0368,  0.0097, -0.0731,  0.0816, -0.0421, -0.0192, -0.0546,  0.0163,\n",
      "         0.0369,  0.0193, -0.0264, -0.0645, -0.0581, -0.0677, -0.0154,  0.0552,\n",
      "        -0.0798,  0.0711,  0.0431,  0.0272, -0.0204,  0.0371, -0.0032,  0.0675,\n",
      "        -0.0399, -0.0378, -0.0247,  0.0556, -0.0469, -0.0313,  0.0346, -0.0774,\n",
      "         0.0135, -0.0863, -0.0374, -0.0790,  0.0758, -0.0534, -0.0496, -0.0032,\n",
      "         0.0652, -0.0750, -0.0940, -0.0387, -0.0594, -0.0751,  0.0502,  0.0511,\n",
      "        -0.0417,  0.0018, -0.0762, -0.0004,  0.0037,  0.0627,  0.0120,  0.0338,\n",
      "        -0.0688,  0.0400,  0.0855,  0.0265,  0.0423, -0.0375,  0.0245,  0.0013,\n",
      "         0.0121, -0.0373, -0.0252,  0.0247,  0.0048,  0.0476,  0.0585, -0.0490,\n",
      "         0.0448, -0.0205, -0.0537, -0.0524,  0.0103,  0.0989, -0.0534,  0.0832,\n",
      "         0.0281,  0.0697, -0.0782,  0.0404, -0.0589,  0.0134, -0.0148, -0.0104,\n",
      "         0.0143, -0.0749, -0.0312,  0.0053, -0.0827, -0.0643, -0.0590,  0.0067],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    count=count+1\n",
    "    if(count==2):\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input1 tensor([[0.1725, 0.7784, 0.7692, 0.3667]])\n",
      "param Parameter containing:\n",
      "tensor([[ 0.0794, -0.2791,  0.0171, -0.2814],\n",
      "        [-0.0677, -0.3197, -0.2683, -0.3466]], requires_grad=True)\n",
      "param Parameter containing:\n",
      "tensor([ 0.0877, -0.3602], requires_grad=True)\n",
      "tensor([[-0.2058, -0.9542]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a=nn.Linear(4,2)\n",
    "input1=torch.rand((1,4))\n",
    "print(\"input1\",input1)\n",
    "for param in a.parameters():\n",
    "    print(\"param\",param)\n",
    "print(a(input1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/csgrad/byalavar/FHE/HEAAN/faceAnlaytics.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-01.cse.buffalo.edu/home/csgrad/byalavar/FHE/HEAAN/faceAnlaytics.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m        ageBias \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btrail-01.cse.buffalo.edu/home/csgrad/byalavar/FHE/HEAAN/faceAnlaytics.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     count\u001b[39m=\u001b[39mcount\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btrail-01.cse.buffalo.edu/home/csgrad/byalavar/FHE/HEAAN/faceAnlaytics.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(ageBias),\u001b[39mlen\u001b[39;49m(ageBias[\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "model=faceAnalytics()\n",
    "model=torch.load(\"modelUsing0.pt\")\n",
    "model.to(device)\n",
    "count=0\n",
    "ageBias=[]\n",
    "for param in model.parameters(): \n",
    "    print(param.shape)\n",
    "    if(count==7):\n",
    "       ageBias = param.tolist()\n",
    "    count=count+1\n",
    "print(len(ageBias),len(ageBias[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09601110219955444]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ageBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007277209311723709,\n",
       " -0.0321158841252327,\n",
       " -0.031418390572071075,\n",
       " -0.006457424722611904,\n",
       " 0.0034283681306988,\n",
       " 0.018647771328687668,\n",
       " 0.010360945016145706,\n",
       " -0.046598006039857864,\n",
       " -0.05573476478457451,\n",
       " 0.01843833737075329,\n",
       " 0.0027071069926023483,\n",
       " 0.03294723108410835,\n",
       " 0.02504689060151577,\n",
       " -0.03142755106091499,\n",
       " -0.022691987454891205,\n",
       " 0.03483140096068382,\n",
       " -0.05341877415776253,\n",
       " 0.04222894087433815,\n",
       " -0.01728733628988266,\n",
       " -0.04929398000240326,\n",
       " 0.00046024261973798275,\n",
       " -0.044817082583904266,\n",
       " 0.0034655649214982986,\n",
       " -0.03304927796125412,\n",
       " -0.0016231774352490902,\n",
       " -0.04087826982140541,\n",
       " 0.01253503654152155,\n",
       " -0.030864598229527473,\n",
       " -0.013328468427062035,\n",
       " 0.012476014904677868,\n",
       " -0.037187058478593826,\n",
       " 0.006219789385795593,\n",
       " -0.013966446742415428,\n",
       " 0.01565675437450409,\n",
       " 0.0035794111900031567,\n",
       " -0.005585063714534044,\n",
       " 0.006055895704776049,\n",
       " 0.00048314392915926874,\n",
       " -0.010381164960563183,\n",
       " -0.018931696191430092,\n",
       " 0.012918422929942608,\n",
       " 0.014919551089406013,\n",
       " -0.015488061122596264,\n",
       " -0.01959354802966118,\n",
       " 0.038306545466184616,\n",
       " -0.05790992081165314,\n",
       " -0.017201725393533707,\n",
       " -0.03371630981564522,\n",
       " -0.024791700765490532,\n",
       " -0.031961359083652496,\n",
       " -0.030725467950105667,\n",
       " -0.045284777879714966,\n",
       " -0.012051970697939396,\n",
       " 0.01729130558669567,\n",
       " -0.058299340307712555,\n",
       " -0.008241587318480015,\n",
       " 0.008392270654439926,\n",
       " 0.012528457678854465,\n",
       " -0.02050808258354664,\n",
       " 0.018400557339191437,\n",
       " -0.05368071794509888,\n",
       " -0.04868054389953613,\n",
       " -0.011333262547850609,\n",
       " 0.036896102130413055,\n",
       " -0.029022417962551117,\n",
       " 0.023950502276420593,\n",
       " 0.019032996147871017,\n",
       " 0.0386575311422348,\n",
       " 0.04340917244553566,\n",
       " -0.05563858523964882,\n",
       " -0.02269531600177288,\n",
       " -0.008079467341303825,\n",
       " 0.027136100456118584,\n",
       " 0.024767564609646797,\n",
       " 0.0464879646897316,\n",
       " -0.034298501908779144,\n",
       " -0.05876478925347328,\n",
       " 0.019566943868994713,\n",
       " -0.00596601003780961,\n",
       " 0.012794088572263718,\n",
       " 0.028237752616405487,\n",
       " 0.0027152879629284143,\n",
       " -0.018138255923986435,\n",
       " 0.024093421176075935,\n",
       " 0.014445447362959385,\n",
       " -0.029630817472934723,\n",
       " -0.009077931754291058,\n",
       " 0.04275999963283539,\n",
       " 0.019907813519239426,\n",
       " 0.03173178434371948,\n",
       " -0.010339722968637943,\n",
       " 0.021917376667261124,\n",
       " 0.00547691760584712,\n",
       " 0.04024359956383705,\n",
       " 0.00037949683610349894,\n",
       " -0.043355707079172134,\n",
       " -0.029875755310058594,\n",
       " 0.012577931396663189,\n",
       " -0.01590362749993801,\n",
       " -0.02837674878537655,\n",
       " -0.0026315744034945965,\n",
       " 0.029960552230477333,\n",
       " 0.048201654106378555,\n",
       " 0.05135548114776611,\n",
       " -0.059926338493824005,\n",
       " -0.022241834551095963,\n",
       " -0.047569092363119125,\n",
       " 0.007798505946993828,\n",
       " 0.024846207350492477,\n",
       " -0.06639613211154938,\n",
       " -0.0007212079362943769,\n",
       " -0.022007303312420845,\n",
       " 0.0007844719802960753,\n",
       " -0.03660847619175911,\n",
       " 0.010727467015385628,\n",
       " -0.024773655459284782,\n",
       " 0.015029202215373516,\n",
       " 0.009059355594217777,\n",
       " 0.02116716280579567,\n",
       " 0.04279369115829468,\n",
       " -0.07188471406698227,\n",
       " -0.002769036218523979,\n",
       " 0.03128764033317566,\n",
       " 0.018925964832305908,\n",
       " -0.01784098893404007,\n",
       " 0.029889674857258797,\n",
       " 0.0049612135626375675,\n",
       " -0.006710418500006199]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a list of lists with shape (512, 128)\n",
    "\n",
    "\n",
    "# Define the directory where you want to save the text files\n",
    "output_directory = \"ageWeights\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Write each list to a separate text file\n",
    "\n",
    "\n",
    "# Write each list to a separate text file\n",
    "for i, sublist in enumerate(ageWeights):\n",
    "    # Define the file name with leading zeros\n",
    "    file_name = f\"{i:03d}.txt\"\n",
    "\n",
    "    # Write each value in the sublist on a new line\n",
    "    with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in sublist:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "print(\"Files saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/csgrad/byalavar/FHE/HEAAN/faceAnlaytics.ipynb Cell 39\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/byalavar/FHE/HEAAN/faceAnlaytics.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m        \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/byalavar/FHE/HEAAN/faceAnlaytics.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     count\u001b[39m=\u001b[39mcount\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/byalavar/FHE/HEAAN/faceAnlaytics.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(layer1Bias),\u001b[39mlen\u001b[39;49m(layer1Bias[\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "count=1\n",
    "layer1Bias=[]\n",
    "for param in model.parameters(): \n",
    "    print(param.shape)\n",
    "    if(count==2):\n",
    "       layer1Bias = param.tolist()\n",
    "       break\n",
    "    count=count+1\n",
    "print(len(layer1Bias),len(layer1Bias[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory=\"\"\n",
    "file_name = \"ageBias.txt\"\n",
    "\n",
    "with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in ageBias:\n",
    "            file.write(f\"{value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
