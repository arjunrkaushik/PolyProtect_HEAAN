{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet_pytorch in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (2.5.3)\n",
      "Requirement already satisfied: numpy in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from facenet_pytorch) (1.26.0)\n",
      "Requirement already satisfied: requests in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from facenet_pytorch) (2.31.0)\n",
      "Requirement already satisfied: torchvision in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from facenet_pytorch) (0.16.0)\n",
      "Requirement already satisfied: pillow in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from facenet_pytorch) (10.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from requests->facenet_pytorch) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from requests->facenet_pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from requests->facenet_pytorch) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from requests->facenet_pytorch) (2022.12.7)\n",
      "Requirement already satisfied: torch in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from torchvision->facenet_pytorch) (2.1.0)\n",
      "Requirement already satisfied: filelock in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from jinja2->torch->torchvision->facenet_pytorch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from sympy->torch->torchvision->facenet_pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (4.44.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/kaushik3/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import os\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# from tensorflow.keras.models import Model\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.gender_mapping = {'male': 0, 'female': 1}\n",
    "        self.ethnicity_mapping = {'white': 0, 'black': 1, 'asian': 2, 'hispanic': 3}\n",
    "        self.data['nameNum'] = self.data['name'].astype('category').cat.codes\n",
    "        self.data['nameNum'] = self.data['nameNum'].astype(int)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    \n",
    "    def getAgeLabel(self,value1):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(15, 22),(22,40),(40,60),(60,80)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        if(class_ranges[0][0]<=value1 and value1<class_ranges[0][1]):\n",
    "            return 0\n",
    "        elif(class_ranges[1][0]<=value1 and value1<class_ranges[1][1]):\n",
    "            return 1\n",
    "        elif(class_ranges[2][0]<=value1 and value1<class_ranges[2][1]):\n",
    "            return 2\n",
    "        elif(class_ranges[3][0]<=value1 and value1<class_ranges[3][1]):\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = '/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/Data/CELEBTEST/'+row['name']+'/' + row['filename']  # Assuming images are in a folder named 'images'\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "        except Exception as e:\n",
    "            # Handle the error, for example, you can return a placeholder image\n",
    "            print(\"here\")\n",
    "            #self.__getitem__(idx + 1)\n",
    "            #image = Image.new('RGB', (224, 224))  # Create a blank image\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        age = row['age']\n",
    "        \n",
    "        if(row['age']<=0):\n",
    "            age=35\n",
    "        label = {\n",
    "            'age': self.getAgeLabel(age),\n",
    "            'gender': self.gender_mapping.get(row['gender'], 0),  # -1 for unknown\n",
    "            'ethnicity': self.ethnicity_mapping.get(row['ethnicity'], 0),\n",
    "            'age1':age,\n",
    "            'name': row['nameNum']\n",
    "        \n",
    "        }\n",
    "        #print(row['name'])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv'  # Replace with the actual path to your CSV file\n",
    "# df = pd.read_csv(csv_file)\n",
    "\n",
    "# # Create a list to store the indices of rows with missing files\n",
    "# rows_to_remove = []\n",
    "# count=0\n",
    "# # Iterate through the DataFrame and check if the files exist\n",
    "# for index, row in df.iterrows():\n",
    "#     image_path = '/home/csgrad/byalavar/FHE/celebSet/CELEBTEST/CELEBTEST/'+row['name']+'/' + row['filename'] \n",
    "#     if not os.path.exists(image_path):\n",
    "#         rows_to_remove.append(index)\n",
    "#         count=count+1\n",
    "# df = df.drop(rows_to_remove)\n",
    "# df.to_csv(csv_file, index=False)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor(),          # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainSet = CustomDataset('/home/csgrad/byalavar/FHE/HEAAN/FA_CVPR_Exp/celebSetIdentity.csv', transform=transform, get_pairs=True, num_pairs=25)\n",
    "# trainloader = DataLoader(trainSet, batch_size=16, shuffle=True, num_workers = 2)\n",
    "\n",
    "# testSet = CustomDataset('/home/csgrad/byalavar/FHE/HEAAN/FA_CVPR_Exp/celebSetIdentityTest.csv', transform=transform, get_pairs=False)\n",
    "# testloader = DataLoader(testSet, batch_size=16, shuffle=False)\n",
    "\n",
    "trainSet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv', transform=transform)\n",
    "trainloader = DataLoader(trainSet, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "testSet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/final_clebSET_test.csv', transform=transform)\n",
    "testloader = DataLoader(testSet, batch_size=16, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempIter = iter(testloader)\n",
    "# images,gen_pairs, imp_pairs = next(tempIter)\n",
    "# # imshow(images[0])\n",
    "# # print(labels['age'][0],labels['gender'][0],labels['ethnicity'][0])\n",
    "# # print(image_pairs)\n",
    "# imshow(images[0])\n",
    "# imshow(gen_pairs[0][0])\n",
    "# imshow(imp_pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import Conv2d, Linear\n",
    "from torch.nn import BatchNorm1d, BatchNorm2d\n",
    "from torch.nn import ReLU, Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.nn import PReLU\n",
    "import os\n",
    "\n",
    "def build_model(model_name='ir_50'):\n",
    "    if model_name == 'ir_101':\n",
    "        return IR_101(input_size=(112,112))\n",
    "    elif model_name == 'ir_50':\n",
    "        return IR_50(input_size=(112,112))\n",
    "    elif model_name == 'ir_se_50':\n",
    "        return IR_SE_50(input_size=(112,112))\n",
    "    elif model_name == 'ir_34':\n",
    "        return IR_34(input_size=(112,112))\n",
    "    elif model_name == 'ir_18':\n",
    "        return IR_18(input_size=(112,112))\n",
    "    else:\n",
    "        raise ValueError('not a correct model name', model_name)\n",
    "\n",
    "def initialize_weights(modules):\n",
    "    \"\"\" Weight initilize, conv2d and linear is initialized with kaiming_normal\n",
    "    \"\"\"\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight,\n",
    "                                    mode='fan_out',\n",
    "                                    nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight,\n",
    "                                    mode='fan_out',\n",
    "                                    nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class Flatten(Module):\n",
    "    \"\"\" Flat tensor\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class LinearBlock(Module):\n",
    "    \"\"\" Convolution block without no-linear activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_c, kernel, stride, padding, groups=groups, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNAP(Module):\n",
    "    \"\"\" Global Norm-Aware Pooling block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c):\n",
    "        super(GNAP, self).__init__()\n",
    "        self.bn1 = BatchNorm2d(in_c, affine=False)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.bn2 = BatchNorm1d(in_c, affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x_norm = torch.norm(x, 2, 1, True)\n",
    "        x_norm_mean = torch.mean(x_norm)\n",
    "        weight = x_norm_mean / x_norm\n",
    "        x = x * weight\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        feature = self.bn2(x)\n",
    "        return feature\n",
    "\n",
    "\n",
    "class GDC(Module):\n",
    "    \"\"\" Global Depthwise Convolution block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, embedding_size):\n",
    "        super(GDC, self).__init__()\n",
    "        self.conv_6_dw = LinearBlock(in_c, in_c,\n",
    "                                     groups=in_c,\n",
    "                                     kernel=(7, 7),\n",
    "                                     stride=(1, 1),\n",
    "                                     padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(in_c, embedding_size, bias=False)\n",
    "        self.bn = BatchNorm1d(embedding_size, affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_6_dw(x)\n",
    "        x = self.conv_6_flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEModule(Module):\n",
    "    \"\"\" SE block\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = Conv2d(channels, channels // reduction,\n",
    "                          kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight.data)\n",
    "\n",
    "        self.relu = ReLU(inplace=True)\n",
    "        self.fc2 = Conv2d(channels // reduction, channels,\n",
    "                          kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlockIR(Module):\n",
    "    \"\"\" BasicBlock for IRNet\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(BasicBlockIR, self).__init__()\n",
    "        if in_channel == depth:\n",
    "            self.shortcut_layer = MaxPool2d(1, stride)\n",
    "        else:\n",
    "            self.shortcut_layer = Sequential(\n",
    "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
    "                BatchNorm2d(depth))\n",
    "        self.res_layer = Sequential(\n",
    "            BatchNorm2d(in_channel),\n",
    "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n",
    "            BatchNorm2d(depth),\n",
    "            PReLU(depth),\n",
    "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False),\n",
    "            BatchNorm2d(depth))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut_layer(x)\n",
    "        res = self.res_layer(x)\n",
    "\n",
    "        return res + shortcut\n",
    "\n",
    "\n",
    "class BottleneckIR(Module):\n",
    "    \"\"\" BasicBlock with bottleneck for IRNet\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(BottleneckIR, self).__init__()\n",
    "        reduction_channel = depth // 4\n",
    "        if in_channel == depth:\n",
    "            self.shortcut_layer = MaxPool2d(1, stride)\n",
    "        else:\n",
    "            self.shortcut_layer = Sequential(\n",
    "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
    "                BatchNorm2d(depth))\n",
    "        self.res_layer = Sequential(\n",
    "            BatchNorm2d(in_channel),\n",
    "            Conv2d(in_channel, reduction_channel, (1, 1), (1, 1), 0, bias=False),\n",
    "            BatchNorm2d(reduction_channel),\n",
    "            PReLU(reduction_channel),\n",
    "            Conv2d(reduction_channel, reduction_channel, (3, 3), (1, 1), 1, bias=False),\n",
    "            BatchNorm2d(reduction_channel),\n",
    "            PReLU(reduction_channel),\n",
    "            Conv2d(reduction_channel, depth, (1, 1), stride, 0, bias=False),\n",
    "            BatchNorm2d(depth))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut_layer(x)\n",
    "        res = self.res_layer(x)\n",
    "\n",
    "        return res + shortcut\n",
    "\n",
    "\n",
    "class BasicBlockIRSE(BasicBlockIR):\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(BasicBlockIRSE, self).__init__(in_channel, depth, stride)\n",
    "        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\n",
    "\n",
    "\n",
    "class BottleneckIRSE(BottleneckIR):\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(BottleneckIRSE, self).__init__(in_channel, depth, stride)\n",
    "        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\n",
    "\n",
    "\n",
    "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
    "    '''A named tuple describing a ResNet block.'''\n",
    "\n",
    "\n",
    "def get_block(in_channel, depth, num_units, stride=2):\n",
    "\n",
    "    return [Bottleneck(in_channel, depth, stride)] +\\\n",
    "           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
    "\n",
    "\n",
    "def get_blocks(num_layers):\n",
    "    if num_layers == 18:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=2),\n",
    "            get_block(in_channel=64, depth=128, num_units=2),\n",
    "            get_block(in_channel=128, depth=256, num_units=2),\n",
    "            get_block(in_channel=256, depth=512, num_units=2)\n",
    "        ]\n",
    "    elif num_layers == 34:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=4),\n",
    "            get_block(in_channel=128, depth=256, num_units=6),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 50:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=4),\n",
    "            get_block(in_channel=128, depth=256, num_units=14),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 100:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=13),\n",
    "            get_block(in_channel=128, depth=256, num_units=30),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 152:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=256, num_units=3),\n",
    "            get_block(in_channel=256, depth=512, num_units=8),\n",
    "            get_block(in_channel=512, depth=1024, num_units=36),\n",
    "            get_block(in_channel=1024, depth=2048, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 200:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=256, num_units=3),\n",
    "            get_block(in_channel=256, depth=512, num_units=24),\n",
    "            get_block(in_channel=512, depth=1024, num_units=36),\n",
    "            get_block(in_channel=1024, depth=2048, num_units=3)\n",
    "        ]\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "class Backbone(Module):\n",
    "    def __init__(self, input_size, num_layers, mode='ir'):\n",
    "        \"\"\" Args:\n",
    "            input_size: input_size of backbone\n",
    "            num_layers: num_layers of backbone\n",
    "            mode: support ir or irse\n",
    "        \"\"\"\n",
    "        super(Backbone, self).__init__()\n",
    "        assert input_size[0] in [112, 224], \\\n",
    "            \"input_size should be [112, 112] or [224, 224]\"\n",
    "        assert num_layers in [18, 34, 50, 100, 152, 200], \\\n",
    "            \"num_layers should be 18, 34, 50, 100 or 152\"\n",
    "        assert mode in ['ir', 'ir_se'], \\\n",
    "            \"mode should be ir or ir_se\"\n",
    "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
    "                                      BatchNorm2d(64), PReLU(64))\n",
    "        blocks = get_blocks(num_layers)\n",
    "        if num_layers <= 100:\n",
    "            if mode == 'ir':\n",
    "                unit_module = BasicBlockIR\n",
    "            elif mode == 'ir_se':\n",
    "                unit_module = BasicBlockIRSE\n",
    "            output_channel = 512\n",
    "        else:\n",
    "            if mode == 'ir':\n",
    "                unit_module = BottleneckIR\n",
    "            elif mode == 'ir_se':\n",
    "                unit_module = BottleneckIRSE\n",
    "            output_channel = 2048\n",
    "\n",
    "        if input_size[0] == 112:\n",
    "            self.output_layer = Sequential(BatchNorm2d(output_channel),\n",
    "                                        Dropout(0.4), Flatten(),\n",
    "                                        Linear(output_channel * 7 * 7, 512),\n",
    "                                        BatchNorm1d(512, affine=False))\n",
    "        else:\n",
    "            self.output_layer = Sequential(\n",
    "                BatchNorm2d(output_channel), Dropout(0.4), Flatten(),\n",
    "                Linear(output_channel * 14 * 14, 512),\n",
    "                BatchNorm1d(512, affine=False))\n",
    "\n",
    "        modules = []\n",
    "        for block in blocks:\n",
    "            for bottleneck in block:\n",
    "                modules.append(\n",
    "                    unit_module(bottleneck.in_channel, bottleneck.depth,\n",
    "                                bottleneck.stride))\n",
    "        self.body = Sequential(*modules)\n",
    "\n",
    "        initialize_weights(self.modules())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # current code only supports one extra image\n",
    "        # it comes with a extra dimension for number of extra image. We will just squeeze it out for now\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        for idx, module in enumerate(self.body):\n",
    "            x = module(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        norm = torch.norm(x, 2, 1, True)\n",
    "        output = torch.div(x, norm)\n",
    "\n",
    "        return output, norm\n",
    "\n",
    "\n",
    "\n",
    "def IR_18(input_size):\n",
    "    \"\"\" Constructs a ir-18 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 18, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_34(input_size):\n",
    "    \"\"\" Constructs a ir-34 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 34, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_50(input_size):\n",
    "    \"\"\" Constructs a ir-50 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 50, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_101(input_size):\n",
    "    \"\"\" Constructs a ir-101 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 100, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_152(input_size):\n",
    "    \"\"\" Constructs a ir-152 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 152, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_200(input_size):\n",
    "    \"\"\" Constructs a ir-200 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 200, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_50(input_size):\n",
    "    \"\"\" Constructs a ir_se-50 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 50, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_101(input_size):\n",
    "    \"\"\" Constructs a ir_se-101 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 100, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_152(input_size):\n",
    "    \"\"\" Constructs a ir_se-152 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 152, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_200(input_size):\n",
    "    \"\"\" Constructs a ir_se-200 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 200, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "adaface_models = {\n",
    "    'ir_18':\"/home/csgrad/byalavar/FHE/HEAAN/FA_CVPR_Exp/adaface_ir18_webface4m.ckpt\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_pretrained_model(architecture='ir_18'):\n",
    "    # load model and pretrained statedict\n",
    "    assert architecture in adaface_models.keys()\n",
    "    model = build_model(architecture)\n",
    "    statedict = torch.load(adaface_models[architecture])['state_dict']\n",
    "    model_statedict = {key[6:]:val for key, val in statedict.items() if key.startswith('model.')}\n",
    "    model.load_state_dict(model_statedict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def to_input(pil_rgb_image):\n",
    "    np_img = np.array(pil_rgb_image)\n",
    "    brg_img = ((np_img[:,:,::-1] / 255.) - 0.5) / 0.5\n",
    "    tensor = torch.tensor([brg_img.transpose(2,0,1)]).float()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaFaceModel = load_pretrained_model('ir_18')\n",
    "\n",
    "# print(images.shape)\n",
    "# bgr_image = images[:, [2, 1, 0], :, :]\n",
    "# print(bgr_image.shape)\n",
    "\n",
    "\n",
    "#feature, _ = adaFaceModel(bgr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet.data['age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_age_value(original_age_value):\n",
    "    return (original_age_value - trainSet.data['age'].min())/(trainSet.data['age'].max() - trainSet.data['age'].min())\n",
    "\n",
    "\n",
    "def get_original_age_value(normalized_age_value):\n",
    "    return normalized_age_value * (trainSet.data['age'].max()  - trainSet.data['age'].min()) + trainSet.data['age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import MSELoss\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "class GenderDet(nn.Module):\n",
    "    def __init__(self, compression_size):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(compression_size,64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class AgeDet(nn.Module):\n",
    "    def __init__(self, compression_size):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(compression_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "class EthnicityDet(nn.Module):\n",
    "    def __init__(self, compression_size):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(compression_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "class IdentityDet(nn.Module):\n",
    "    def __init__(self, compression_size):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(compression_size, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 80))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "\n",
    "class Matryoshka_CE_Loss(nn.Module):\n",
    "    def __init__(self, relative_importance = 1.0):\n",
    "        super(Matryoshka_CE_Loss, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.relative_importance = relative_importance # usually set to all ones\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        loss = 0\n",
    "        for i in range(len(output)):\n",
    "            # loss += self.relative_importance[i] * self.criterion(output[i], target)\n",
    "            loss += self.relative_importance * self.criterion(output[i], target)\n",
    "        return loss\n",
    "\n",
    "class MRL_Linear_Layer(nn.Module):\n",
    "    def __init__(self, nesting_list, num_classes=1000, efficient=False, **kwargs):\n",
    "        super(MRL_Linear_Layer, self).__init__()\n",
    "        self.nesting_list=nesting_list\n",
    "        self.num_classes=num_classes # Number of classes for classification\n",
    "        self.efficient = efficient\n",
    "        if self.efficient:\n",
    "            setattr(self, f\"nesting_classifier_{0}\", nn.Linear(nesting_list[-1], self.num_classes, **kwargs))\t\t\n",
    "        else:\t\n",
    "            for i, num_feat in enumerate(self.nesting_list):\n",
    "                setattr(self, f\"nesting_classifier_{i}\", nn.Linear(num_feat, self.num_classes, **kwargs))\t\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        nesting_logits = ()\n",
    "        for i, num_feat in enumerate(self.nesting_list):\n",
    "            if self.efficient:\n",
    "                if self.nesting_classifier_0.bias is None:\n",
    "                    nesting_logits+= (torch.matmul(x[:, :num_feat], (self.nesting_classifier_0.weight[:, :num_feat]).t()), )\n",
    "                else:\n",
    "                    nesting_logits+= (torch.matmul(x[:, :num_feat], (self.nesting_classifier_0.weight[:, :num_feat]).t()) + self.nesting_classifier_0.bias, )\n",
    "            else:\n",
    "                nesting_logits +=  (getattr(self, f\"nesting_classifier_{i}\")(x[:, :num_feat]),)\n",
    "\n",
    "        return nesting_logits\n",
    "\n",
    "class MRL_Model(nn.Module):\n",
    "    def __init__(self, compression_size, efficient=False):\n",
    "        super(MRL_Model, self).__init__()\n",
    "        self.ll1 = MRL_Linear_Layer([256], num_classes = compression_size, efficient = efficient)\n",
    "        self.ll2 = MRL_Linear_Layer([compression_size], num_classes = 80, efficient = efficient)\n",
    "        # self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        x = self.ll1(x)\n",
    "        return self.ll2(x[0]), x\n",
    "    \n",
    "\n",
    "class CompressionLoss(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(CompressionLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, img, genuine_imgs, imposter_imgs, genuine_sim, imposter_sim):\n",
    "        # img -> base image embedding\n",
    "        # genuine_imgs, imposter_imgs -> Compressed embeddings of dimension 128\n",
    "        # genuine_sim, imposter_sim -> cosine smilairty scores of uncompressed embeddings of dimension 512\n",
    "\n",
    "\n",
    "        # Compute cosine similarity loss\n",
    "        genuine_loss = 0\n",
    "        for i in range(len(genuine_imgs)):\n",
    "            sim_score = F.cosine_similarity(img, genuine_imgs[i])\n",
    "            genuine_loss += torch.mean(torch.abs(sim_score - genuine_sim[i].view(-1)))\n",
    "\n",
    "        imp_loss = 0\n",
    "        for i in range(len(imposter_imgs)):\n",
    "            imp_sim_score = F.cosine_similarity(img, imposter_imgs[i])\n",
    "            imp_loss += torch.mean(torch.abs(imp_sim_score - imposter_sim[i].view(-1)))\n",
    "\n",
    "        \n",
    "        # Compute Covariance Loss\n",
    "        # matrix -> Concatenates compressed genuine and imposter embeddings\n",
    "        \n",
    "        matrix = torch.cat([genuine_imgs[0], imposter_imgs[0]], dim=0)\n",
    "        for i in range(1, len(genuine_imgs)):\n",
    "            matrix = torch.cat([matrix, genuine_imgs[i]], dim = 0)\n",
    "            matrix = torch.cat([matrix, imposter_imgs[i]], dim = 0)\n",
    "\n",
    "\n",
    "        mean_matrix = torch.mean(matrix, dim = 0)\n",
    "        mx = torch.matmul(mean_matrix.t(), mean_matrix)\n",
    "        vx = torch.matmul(matrix.t(), matrix) / self.batch_size # Dividing by batch size as done here https://github.com/human-analysis/hers-encrypted-image-search/blob/master/deep_mds%2B%2B/nntools/tensorflow/networks/deepmds.py#L94\n",
    "        cov_matrix = mx - vx\n",
    "        diag = torch.diag(cov_matrix.diag())\n",
    "        cov_loss = torch.mean(torch.abs(cov_matrix - diag))\n",
    "        \n",
    "        # Compute Supervise Loss - Not used for ArcFace\n",
    "        # # g_dist -> Concatenates compressed genuine embeddings\n",
    "        # g_dist = torch.cat([genuine_imgs[0], genuine_imgs[1]], dim = 0)\n",
    "        # for i in range(2, len(genuine_imgs)):\n",
    "        #     g_dist = torch.cat([g_dist, genuine_imgs[i]], dim = 0)\n",
    "\n",
    "        # # g_dist -> Concatenates compressed imposter embeddings\n",
    "        # imp_dist = torch.cat([imposter_imgs[0], imposter_imgs[1]], dim = 0)\n",
    "        # for i in range(2, len(imposter_imgs)):\n",
    "        #     imp_dist = torch.cat([imp_dist, imposter_imgs[i]], dim = 0)\n",
    "\n",
    "        # gloss = (torch.mean(g_dist) + 1.0) * 0.5\n",
    "        # iloss = (torch.mean(imp_dist) + 1.0) * 0.5\n",
    "\n",
    "        # lda_term = iloss / gloss\n",
    "        # return genuine_loss + imp_loss + 10 * cov_loss \n",
    "        return 10*genuine_loss + imp_loss + cov_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "def generate_C(C_range, m):\n",
    "    \"\"\" Randomly generates m coefficients for the PolyProtect mapping.\n",
    "\n",
    "    **Inputs:**\n",
    "\n",
    "    C_range : integer\n",
    "        The absolute min/max values of the coefficients range.\n",
    "\n",
    "    m : int\n",
    "        The number of coefficients to generate.\n",
    "\n",
    "    **Outputs:**\n",
    "\n",
    "    C : 1D numpy array of integers\n",
    "        Array of m coefficients.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate coefficient range (excluding 0):\n",
    "    neg_range = numpy.arange(-1 * C_range, 0)\n",
    "    pos_range = numpy.arange(1, C_range + 1)\n",
    "    whole_range = numpy.concatenate([neg_range, pos_range])\n",
    "\n",
    "    # Randomly generate m unique coefficients:\n",
    "    C = numpy.random.permutation(whole_range)[0 : m] # randomly permute the whole range and pick the first few m values\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def generate_E(m):\n",
    "    \"\"\" Randomly generates m exponents for the PolyProtect mapping.\n",
    "\n",
    "    **Inputs:**\n",
    "\n",
    "    m : int\n",
    "        The number of exponents to generate.\n",
    "\n",
    "    **Outputs:**\n",
    "\n",
    "    E : 1D numpy array of integers\n",
    "        Array of m exponents.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly generate m unique exponents:\n",
    "    E = numpy.random.permutation(range(1, m + 1))[0 : m] # permute the integers in the range [1, m]\n",
    "\n",
    "    return E\n",
    "\n",
    "\n",
    "def polyprotect(overlap, V):\n",
    "    \"\"\" Maps an embedding to a PolyProtected template.\n",
    "\n",
    "    **Inputs:**\n",
    "\n",
    "    overlap : int\n",
    "        The amount of overlap between sets of embedding elements used to generate each PolyProtected element (0, 1, 2, 3, or 4).\n",
    "\n",
    "    V : torch.Tensor\n",
    "        The embedding as a PyTorch tensor.\n",
    "\n",
    "    **Outputs:**\n",
    "\n",
    "    P : torch.Tensor\n",
    "        The PolyProtected template as a PyTorch tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    # print(\"V = \",V.shape)\n",
    "    C = torch.tensor([-42, -35, 31, 4], dtype=torch.float32, device=V.device)\n",
    "    E = torch.tensor([3, 2, 1, 4], dtype=torch.float32, device=V.device)\n",
    "\n",
    "    if C.shape[0] != E.shape[0]:\n",
    "        print(\"Number of coefficients and exponents must be the same.\")\n",
    "        return None\n",
    "    #print(\"here\")\n",
    "    # print(C.shape)\n",
    "    m = C.shape[0] # number of embedding elements used to generate each PolyProtected element\n",
    "    step_size = m - overlap\n",
    "    decimal_remainder, integer = math.modf((V.shape[1] - m) / step_size)\n",
    "    if decimal_remainder > 0:\n",
    "        padding = math.ceil((1 - decimal_remainder) * step_size)\n",
    "    else:\n",
    "        padding = 0\n",
    "   # print(\"here1\")\n",
    "    # Pad V by \"padding\" zeros at the end\n",
    "    V = torch.cat((V, torch.zeros(padding, device=V.device)), dim=1)\n",
    "\n",
    "    starting_indices = torch.arange(0, V.shape[1] - m + 1, step_size)\n",
    "    #print(\"here2\")\n",
    "    P = torch.zeros((V.shape[0],len(starting_indices)), device=V.device)\n",
    "    \n",
    "    for storage_ind, ind in enumerate(starting_indices):\n",
    "        \n",
    "        final_ind = ind + m\n",
    "        crnt_word = V[:, ind:final_ind]\n",
    "        # print(crnt_word.shape)\n",
    "        P[:, storage_ind] = torch.sum(C * (crnt_word ** E), dim = 1)\n",
    "        # print(P)\n",
    "    #print(\"here3\")\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class faceAnalytics(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1=nn.Linear(128,64)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.layer2=nn.Linear(64,32)\n",
    "        #self.layer3=nn.Linear(1024,512)\n",
    "        # self.layer4=nn.Linear(128,64)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        self.genderOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "        self.ageOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,4)\n",
    "        )\n",
    "        self.ethnicityOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "\n",
    "        self.identity = nn.Sequential(\n",
    "            nn.Linear(128,64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64,80))\n",
    "        # self.maxVal = 0\n",
    "        # self.min=0\n",
    "        \n",
    "    \n",
    "    def writeResult(self,result):\n",
    "       output_directory=\"\"\n",
    "       file_name = \"resultAge.txt\"\n",
    "\n",
    "       with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in result:\n",
    "            file.write(f\"{value}\\n\")\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        #print(\"Input\",x[0])\n",
    "        # x=self.layer1(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "        # x=self.layer2(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "\n",
    "        gender = self.genderOut(x)\n",
    "        age = self.ageOut(x)\n",
    "        ethn = self.ethnicityOut(x)\n",
    "        id = self.identity(x)\n",
    "        # identity = self.identity(x)\n",
    "        return gender, age, ethn, id\n",
    "    \n",
    "    \n",
    "    def are_values_in_same_class(self,value1, value2):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(15, 22),(22,40),(40,60),(60,80)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        for class_range in class_ranges:\n",
    "            if class_range[0] <= value1 < class_range[1] and class_range[0] <= value2 < class_range[1]:\n",
    "                return True\n",
    "    \n",
    "        return False    \n",
    "    \n",
    "    def trainMRL(self, compressed_dim, trainloader, adaFace, MRL_Model, MRL_Loss, device, episodes):\n",
    "        # input_dim = 512\n",
    "        # compressed_dim = 128\n",
    "        model = MRL_Model\n",
    "        # Define optimizer and loss function\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        MRLoss = MRL_Loss.to(device)\n",
    "        model.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "\n",
    "            for img, data in trainloader:\n",
    "                img = img.to(device=device)\n",
    "                label = data['name'].to(device=device)\n",
    "                inputs = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings, _ = adaFace(inputs)\n",
    "                output, _ = model(embeddings)\n",
    "                loss = MRLoss(output, label)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch [{ep + 1}/{episodes}], Loss: {total_loss}\")\n",
    "        torch.save(model, f'MRL_CelebSet_512x{compressed_dim}_AdaFace_{episodes}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def testMRL(self, trainloader, testloader, adaFace, MRL_Model, idModel, genderModel, ageModel, ethModel, device, episodes):\n",
    "        # self.train()\n",
    "        learningRate = 0.001\n",
    "        gender_loss = nn.CrossEntropyLoss().to(device) \n",
    "        age_loss = nn.CrossEntropyLoss().to(device) \n",
    "        ethn_loss = nn.CrossEntropyLoss().to(device)\n",
    "        identityLoss = nn.CrossEntropyLoss().to(device)\n",
    "        IdOptimizer = torch.optim.Adam(idModel.parameters(), lr=learningRate)\n",
    "        AgeOptimizer = torch.optim.Adam(ageModel.parameters(), lr=learningRate)\n",
    "        GenderOptimizer = torch.optim.Adam(genderModel.parameters(), lr=learningRate)\n",
    "        EthOptimizer = torch.optim.Adam(ethModel.parameters(), lr=learningRate)\n",
    "        model = MRL_Model\n",
    "        model.eval()\n",
    "        idModel.train()\n",
    "        genderModel.train()\n",
    "        ageModel.train()\n",
    "        ethModel.train()\n",
    "        for ep in range(0, episodes):\n",
    "            total_gender_loss = 0\n",
    "            total_age_loss = 0\n",
    "            total_ethn_loss = 0\n",
    "            total_id_loss = 0\n",
    "            for i, data in enumerate(trainloader):\n",
    "\n",
    "                inputs = data[0].to(device=device)\n",
    "                age_label = (data[1][\"age\"]).to(device=device)\n",
    "                gender_label = data[1][\"gender\"].to(device=device)\n",
    "                ethn_label = data[1][\"ethnicity\"].to(device=device)\n",
    "                id_label = data[1][\"name\"].to(device=device)\n",
    "                inputs = inputs[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings, _ = adaFace(inputs)\n",
    "                output, compressedOutput = model(embeddings)\n",
    "                # print(output[0].shape)\n",
    "                # print(compressedOutput[0].shape)\n",
    "                # return\n",
    "                compressedOutput = polyprotect(3, compressedOutput[0])\n",
    "                # print(compressedOutput.shape)\n",
    "                output1 = torch.clone(compressedOutput)\n",
    "                id = idModel(output1)\n",
    "                output2 = torch.clone(compressedOutput)\n",
    "                gender = genderModel(output2)\n",
    "                output3 = torch.clone(compressedOutput)\n",
    "                age = ageModel(output3)\n",
    "                output4 = torch.clone(compressedOutput)\n",
    "                eth = ethModel(output4)\n",
    "                # gender, age, ethn, id = self(new_output)\n",
    "\n",
    "                idLoss = identityLoss(id, id_label) \n",
    "                ageLoss = age_loss(age, age_label) \n",
    "                genLoss =  gender_loss(gender, gender_label)\n",
    "                ethLoss = ethn_loss(eth, ethn_label) \n",
    "                \n",
    "                IdOptimizer.zero_grad() \n",
    "                AgeOptimizer.zero_grad() \n",
    "                GenderOptimizer.zero_grad() \n",
    "                EthOptimizer.zero_grad()\n",
    "\n",
    "                idLoss.backward(retain_graph = True)\n",
    "                ageLoss.backward(retain_graph = True)\n",
    "                genLoss.backward(retain_graph = True)\n",
    "                ethLoss.backward(retain_graph = True) \n",
    "                \n",
    "                IdOptimizer.step() \n",
    "                AgeOptimizer.step() \n",
    "                GenderOptimizer.step() \n",
    "                EthOptimizer.step()\n",
    "                \n",
    "                total_gender_loss += genLoss.item()\n",
    "                total_age_loss += ageLoss.item()\n",
    "                total_ethn_loss += ethLoss.item()\n",
    "                total_id_loss += idLoss.item()\n",
    "            print(f\"Epoch [{ep + 1}/{episodes}], Id Loss: {total_id_loss}, Gender Loss: {total_gender_loss}, Age Loss: {total_age_loss}, Eth Loss: {total_ethn_loss} \")\n",
    "        print(\"Training done!\")\n",
    "\n",
    "        # self.eval()\n",
    "        count = 0\n",
    "        genderAcc = 0\n",
    "        ageAcc = 0\n",
    "        ethnAcc = 0\n",
    "        idAcc = 0\n",
    "        for i, data in enumerate(testloader):\n",
    "\n",
    "            inputs = data[0].to(device=device)\n",
    "            age_label = (data[1][\"age\"]).to(device=device)\n",
    "            gender_label = data[1][\"gender\"].to(device=device)\n",
    "            ethn_label = data[1][\"ethnicity\"].to(device=device)\n",
    "            id_label = data[1][\"name\"].to(device = device)\n",
    "            inputs = inputs[:, [2, 1, 0], :, :]\n",
    "            #print(inputs.shape)\n",
    "            embeddings, _ = adaFace(inputs)\n",
    "            output, compressedOutput = model(embeddings)\n",
    "            compressedOutput = polyprotect(3, compressedOutput[0])\n",
    "            # gender, age, ethn, id = self(compressedOutput[0])\n",
    "\n",
    "            output1 = torch.clone(compressedOutput)\n",
    "            id = idModel(output1)\n",
    "            output2 = torch.clone(compressedOutput)\n",
    "            gender = genderModel(output2)\n",
    "            output3 = torch.clone(compressedOutput)\n",
    "            age = ageModel(output3)\n",
    "            output4 = torch.clone(compressedOutput)\n",
    "            eth = ethModel(output4)\n",
    "\n",
    "            predictedGender = torch.argmax(gender, dim = 1)\n",
    "            predictedEthn = torch.argmax(eth, dim = 1)\n",
    "            predictedAge = torch.argmax(age, dim = 1)\n",
    "            predictedId = torch.argmax(id, dim = 1)\n",
    "            for j in range(0, predictedGender.shape[0]):\n",
    "                count = count + 1\n",
    "                if(predictedGender[j].item()==gender_label[j].item()):\n",
    "                    genderAcc = genderAcc + 1\n",
    "            \n",
    "                if(predictedEthn[j].item()==ethn_label[j].item()):\n",
    "                    ethnAcc = ethnAcc + 1\n",
    "\n",
    "                if(predictedAge[j].item() == age_label[j].item()):\n",
    "                    ageAcc = ageAcc + 1\n",
    "                \n",
    "                if(predictedId[j].item() == id_label[j].item()):\n",
    "                    idAcc = idAcc + 1\n",
    "\n",
    "        print(\"Id Accuracy : \", idAcc / count, \"Gender Accuracy : \", genderAcc / count, \" Age Acc : \", ageAcc / count, \" ethnAcc : \", ethnAcc/count)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Backbone(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=64)\n",
       "  )\n",
       "  (output_layer): Sequential(\n",
       "    (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.4, inplace=False)\n",
       "    (2): Flatten()\n",
       "    (3): Linear(in_features=25088, out_features=512, bias=True)\n",
       "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (0): BasicBlockIR(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): PReLU(num_parameters=64)\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlockIR(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): PReLU(num_parameters=64)\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlockIR(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): PReLU(num_parameters=128)\n",
       "        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlockIR(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): PReLU(num_parameters=128)\n",
       "        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): BasicBlockIR(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): PReLU(num_parameters=256)\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): BasicBlockIR(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): PReLU(num_parameters=256)\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): BasicBlockIR(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): PReLU(num_parameters=512)\n",
       "        (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): BasicBlockIR(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): PReLU(num_parameters=512)\n",
       "        (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "FAmodel=faceAnalytics()\n",
    "# nesting_list = [64, 128, 256]\n",
    "# num_classes = 80\n",
    "compression_size = 64\n",
    "# MRL_model = MRL_Model(compression_size)\n",
    "MRL_model = torch.load('/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/FGPaper/Code/MRL/MRL_CelebSet_512x64_AdaFace_20.pt')\n",
    "MRL_Loss = Matryoshka_CE_Loss()\n",
    "FAmodel.to(device)\n",
    "MRL_model.to(device)\n",
    "adaFaceModel.to(device)\n",
    "adaFaceModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAmodel.trainMRL(compression_size, trainloader, adaFaceModel, MRL_model, MRL_Loss, device, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Id Loss: 4228.504455263726, Gender Loss: 767.5094219833845, Age Loss: 2402.765411056578, Eth Loss: 365.16578882593603 \n",
      "Epoch [2/5], Id Loss: 1893.3193493662257, Gender Loss: 509.0271213449887, Age Loss: 1712.691794604063, Eth Loss: 213.1162373920979 \n",
      "Epoch [3/5], Id Loss: 1429.3192988506853, Gender Loss: 449.7805025935304, Age Loss: 1413.558504840359, Eth Loss: 163.47917260463691 \n",
      "Epoch [4/5], Id Loss: 1185.1744020236274, Gender Loss: 419.4918862645427, Age Loss: 1190.1566150896251, Eth Loss: 136.09744303844462 \n",
      "Epoch [5/5], Id Loss: 1061.8027829648954, Gender Loss: 401.0982089775425, Age Loss: 1040.438982438296, Eth Loss: 125.57719609562919 \n",
      "Training done!\n",
      "Id Accuracy :  0.93625 Gender Accuracy :  0.968125  Age Acc :  0.89125  ethnAcc :  0.9825\n"
     ]
    }
   ],
   "source": [
    "m = 4\n",
    "# overlap = 3\n",
    "polyProtectSize = compression_size - m + 1\n",
    "idModel = IdentityDet(polyProtectSize)\n",
    "idModel.to(device)\n",
    "ageModel = AgeDet(polyProtectSize)\n",
    "ageModel.to(device)\n",
    "genderModel = GenderDet(polyProtectSize)\n",
    "genderModel.to(device)\n",
    "ethModel = EthnicityDet(polyProtectSize)\n",
    "ethModel.to(device)\n",
    "FAmodel.testMRL(trainloader, testloader, adaFaceModel, MRL_model, idModel, genderModel, ageModel, ethModel, device, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_outputs_list = []\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs = data[0].to(device=device)\n",
    "    inputs = inputs[:, [2, 1, 0], :, :]\n",
    "    #print(inputs.shape)\n",
    "    embeddings, _ = adaFaceModel(inputs)\n",
    "    output, compressedOutput = MRL_model(embeddings)\n",
    "    compressedOutput = polyprotect(3, compressedOutput[0])\n",
    "    compressed_output_np = compressedOutput.detach().cpu().numpy()\n",
    "    compressed_outputs_list.append(compressed_output_np)\n",
    "\n",
    "for i, data in enumerate(testloader):\n",
    "    inputs = data[0].to(device=device)\n",
    "    inputs = inputs[:, [2, 1, 0], :, :]\n",
    "    #print(inputs.shape)\n",
    "    embeddings, _ = adaFaceModel(inputs)\n",
    "    output, compressedOutput = MRL_model(embeddings)\n",
    "    compressedOutput = polyprotect(3, compressedOutput[0])\n",
    "    compressed_output_np = compressedOutput.detach().cpu().numpy()\n",
    "    compressed_outputs_list.append(compressed_output_np)\n",
    "\n",
    "\n",
    "combined_compressed_output = np.concatenate(compressed_outputs_list, axis=0)\n",
    "\n",
    "file_path_combined = 'MRL+PP_AdaFace_Celebset_output.txt'\n",
    "\n",
    "# Write the combined array to a text file with floating-point format\n",
    "np.savetxt(file_path_combined, combined_compressed_output, fmt='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_outputs_list = []\n",
    "# for i, data in enumerate(trainloader):\n",
    "#     inputs = data[0].to(device=device)\n",
    "#     inputs = inputs[:, [2, 1, 0], :, :]\n",
    "#     #print(inputs.shape)\n",
    "#     embeddings, _ = adaFaceModel(inputs)\n",
    "#     # output, compressedOutput = MRL_model(embeddings)\n",
    "#     compressedOutput = polyprotect(3, embeddings)\n",
    "#     compressed_output_np = compressedOutput.detach().cpu().numpy()\n",
    "#     compressed_outputs_list.append(compressed_output_np)\n",
    "\n",
    "# for i, data in enumerate(testloader):\n",
    "#     inputs = data[0].to(device=device)\n",
    "#     inputs = inputs[:, [2, 1, 0], :, :]\n",
    "#     #print(inputs.shape)\n",
    "#     embeddings, _ = adaFaceModel(inputs)\n",
    "#     # output, compressedOutput = MRL_model(embeddings)\n",
    "#     compressedOutput = polyprotect(3, embeddings)\n",
    "#     compressed_output_np = compressedOutput.detach().cpu().numpy()\n",
    "#     compressed_outputs_list.append(compressed_output_np)\n",
    "\n",
    "\n",
    "# combined_compressed_output = np.concatenate(compressed_outputs_list, axis=0)\n",
    "\n",
    "# file_path_combined = 'PP_AdaFace_Celebset_output.txt'\n",
    "\n",
    "# # Write the combined array to a text file with floating-point format\n",
    "# np.savetxt(file_path_combined, combined_compressed_output, fmt='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ignore Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "processedTensor = torch.zeros((len(testSet), 128))\n",
    "identityLabel = torch.zeros((len(testSet)))\n",
    "count = 0\n",
    "\n",
    "for i,data in enumerate(testloader):\n",
    "    \n",
    "    inputs = data[0].to(device=device)\n",
    "    iden_label = data[1]['name'].to(device=device)\n",
    "    inputs = inputs[:, [2, 1, 0], :, :]    \n",
    "    embeddings,_ = adaFaceModel(inputs)\n",
    "    encoded, _ = aeModel(embeddings)\n",
    "    # compressed_img1 = compressionModel1(embeddings)\n",
    "    # compressed_img2 = compressionModel2(compressed_img1)\n",
    "    # compressed = combinedModel(embeddings)\n",
    "    processedTensor[count : count + embeddings.shape[0]] = torch.tensor(encoded)\n",
    "    identityLabel[count:count + iden_label.shape[0]] = torch.tensor(iden_label)\n",
    "\n",
    "\n",
    "    count = count + embeddings.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8325\n"
     ]
    }
   ],
   "source": [
    "identityAccuracy = 0\n",
    "\n",
    "count = 0\n",
    "\n",
    "while(count < processedTensor.shape[0]):\n",
    "\n",
    "    currentFace = processedTensor[count : count + 1]\n",
    "    cosine_similarity = nn.functional.cosine_similarity(currentFace, processedTensor)\n",
    "    if(identityLabel[torch.topk(cosine_similarity, k = 2)[1][1]].item() == identityLabel[count].item()):\n",
    "        identityAccuracy = identityAccuracy +1\n",
    "    count = count + 1\n",
    "    # print(count)\n",
    "print(identityAccuracy/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAmodel.trainCompressionModel(trainloader,testloader,adaFaceModel,compressionModel, compressionLoss, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetDet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv', get_pairs=False, transform=transform)\n",
    "trainloaderDet = DataLoader(trainSetDet, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "testSetDet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/final_clebSET_test.csv', get_pairs=False, transform=transform)\n",
    "testloaderDet = DataLoader(testSetDet, batch_size=16, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Accuracy: 0.9623919084766325 Age Acc: 0.6096150319513606  ethnAcc: 0.9649875156935491\n",
      "Test Gender Accuracy: 0.934375 Test Age Accuracy: 0.571875 Test ethnicity Accuracy: 0.925625\n",
      "Gender Accuracy: 0.969106631494308 Age Acc: 0.5989786849863872  ethnAcc: 0.9704608613466124\n",
      "Gender Accuracy: 0.971490640296802 Age Acc: 0.5989786849863872  ethnAcc: 0.97325396041699\n",
      "Test Gender Accuracy: 0.954375 Test Age Accuracy: 0.571875 Test ethnicity Accuracy: 0.921875\n",
      "Gender Accuracy: 0.9732398538560284 Age Acc: 0.5989786849863872  ethnAcc: 0.9745235509035253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb Cell 27\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m FAmodel\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m aeModel\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m trainingAcc \u001b[39m=\u001b[39m FAmodel\u001b[39m.\u001b[39;49mtrainModel(trainloader,testloader,adaFaceModel,aeModel, device,\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=369'>370</a>\u001b[0m loss \u001b[39m=\u001b[39m gender_loss(gender,gender_label) \u001b[39m+\u001b[39m ageLoss \u001b[39m+\u001b[39m ethn_loss(ethn,ethn_label)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=370'>371</a>\u001b[0m \u001b[39m# loss = gender_loss(gender,gender_label) + ageLoss + ethn_loss(ethn,ethn_label)  + identityLoss(ident,ident_label)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=371'>372</a>\u001b[0m \u001b[39m#print(gender)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=372'>373</a>\u001b[0m \u001b[39m#print(gender_label)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=373'>374</a>\u001b[0m \u001b[39m#totalGenderLoss = totalGenderLoss + loss.item()\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=374'>375</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=375'>376</a>\u001b[0m \u001b[39m#print(\"Loss:\",loss)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=376'>377</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/face_analytics2/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/face_analytics2/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aeModel = torch.load('/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/AeModel_20_bs32_82acc_best.pt')\n",
    "aeModel.to(device)\n",
    "FAmodel=faceAnalytics()\n",
    "FAmodel.to(device)\n",
    "aeModel.eval()\n",
    "trainingAcc = FAmodel.trainModel(trainloader,testloader,adaFaceModel,aeModel, device,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAmodel.test(testloaderDet,adaFaceModel, compressionModel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"arcFaceCelebSetBase.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"negGenderBaseWiki.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainingAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"model92.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "input1=[]\n",
    "for i,data in enumerate(val_dataloader):\n",
    "    \n",
    "    if(count==0):\n",
    "     inputs=resnet(data[\"image\"].to(device))\n",
    "\n",
    " \n",
    "     input1 = polyprotect(0,inputs[0])\n",
    "\n",
    "     break\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    print(param[0])\n",
    "    print(torch.dot(input1,param[1]))\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory=\"\"\n",
    "file_name = \"input1.txt\"\n",
    "\n",
    "with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in input1:\n",
    "            file.write(f\"{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=faceAnalytics()\n",
    "model=torch.load(\"/home/csgrad/byalavar/FHE/HEAAN/modelUsing0.pt\")\n",
    "model.to(device)\n",
    "model.test(dataloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"modelUsing0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    count=count+1\n",
    "    if(count==2):\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=nn.Linear(4,2)\n",
    "input1=torch.rand((1,4))\n",
    "print(\"input1\",input1)\n",
    "for param in a.parameters():\n",
    "    print(\"param\",param)\n",
    "print(a(input1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=faceAnalytics()\n",
    "model=torch.load(\"modelUsing0.pt\")\n",
    "model.to(device)\n",
    "count=0\n",
    "ageBias=[]\n",
    "for param in model.parameters(): \n",
    "    print(param.shape)\n",
    "    if(count==7):\n",
    "       ageBias = param.tolist()\n",
    "    count=count+1\n",
    "print(len(ageBias),len(ageBias[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists with shape (512, 128)\n",
    "\n",
    "\n",
    "# Define the directory where you want to save the text files\n",
    "output_directory = \"ageWeights\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Write each list to a separate text file\n",
    "\n",
    "\n",
    "# Write each list to a separate text file\n",
    "for i, sublist in enumerate(ageWeights):\n",
    "    # Define the file name with leading zeros\n",
    "    file_name = f\"{i:03d}.txt\"\n",
    "\n",
    "    # Write each value in the sublist on a new line\n",
    "    with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in sublist:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "print(\"Files saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "layer1Bias=[]\n",
    "for param in model.parameters(): \n",
    "    print(param.shape)\n",
    "    if(count==2):\n",
    "       layer1Bias = param.tolist()\n",
    "       break\n",
    "    count=count+1\n",
    "print(len(layer1Bias),len(layer1Bias[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory=\"\"\n",
    "file_name = \"ageBias.txt\"\n",
    "\n",
    "with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in ageBias:\n",
    "            file.write(f\"{value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
