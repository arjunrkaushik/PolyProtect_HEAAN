{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet_pytorch in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (2.5.3)\n",
      "Requirement already satisfied: numpy in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from facenet_pytorch) (1.26.0)\n",
      "Requirement already satisfied: requests in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from facenet_pytorch) (2.31.0)\n",
      "Requirement already satisfied: torchvision in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from facenet_pytorch) (0.16.0)\n",
      "Requirement already satisfied: pillow in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from facenet_pytorch) (10.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from requests->facenet_pytorch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from requests->facenet_pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from requests->facenet_pytorch) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from requests->facenet_pytorch) (2023.7.22)\n",
      "Requirement already satisfied: torch in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torchvision->facenet_pytorch) (2.1.0)\n",
      "Requirement already satisfied: filelock in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from jinja2->torch->torchvision->facenet_pytorch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from sympy->torch->torchvision->facenet_pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import os\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# from tensorflow.keras.models import Model\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.gender_mapping = {'male': 0, 'female': 1}\n",
    "        self.ethnicity_mapping = {'white': 0, 'black': 1, 'asian': 2, 'hispanic': 3}\n",
    "        self.data['nameNum'] = self.data['name'].astype('category').cat.codes\n",
    "        self.data['nameNum'] = self.data['nameNum'].astype(int)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    \n",
    "    def getAgeLabel(self,value1):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(15, 22),(22,40),(40,60),(60,80)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        if(class_ranges[0][0]<=value1 and value1<class_ranges[0][1]):\n",
    "            return 0\n",
    "        elif(class_ranges[1][0]<=value1 and value1<class_ranges[1][1]):\n",
    "            return 1\n",
    "        elif(class_ranges[2][0]<=value1 and value1<class_ranges[2][1]):\n",
    "            return 2\n",
    "        elif(class_ranges[3][0]<=value1 and value1<class_ranges[3][1]):\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = '/home/csgrad/byalavar/FHE/celebSet/CELEBTEST/CELEBTEST/'+row['name']+'/' + row['filename']  # Assuming images are in a folder named 'images'\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "        except Exception as e:\n",
    "            # Handle the error, for example, you can return a placeholder image\n",
    "            print(\"here\")\n",
    "            #self.__getitem__(idx + 1)\n",
    "            #image = Image.new('RGB', (224, 224))  # Create a blank image\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        age = row['age']\n",
    "        \n",
    "        if(row['age']<=0):\n",
    "            age=35\n",
    "        label = {\n",
    "            'age': self.getAgeLabel(age),\n",
    "            'gender': self.gender_mapping.get(row['gender'], 0),  # -1 for unknown\n",
    "            'ethnicity': self.ethnicity_mapping.get(row['ethnicity'], 0),\n",
    "            'age1':age,\n",
    "            'name': row['nameNum']\n",
    "        \n",
    "        }\n",
    "        #print(row['name'])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv'  # Replace with the actual path to your CSV file\n",
    "# df = pd.read_csv(csv_file)\n",
    "\n",
    "# # Create a list to store the indices of rows with missing files\n",
    "# rows_to_remove = []\n",
    "# count=0\n",
    "# # Iterate through the DataFrame and check if the files exist\n",
    "# for index, row in df.iterrows():\n",
    "#     image_path = '/home/csgrad/byalavar/FHE/celebSet/CELEBTEST/CELEBTEST/'+row['name']+'/' + row['filename'] \n",
    "#     if not os.path.exists(image_path):\n",
    "#         rows_to_remove.append(index)\n",
    "#         count=count+1\n",
    "# df = df.drop(rows_to_remove)\n",
    "# df.to_csv(csv_file, index=False)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor(),          # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv', transform=transform)\n",
    "trainloader = DataLoader(trainSet, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "testSet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/final_clebSET_test.csv', transform=transform)\n",
    "testloader = DataLoader(testSet, batch_size=16, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempIter = iter(testloader)\n",
    "# images,image_pairs = next(tempIter)\n",
    "# # imshow(images[0])\n",
    "# # print(labels['age'][0],labels['gender'][0],labels['ethnicity'][0])\n",
    "# # print(image_pairs)\n",
    "# imshow(images[0])\n",
    "# imshow(image_pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout, MaxPool2d, \\\n",
    "    AdaptiveAvgPool2d, Sequential, Module\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Support: ['IR_50', 'IR_101', 'IR_152', 'IR_SE_50', 'IR_SE_101', 'IR_SE_152']\n",
    "\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "def l2_norm(input, axis=1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class SEModule(Module):\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = Conv2d(\n",
    "            channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight.data)\n",
    "\n",
    "        self.relu = ReLU(inplace=True)\n",
    "        self.fc2 = Conv2d(\n",
    "            channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "class bottleneck_IR(Module):\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(bottleneck_IR, self).__init__()\n",
    "        if in_channel == depth:\n",
    "            self.shortcut_layer = MaxPool2d(1, stride)\n",
    "        else:\n",
    "            self.shortcut_layer = Sequential(\n",
    "                Conv2d(in_channel, depth, (1, 1), stride, bias=False), BatchNorm2d(depth))\n",
    "        self.res_layer = Sequential(\n",
    "            BatchNorm2d(in_channel),\n",
    "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth),\n",
    "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut_layer(x)\n",
    "        res = self.res_layer(x)\n",
    "\n",
    "        return res + shortcut\n",
    "\n",
    "\n",
    "class bottleneck_IR_SE(Module):\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(bottleneck_IR_SE, self).__init__()\n",
    "        if in_channel == depth:\n",
    "            self.shortcut_layer = MaxPool2d(1, stride)\n",
    "        else:\n",
    "            self.shortcut_layer = Sequential(\n",
    "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
    "                BatchNorm2d(depth))\n",
    "        self.res_layer = Sequential(\n",
    "            BatchNorm2d(in_channel),\n",
    "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n",
    "            PReLU(depth),\n",
    "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False),\n",
    "            BatchNorm2d(depth),\n",
    "            SEModule(depth, 16)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut_layer(x)\n",
    "        res = self.res_layer(x)\n",
    "\n",
    "        return res + shortcut\n",
    "\n",
    "\n",
    "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
    "    '''A named tuple describing a ResNet block.'''\n",
    "\n",
    "\n",
    "def get_block(in_channel, depth, num_units, stride=2):\n",
    "\n",
    "    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
    "\n",
    "\n",
    "def get_blocks(num_layers):\n",
    "    if num_layers == 50:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=4),\n",
    "            get_block(in_channel=128, depth=256, num_units=14),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 100:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=13),\n",
    "            get_block(in_channel=128, depth=256, num_units=30),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 152:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=8),\n",
    "            get_block(in_channel=128, depth=256, num_units=36),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "class Backbone(Module):\n",
    "    def __init__(self, input_size, num_layers, mode='ir'):\n",
    "        super(Backbone, self).__init__()\n",
    "        assert input_size[0] in [112, 224], \"input_size should be [112, 112] or [224, 224]\"\n",
    "        assert num_layers in [50, 100, 152], \"num_layers should be 50, 100 or 152\"\n",
    "        assert mode in ['ir', 'ir_se'], \"mode should be ir or ir_se\"\n",
    "        blocks = get_blocks(num_layers)\n",
    "        if mode == 'ir':\n",
    "            unit_module = bottleneck_IR\n",
    "        elif mode == 'ir_se':\n",
    "            unit_module = bottleneck_IR_SE\n",
    "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
    "                                      BatchNorm2d(64),\n",
    "                                      PReLU(64))\n",
    "        if input_size[0] == 112:\n",
    "            self.output_layer = Sequential(BatchNorm2d(512),\n",
    "                                           Dropout(),\n",
    "                                           Flatten(),\n",
    "                                           Linear(512 * 7 * 7, 512),\n",
    "                                           BatchNorm1d(512))\n",
    "        else:\n",
    "            self.output_layer = Sequential(BatchNorm2d(512),\n",
    "                                           Dropout(),\n",
    "                                           Flatten(),\n",
    "                                           Linear(512 * 14 * 14, 512),\n",
    "                                           BatchNorm1d(512))\n",
    "\n",
    "        modules = []\n",
    "        for block in blocks:\n",
    "            for bottleneck in block:\n",
    "                modules.append(\n",
    "                    unit_module(bottleneck.in_channel,\n",
    "                                bottleneck.depth,\n",
    "                                bottleneck.stride))\n",
    "        self.body = Sequential(*modules)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.body(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def IR_50(input_size):\n",
    "    \"\"\"Constructs a ir-50 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 50, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_101(input_size):\n",
    "    \"\"\"Constructs a ir-101 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 100, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_152(input_size):\n",
    "    \"\"\"Constructs a ir-152 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 152, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_50(input_size):\n",
    "    \"\"\"Constructs a ir_se-50 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 50, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_101(input_size):\n",
    "    \"\"\"Constructs a ir_se-101 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 100, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_152(input_size):\n",
    "    \"\"\"Constructs a ir_se-152 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 152, 'ir_se')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcFaceModel = IR_50([112,112])\n",
    "arcFaceModel.load_state_dict(torch.load(\"/home/csgrad/byalavar/FHE/HEAAN/FA_CVPR_Exp/backbone_ir50_ms1m_epoch120.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet.data['age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_age_value(original_age_value):\n",
    "    return (original_age_value - trainSet.data['age'].min())/(trainSet.data['age'].max() - trainSet.data['age'].min())\n",
    "\n",
    "\n",
    "def get_original_age_value(normalized_age_value):\n",
    "    return normalized_age_value * (trainSet.data['age'].max()  - trainSet.data['age'].min()) + trainSet.data['age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import MSELoss\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "class CompressionModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompressionModel1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "class CompressionModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompressionModel2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        x = self.model2(x)\n",
    "        return x\n",
    "\n",
    "class OgId(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OgId, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,80)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class CompId(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompId, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,80)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CompressionLoss(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(CompressionLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, img, genuine_imgs, imposter_imgs, genuine_sim, imposter_sim):\n",
    "        # img -> base image embedding\n",
    "        # genuine_imgs, imposter_imgs -> Compressed embeddings of dimension 128\n",
    "        # genuine_sim, imposter_sim -> cosine smilairty scores of uncompressed embeddings of dimension 512\n",
    "\n",
    "\n",
    "        # Compute cosine similarity loss\n",
    "        genuine_loss = 0\n",
    "        for i in range(len(genuine_imgs)):\n",
    "            sim_score = F.cosine_similarity(img, genuine_imgs[i])\n",
    "            genuine_loss += torch.mean(torch.abs(sim_score - genuine_sim[i].view(-1)))\n",
    "\n",
    "        imp_loss = 0\n",
    "        for i in range(len(imposter_imgs)):\n",
    "            imp_sim_score = F.cosine_similarity(img, imposter_imgs[i])\n",
    "            imp_loss += torch.mean(torch.abs(imp_sim_score - imposter_sim[i].view(-1)))\n",
    "\n",
    "        \n",
    "        # Compute Covariance Loss\n",
    "        # matrix -> Concatenates compressed genuine and imposter embeddings\n",
    "        \n",
    "        matrix = torch.cat([genuine_imgs[0], imposter_imgs[0]], dim=0)\n",
    "        for i in range(1, len(genuine_imgs)):\n",
    "            matrix = torch.cat([matrix, genuine_imgs[i]], dim = 0)\n",
    "            matrix = torch.cat([matrix, imposter_imgs[i]], dim = 0)\n",
    "\n",
    "\n",
    "        mean_matrix = torch.mean(matrix, dim = 0)\n",
    "        mx = torch.matmul(mean_matrix.t(), mean_matrix)\n",
    "        vx = torch.matmul(matrix.t(), matrix) / self.batch_size # Dividing by batch size as done here https://github.com/human-analysis/hers-encrypted-image-search/blob/master/deep_mds%2B%2B/nntools/tensorflow/networks/deepmds.py#L94\n",
    "        cov_matrix = mx - vx\n",
    "        diag = torch.diag(cov_matrix.diag())\n",
    "        cov_loss = torch.mean(torch.abs(cov_matrix - diag))\n",
    "        \n",
    "        # Compute Supervise Loss - Not used for ArcFace\n",
    "        # # g_dist -> Concatenates compressed genuine embeddings\n",
    "        # g_dist = torch.cat([genuine_imgs[0], genuine_imgs[1]], dim = 0)\n",
    "        # for i in range(2, len(genuine_imgs)):\n",
    "        #     g_dist = torch.cat([g_dist, genuine_imgs[i]], dim = 0)\n",
    "\n",
    "        # # g_dist -> Concatenates compressed imposter embeddings\n",
    "        # imp_dist = torch.cat([imposter_imgs[0], imposter_imgs[1]], dim = 0)\n",
    "        # for i in range(2, len(imposter_imgs)):\n",
    "        #     imp_dist = torch.cat([imp_dist, imposter_imgs[i]], dim = 0)\n",
    "\n",
    "        # gloss = (torch.mean(g_dist) + 1.0) * 0.5\n",
    "        # iloss = (torch.mean(imp_dist) + 1.0) * 0.5\n",
    "\n",
    "        # lda_term = iloss / gloss\n",
    "        # return genuine_loss + imp_loss + 10 * cov_loss \n",
    "        return 10*genuine_loss + imp_loss + cov_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class faceAnalytics(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1=nn.Linear(128,64)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.layer2=nn.Linear(64,32)\n",
    "        #self.layer3=nn.Linear(1024,512)\n",
    "        # self.layer4=nn.Linear(128,64)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        self.genderOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "        self.ageOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,4)\n",
    "        )\n",
    "        self.ethnicityOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "        # self.identity = nn.Sequential(nn.Linear(32,64), nn.ReLU(), nn.Linear(64,80))\n",
    "        # self.maxVal = 0\n",
    "        # self.min=0\n",
    "        \n",
    "    \n",
    "    def writeResult(self,result):\n",
    "       output_directory=\"\"\n",
    "       file_name = \"resultAge.txt\"\n",
    "\n",
    "       with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in result:\n",
    "            file.write(f\"{value}\\n\")\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        #print(\"Input\",x[0])\n",
    "        # x=self.layer1(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "        # x=self.layer2(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "\n",
    "        gender=self.genderOut(x)\n",
    "        age=self.ageOut(x)\n",
    "        ethn = self.ethnicityOut(x)\n",
    "        # identity = self.identity(x)\n",
    "        return gender,age,ethn\n",
    "    \n",
    "    \n",
    "    def are_values_in_same_class(self,value1, value2):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(15, 22),(22,40),(40,60),(60,80)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        for class_range in class_ranges:\n",
    "            if class_range[0] <= value1 < class_range[1] and class_range[0] <= value2 < class_range[1]:\n",
    "                return True\n",
    "    \n",
    "        return False    \n",
    "    \n",
    "    def trainAE(self, trainloader, testloader, arcFace, aeModel,OgIdLoss, CompIdLoss, device, episodes, completed_eps=0):\n",
    "        input_dim = 512\n",
    "        compressed_dim = 128\n",
    "        model = aeModel\n",
    "        # Define optimizer and loss function\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        MSELoss = nn.MSELoss().to(device)\n",
    "        celoss = nn.CrossEntropyLoss().to(device)\n",
    "        compLoss = CompIdLoss\n",
    "        ogLoss = OgIdLoss  \n",
    "        model.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "            cnt = 0\n",
    "\n",
    "            for img, data in trainloader:\n",
    "                cnt += 1\n",
    "                if cnt%50 == 0:\n",
    "                    print(\"Image number = \" + str(cnt))\n",
    "                img = img.to(device=device)\n",
    "                label = data['name'].to(device=device)\n",
    "                # input = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings = arcFace(img)\n",
    "                encoded, decoded = model(embeddings)\n",
    "                x1 = compLoss(encoded) \n",
    "                x2 = ogLoss(decoded)\n",
    "                loss = celoss(x1, label) + celoss(x2, label)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (ep+1) % 5 == 0:\n",
    "                torch.save(model, f'AeModel_ArcFace_{ep+1+completed_eps}.pt')\n",
    "            print(f\"Epoch [{ep+completed_eps + 1}/{episodes+completed_eps}], Loss: {total_loss}\")\n",
    "        torch.save(model, f'AeModel_ArcFace_{episodes+completed_eps}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "\n",
    "    def trainCombinedModel(self, trainloader, testloader, adaFace, combinedModel, compressionLoss, device, episodes, completed_eps=0):\n",
    "        input_dim = 512\n",
    "        compressed_dim = 128\n",
    "        model = combinedModel\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=4e-5)\n",
    "        criterion = compressionLoss\n",
    "        model.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "            cnt = 0\n",
    "            for img, gen_pairs, imp_pairs in trainloader:\n",
    "\n",
    "                # print(img.shape)\n",
    "                # print(image_pairs[0].shape)\n",
    "                # print(img)\n",
    "                # print(len(image_pairs))\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt%50 == 0:\n",
    "                    print(\"Image number = \" + str(cnt))\n",
    "                \n",
    "\n",
    "                img = img.to(device=device)\n",
    "                input = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings,_ = adaFace(input)\n",
    "                compressed_img = model(embeddings)\n",
    "                gen_sim_scores = []\n",
    "                gen_embedding_pairs = []\n",
    "                for img_pair in gen_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compressed_img2 = model(embedding_pair)\n",
    "                    gen_embedding_pairs.append(compressed_img2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    gen_sim_scores.append(sim_score)\n",
    "                \n",
    "                imp_sim_scores = []\n",
    "                imp_embedding_pairs = []\n",
    "                for img_pair in imp_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compressed_img2 = model(embedding_pair)\n",
    "                    imp_embedding_pairs.append(compressed_img2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    imp_sim_scores.append(sim_score)\n",
    "                \n",
    "                loss = criterion(compressed_img, gen_embedding_pairs, imp_embedding_pairs, gen_sim_scores, imp_sim_scores)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (ep+1) % 5 == 0:\n",
    "                torch.save(model, f'CombinedModel_{ep+1+completed_eps}.pt')\n",
    "            print(f\"Epoch [{ep+completed_eps + 1}/{episodes+completed_eps}], Loss: {total_loss}\")\n",
    "        torch.save(model, f'CombinedModel_{episodes+completed_eps}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def trainCompressionModel2(self, trainloader, testloader, adaFace, compressionModel1, compressionModel2, compressionLoss, device, episodes, completed_eps = 0):\n",
    "        \n",
    "        # Define optimizer and loss function\n",
    "        compressionModel1.eval()\n",
    "        optimizer = optim.Adam(compressionModel2.parameters(), lr=0.001, weight_decay=4e-5)\n",
    "        criterion = compressionLoss\n",
    "        compressionModel2.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "            cnt = 0\n",
    "            for img, gen_pairs, imp_pairs in trainloader:\n",
    "\n",
    "                # print(img.shape)\n",
    "                # print(image_pairs[0].shape)\n",
    "                # print(img)\n",
    "                # print(len(image_pairs))\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt%50 == 0:\n",
    "                    print(\"Image number = \" + str(cnt))\n",
    "                \n",
    "\n",
    "                img = img.to(device=device)\n",
    "                input = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings,_ = adaFace(input)\n",
    "                compressed_img = compressionModel1(embeddings)\n",
    "                compressed_img2 = compressionModel2(compressed_img)\n",
    "                gen_sim_scores = []\n",
    "                gen_embedding_pairs = []\n",
    "                for img_pair in gen_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compression_pair1 = compressionModel1(embedding_pair)\n",
    "                    compressed_pair2 = compressionModel2(compression_pair1)\n",
    "                    gen_embedding_pairs.append(compressed_pair2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    gen_sim_scores.append(sim_score)\n",
    "                \n",
    "                imp_sim_scores = []\n",
    "                imp_embedding_pairs = []\n",
    "                for img_pair in imp_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compression_pair1 = compressionModel1(embedding_pair)\n",
    "                    compressed_pair2 = compressionModel2(compression_pair1)\n",
    "                    imp_embedding_pairs.append(compressed_pair2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    imp_sim_scores.append(sim_score)\n",
    "                \n",
    "                loss = criterion(compressed_img2, gen_embedding_pairs, imp_embedding_pairs, gen_sim_scores, imp_sim_scores)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (ep + 1) % 5 == 0:\n",
    "                torch.save(compressionModel2, f'CompressionModel2_{ep+1+completed_eps}.pt')\n",
    "            print(f\"Epoch [{completed_eps + ep + 1}/{completed_eps + episodes}], Loss: {total_loss}\")\n",
    "        torch.save(compressionModel2, f'CompressionModel2_{completed_eps + episodes}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def trainCompressionModel1(self, trainloader, testloader, adaFace, compressionModel1, compressionLoss, device, episodes, completed_eps = 0):\n",
    "        input_dim = 512\n",
    "        compressed_dim = 128\n",
    "        model = compressionModel1\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        criterion = compressionLoss\n",
    "        model.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "            cnt = 0\n",
    "            for img, gen_pairs, imp_pairs in trainloader:\n",
    "\n",
    "                # print(img.shape)\n",
    "                # print(image_pairs[0].shape)\n",
    "                # print(img)\n",
    "                # print(len(image_pairs))\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt%50 == 0:\n",
    "                    print(\"Image number = \" + str(cnt))\n",
    "                \n",
    "\n",
    "                img = img.to(device=device)\n",
    "                input = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings,_ = adaFace(input)\n",
    "                compressed_img = model(embeddings)\n",
    "                gen_sim_scores = []\n",
    "                gen_embedding_pairs = []\n",
    "                for img_pair in gen_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compressed_img2 = model(embedding_pair)\n",
    "                    gen_embedding_pairs.append(compressed_img2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    gen_sim_scores.append(sim_score)\n",
    "                \n",
    "                imp_sim_scores = []\n",
    "                imp_embedding_pairs = []\n",
    "                for img_pair in imp_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compressed_img2 = model(embedding_pair)\n",
    "                    imp_embedding_pairs.append(compressed_img2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    imp_sim_scores.append(sim_score)\n",
    "                \n",
    "                loss = criterion(compressed_img, gen_embedding_pairs, imp_embedding_pairs, gen_sim_scores, imp_sim_scores)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (ep+1) % 5 == 0:\n",
    "                torch.save(model, f'CompressionModel1_{ep+1+completed_eps}.pt')\n",
    "            print(f\"Epoch [{ep + 1 + completed_eps}/{completed_eps + episodes}], Loss: {total_loss}\")\n",
    "        torch.save(model, f'CompressionModel1_{episodes+completed_eps}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "\n",
    "    def trainModel(self,trainLoader,testLoader,arcFace, compressionModel, device,episodes):\n",
    "        self.train()\n",
    "        #maxVal = 0\n",
    "        learningRate=0.001\n",
    "        gender_loss = nn.CrossEntropyLoss() \n",
    "        age_loss = nn.CrossEntropyLoss() \n",
    "        ethn_loss = nn.CrossEntropyLoss()\n",
    "        identityLoss = nn.CrossEntropyLoss()\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=learningRate)\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=learningRate)\n",
    "        # momentum=0.9, weight_decay=5e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=episodes)\n",
    "        trainingAcc = []\n",
    "        model = compressionModel\n",
    "        for e in range(0,episodes):\n",
    "            total_training_loss =0\n",
    "            genderAcc=0\n",
    "            ethnAcc = 0\n",
    "            ageAcc=0\n",
    "            identAcc=0\n",
    "            count=0\n",
    "            totalGenderLoss=0\n",
    "\n",
    "\n",
    "            for i,data in enumerate(trainLoader):\n",
    "\n",
    "                #print(inputs.shape)\n",
    "                #inputs = inputs.type(torch.double)\n",
    "                \n",
    "                #gender_label = data[\"gender\"].to(device=device)\n",
    "                #race_label = data[\"race\"].to(device=device)\n",
    "            # print(i)\n",
    "                inputs = data[0].to(device=device)\n",
    "                #inputs = inputs.type(torch.double)\n",
    "                age_label = (data[1][\"age\"]).to(device=device)\n",
    "            \n",
    "                gender_label = data[1][\"gender\"].to(device=device)\n",
    "                ethn_label = data[1][\"ethnicity\"].to(device=device)\n",
    "\n",
    "                ident_label = data[1][\"name\"].to(device=device)\n",
    "                \n",
    "                #print(torch.max(ethn_label),torch.min(ethn_label))\n",
    "                #print(torch.max(gender_label),torch.min(gender_label))\n",
    "\n",
    "                #print(count)\n",
    "                #print(inputs.shape)\n",
    "                # inputs = inputs[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings = arcFace(inputs)\n",
    "                encoded, _ = model(embeddings)\n",
    "                gender,age,ethn = self(encoded)\n",
    "                ageLoss = age_loss(age,age_label)\n",
    "                loss = gender_loss(gender,gender_label) + ageLoss + ethn_loss(ethn,ethn_label)\n",
    "                # loss = gender_loss(gender,gender_label) + ageLoss + ethn_loss(ethn,ethn_label)  + identityLoss(ident,ident_label)\n",
    "                #print(gender)\n",
    "                #print(gender_label)\n",
    "                #totalGenderLoss = totalGenderLoss + loss.item()\n",
    "                loss.backward()\n",
    "                #print(\"Loss:\",loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                #total_training_loss = total_training_loss+loss.item()*512\n",
    "\n",
    "                predictedGender = torch.argmax(gender,dim=1)\n",
    "                predictedEthn = torch.argmax(ethn,dim=1)\n",
    "                predictedAge = torch.argmax(age,dim=1)\n",
    "                # predictedIdent = torch.argmax(ident,dim=1)\n",
    "                #print(predictedGender)\n",
    "                #print(gender_label)\n",
    "                for j in range(0,predictedGender.shape[0]):\n",
    "                    count=count+1\n",
    "                    #print(predictedGender[j].item(),gender_label[j])\n",
    "                    if(predictedGender[j].item()==gender_label[j].item()):\n",
    "                        genderAcc=genderAcc+1\n",
    "                \n",
    "                    if(predictedEthn[j].item()==ethn_label[j].item()):\n",
    "                        ethnAcc=ethnAcc+1\n",
    "    \n",
    "                    if(predictedAge[j].item()==age_label[j].item()):\n",
    "                        ageAcc=ageAcc+1\n",
    "\n",
    "                    # if(predictedIdent[j].item()==ident_label[j].item()):\n",
    "                    #     identAcc=identAcc+1\n",
    "    \n",
    "            genderAccuracy =  genderAcc/count\n",
    "            trainingAcc.append(genderAccuracy)\n",
    "            print(\"Gender Accuracy:\", genderAccuracy,\"Age Acc:\", ageAcc/count, \" ethnAcc:\", ethnAcc/count)\n",
    "            #print(\"total training loss:\",total_training_loss/16595,\"\\n\")\n",
    "            #print(\"\\n\")\n",
    "            scheduler.step()\n",
    "            #print(\"max observed value: \", maxVal)\n",
    "            if(e%2==0):\n",
    "                self.test(testLoader,arcFace, compressionModel, device)\n",
    "            \n",
    "        return trainingAcc\n",
    "\n",
    "    def are_values_in_same_class(self,value1, value2):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(15, 22),(22,40),(40,60),(60,80)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        for class_range in class_ranges:\n",
    "            if class_range[0] <= value1 < class_range[1] and class_range[0] <= value2 < class_range[1]:\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def test(self,testLoader,arcFace, compressionModel, device):\n",
    "\n",
    "         self.eval()\n",
    "         age_loss = nn.L1Loss()\n",
    "\n",
    "         totalAgeError = 0\n",
    "         genderAccuracy = 0\n",
    "         tempAcc=0\n",
    "         count=0\n",
    "         totalGenderLoss=0\n",
    "         maxAge = 0\n",
    "         minAge = 0\n",
    "         ageAccuracy = 0\n",
    "         total_training_loss =0\n",
    "         tempAcc=0\n",
    "         count=0\n",
    "         totalGenderLoss=0\n",
    "\n",
    "         genderAcc=0\n",
    "         ethnAcc = 0\n",
    "         identAcc = 0\n",
    "\n",
    "         model = compressionModel\n",
    "        \n",
    "         for i,data in enumerate(testLoader):\n",
    "\n",
    "            #print(inputs.shape)\n",
    "            #inputs = inputs.type(torch.double)\n",
    "            \n",
    "            #gender_label = data[\"gender\"].to(device=device)\n",
    "            #race_label = data[\"race\"].to(device=device)\n",
    "            \n",
    "   \n",
    "            #inputs = inputs.type(torch.double)\n",
    "            inputs = data[0].to(device=device)\n",
    "            age_label = data[1][\"age\"].to(device=device)\n",
    "            gender_label = data[1][\"gender\"].to(device=device)\n",
    "            ethn_label = data[1][\"ethnicity\"].to(device=device)\n",
    "            ident_label = data[1]['name'].to(device=device)\n",
    "            #print(count)\n",
    "            # inputs = inputs[:, [2, 1, 0], :, :]\n",
    "            embeddings = arcFace(inputs)\n",
    "            compressedEm, _ = model(embeddings)\n",
    "            gender,age,ethn = self(compressedEm)\n",
    "         \n",
    "\n",
    "            predictedGender = torch.argmax(gender,dim=1)\n",
    "            predictedEthn = torch.argmax(ethn,dim=1)\n",
    "            predictedAge = torch.argmax(age,dim=1)\n",
    "            # predictedIdent = torch.argmax(ident,dim=1)\n",
    "            #age = get_original_age_value(age)\n",
    "\n",
    "            for j in range(0,predictedGender.shape[0]):\n",
    "                count=count+1\n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedGender[j].item()==gender_label[j]):\n",
    "                    genderAcc=genderAcc+1\n",
    "                if(predictedEthn[j].item()==ethn_label[j]):\n",
    "                    ethnAcc=ethnAcc+1\n",
    "                if(predictedAge[j].item()==age_label[j]):\n",
    "                   ageAccuracy = ageAccuracy +1\n",
    "                \n",
    "                # if(predictedIdent[j].item()==ident_label[j].item()):\n",
    "                #     identAcc=identAcc+1\n",
    "\n",
    "         genderAccuracy =  genderAcc/count\n",
    "         \n",
    "         print(\"Test Gender Accuracy:\", genderAccuracy,\"Test Age Accuracy:\", ageAccuracy/count, \"Test ethnicity Accuracy:\", ethnAcc/count)\n",
    "\n",
    "         return genderAccuracy,totalAgeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "FAmodel=faceAnalytics()\n",
    "# compressionModel1 = CompressionModel1()\n",
    "# compressionModel2 = CompressionModel2()\n",
    "# compressionLoss = CompressionLoss(batch_size)\n",
    "# combinedModel = CombinedModel(compressionModel1, compressionModel2)\n",
    "aeModel = Autoencoder()\n",
    "#model=torch.load(\"arcFaceCelebSetBase.pt\")\n",
    "FAmodel.to(device)\n",
    "arcFaceModel.to(device)\n",
    "arcFaceModel.eval()\n",
    "# compressionModel1.to(device)\n",
    "# compressionModel2.to(device)\n",
    "# combinedModel.to(device)\n",
    "aeModel.to(device)\n",
    "# compressionLoss = DataParallel(compressionLoss)\n",
    "# compressionLoss.to(device)\n",
    "#torch.save(model,\"bestFaceAn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [1/20], Loss: 32276.581963062286\n",
      "Starting 1\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [2/20], Loss: 27807.523381233215\n",
      "Starting 2\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [3/20], Loss: 26639.550730228424\n",
      "Starting 3\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [4/20], Loss: 25817.52562379837\n",
      "Starting 4\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [5/20], Loss: 25231.266157627106\n",
      "Starting 5\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [6/20], Loss: 24765.43760728836\n",
      "Starting 6\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [7/20], Loss: 24383.846361637115\n",
      "Starting 7\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [8/20], Loss: 24062.87478494644\n",
      "Starting 8\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [9/20], Loss: 23788.44471359253\n",
      "Starting 9\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [10/20], Loss: 23556.78013753891\n",
      "Starting 10\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [11/20], Loss: 23356.596702098846\n",
      "Starting 11\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [12/20], Loss: 23187.401049137115\n",
      "Starting 12\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [13/20], Loss: 23042.57726573944\n",
      "Starting 13\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [14/20], Loss: 22924.49091720581\n",
      "Starting 14\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [15/20], Loss: 22823.259635448456\n",
      "Starting 15\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [16/20], Loss: 22738.50460577011\n",
      "Starting 16\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [17/20], Loss: 22671.423192501068\n",
      "Starting 17\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [18/20], Loss: 22613.947489261627\n",
      "Starting 18\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [19/20], Loss: 22564.978934288025\n",
      "Starting 19\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Image number = 2250\n",
      "Image number = 2300\n",
      "Image number = 2350\n",
      "Image number = 2400\n",
      "Image number = 2450\n",
      "Image number = 2500\n",
      "Image number = 2550\n",
      "Image number = 2600\n",
      "Image number = 2650\n",
      "Image number = 2700\n",
      "Image number = 2750\n",
      "Image number = 2800\n",
      "Image number = 2850\n",
      "Image number = 2900\n",
      "Image number = 2950\n",
      "Image number = 3000\n",
      "Image number = 3050\n",
      "Image number = 3100\n",
      "Image number = 3150\n",
      "Image number = 3200\n",
      "Image number = 3250\n",
      "Image number = 3300\n",
      "Image number = 3350\n",
      "Image number = 3400\n",
      "Image number = 3450\n",
      "Image number = 3500\n",
      "Image number = 3550\n",
      "Image number = 3600\n",
      "Image number = 3650\n",
      "Image number = 3700\n",
      "Image number = 3750\n",
      "Image number = 3800\n",
      "Image number = 3850\n",
      "Image number = 3900\n",
      "Image number = 3950\n",
      "Image number = 4000\n",
      "Image number = 4050\n",
      "Image number = 4100\n",
      "Image number = 4150\n",
      "Image number = 4200\n",
      "Image number = 4250\n",
      "Image number = 4300\n",
      "Image number = 4350\n",
      "Image number = 4400\n",
      "Epoch [20/20], Loss: 22525.84047317505\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "ogLoss = OgId()\n",
    "compLoss = CompId()\n",
    "ogLoss.to(device)\n",
    "compLoss.to(device)\n",
    "FAmodel.trainAE(trainloader, testloader, arcFaceModel, aeModel, ogLoss, compLoss, device, 20, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "processedTensor = torch.zeros((len(testSet), 128))\n",
    "identityLabel = torch.zeros((len(testSet)))\n",
    "count = 0\n",
    "\n",
    "for i,data in enumerate(testloader):\n",
    "    \n",
    "    inputs = data[0].to(device=device)\n",
    "    iden_label = data[1]['name'].to(device=device)\n",
    "    # inputs = inputs[:, [2, 1, 0], :, :]    \n",
    "    embeddings = arcFaceModel(inputs)\n",
    "    encoded, _ = aeModel(embeddings)\n",
    "    # compressed_img1 = compressionModel1(embeddings)\n",
    "    # compressed_img2 = compressionModel2(compressed_img1)\n",
    "    # compressed = combinedModel(embeddings)\n",
    "    processedTensor[count : count + embeddings.shape[0]] = torch.tensor(encoded)\n",
    "    identityLabel[count:count + iden_label.shape[0]] = torch.tensor(iden_label)\n",
    "\n",
    "\n",
    "    count = count + embeddings.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941875\n"
     ]
    }
   ],
   "source": [
    "identityAccuracy = 0\n",
    "\n",
    "count = 0\n",
    "\n",
    "while(count < processedTensor.shape[0]):\n",
    "\n",
    "    currentFace = processedTensor[count : count + 1]\n",
    "    cosine_similarity = nn.functional.cosine_similarity(currentFace, processedTensor)\n",
    "    if(identityLabel[torch.topk(cosine_similarity, k = 2)[1][1]].item() == identityLabel[count].item()):\n",
    "        identityAccuracy = identityAccuracy +1\n",
    "    count = count + 1\n",
    "    # print(count)\n",
    "print(identityAccuracy/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetDet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv', get_pairs=False, transform=transform)\n",
    "trainloaderDet = DataLoader(trainSetDet, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "testSetDet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/final_clebSET_test.csv', get_pairs=False, transform=transform)\n",
    "testloaderDet = DataLoader(testSetDet, batch_size=8, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Accuracy: 0.894003300935265 Age Acc: 0.7199283386703156  ethnAcc: 0.9581458336272201\n",
      "Test Gender Accuracy: 0.93125 Test Age Accuracy: 0.745 Test ethnicity Accuracy: 0.8325\n",
      "Gender Accuracy: 0.9656646306197012 Age Acc: 0.761711972238288  ethnAcc: 0.9725768454908378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb Cell 23\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m FAmodel\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m aeModel\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m trainingAcc \u001b[39m=\u001b[39m FAmodel\u001b[39m.\u001b[39;49mtrainModel(trainloader,testloader,arcFaceModel,aeModel, device,\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=370'>371</a>\u001b[0m loss \u001b[39m=\u001b[39m gender_loss(gender,gender_label) \u001b[39m+\u001b[39m ageLoss \u001b[39m+\u001b[39m ethn_loss(ethn,ethn_label)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=371'>372</a>\u001b[0m \u001b[39m# loss = gender_loss(gender,gender_label) + ageLoss + ethn_loss(ethn,ethn_label)  + identityLoss(ident,ident_label)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=372'>373</a>\u001b[0m \u001b[39m#print(gender)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=373'>374</a>\u001b[0m \u001b[39m#print(gender_label)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=374'>375</a>\u001b[0m \u001b[39m#totalGenderLoss = totalGenderLoss + loss.item()\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=375'>376</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=376'>377</a>\u001b[0m \u001b[39m#print(\"Loss:\",loss)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionArcFaceCelebSet.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=377'>378</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/face_analytics2/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/face_analytics2/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FAmodel=faceAnalytics()\n",
    "FAmodel.to(device)\n",
    "aeModel.eval()\n",
    "trainingAcc = FAmodel.trainModel(trainloader, testloader, arcFaceModel, aeModel, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAmodel.test(testloaderDet,adaFaceModel, compressionModel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"arcFaceCelebSetBase.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"negGenderBaseWiki.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainingAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"model92.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "input1=[]\n",
    "for i,data in enumerate(val_dataloader):\n",
    "    \n",
    "    if(count==0):\n",
    "     inputs=resnet(data[\"image\"].to(device))\n",
    "\n",
    " \n",
    "     input1 = polyprotect(0,inputs[0])\n",
    "\n",
    "     break\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    print(param[0])\n",
    "    print(torch.dot(input1,param[1]))\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory=\"\"\n",
    "file_name = \"input1.txt\"\n",
    "\n",
    "with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in input1:\n",
    "            file.write(f\"{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=faceAnalytics()\n",
    "model=torch.load(\"/home/csgrad/byalavar/FHE/HEAAN/modelUsing0.pt\")\n",
    "model.to(device)\n",
    "model.test(dataloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"modelUsing0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    count=count+1\n",
    "    if(count==2):\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=nn.Linear(4,2)\n",
    "input1=torch.rand((1,4))\n",
    "print(\"input1\",input1)\n",
    "for param in a.parameters():\n",
    "    print(\"param\",param)\n",
    "print(a(input1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=faceAnalytics()\n",
    "model=torch.load(\"modelUsing0.pt\")\n",
    "model.to(device)\n",
    "count=0\n",
    "ageBias=[]\n",
    "for param in model.parameters(): \n",
    "    print(param.shape)\n",
    "    if(count==7):\n",
    "       ageBias = param.tolist()\n",
    "    count=count+1\n",
    "print(len(ageBias),len(ageBias[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists with shape (512, 128)\n",
    "\n",
    "\n",
    "# Define the directory where you want to save the text files\n",
    "output_directory = \"ageWeights\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Write each list to a separate text file\n",
    "\n",
    "\n",
    "# Write each list to a separate text file\n",
    "for i, sublist in enumerate(ageWeights):\n",
    "    # Define the file name with leading zeros\n",
    "    file_name = f\"{i:03d}.txt\"\n",
    "\n",
    "    # Write each value in the sublist on a new line\n",
    "    with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in sublist:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "print(\"Files saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "layer1Bias=[]\n",
    "for param in model.parameters(): \n",
    "    print(param.shape)\n",
    "    if(count==2):\n",
    "       layer1Bias = param.tolist()\n",
    "       break\n",
    "    count=count+1\n",
    "print(len(layer1Bias),len(layer1Bias[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory=\"\"\n",
    "file_name = \"ageBias.txt\"\n",
    "\n",
    "with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in ageBias:\n",
    "            file.write(f\"{value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
