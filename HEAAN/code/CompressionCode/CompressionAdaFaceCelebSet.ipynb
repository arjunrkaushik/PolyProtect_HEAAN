{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet_pytorch in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (2.5.3)\n",
      "Requirement already satisfied: numpy in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from facenet_pytorch) (1.26.0)\n",
      "Requirement already satisfied: requests in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from facenet_pytorch) (2.31.0)\n",
      "Requirement already satisfied: torchvision in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from facenet_pytorch) (0.16.0)\n",
      "Requirement already satisfied: pillow in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from facenet_pytorch) (10.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from requests->facenet_pytorch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from requests->facenet_pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from requests->facenet_pytorch) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from requests->facenet_pytorch) (2023.7.22)\n",
      "Requirement already satisfied: torch in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torchvision->facenet_pytorch) (2.1.0)\n",
      "Requirement already satisfied: filelock in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from torch->torchvision->facenet_pytorch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from jinja2->torch->torchvision->facenet_pytorch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from sympy->torch->torchvision->facenet_pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/csgrad/kaushik3/miniconda3/envs/face_analytics2/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import os\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# from tensorflow.keras.models import Model\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, get_pairs, num_pairs = None, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.gender_mapping = {'male': 0, 'female': 1}\n",
    "        self.ethnicity_mapping = {'white': 0, 'black': 1, 'asian': 2, 'hispanic': 3}\n",
    "        self.data['nameNum'] = self.data['name'].astype('category').cat.codes\n",
    "        self.data['nameNum'] = self.data['nameNum'].astype(int)\n",
    "        self.get_pairs = get_pairs\n",
    "        self.num_pairs = num_pairs\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    \n",
    "    def getAgeLabel(self,value1):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(15, 22),(22,40),(40,60),(60,80)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        if(class_ranges[0][0]<=value1 and value1<class_ranges[0][1]):\n",
    "            return 0\n",
    "        elif(class_ranges[1][0]<=value1 and value1<class_ranges[1][1]):\n",
    "            return 1\n",
    "        elif(class_ranges[2][0]<=value1 and value1<class_ranges[2][1]):\n",
    "            return 2\n",
    "        elif(class_ranges[3][0]<=value1 and value1<class_ranges[3][1]):\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.get_pairs == False:\n",
    "            row = self.data.iloc[idx]\n",
    "            image_path = '/home/csgrad/byalavar/FHE/celebSet/CELEBTEST/CELEBTEST/'+row['name']+'/' + row['filename']  # Assuming images are in a folder named 'images'\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(image_path)\n",
    "            except Exception as e:\n",
    "                # Handle the error, for example, you can return a placeholder image\n",
    "                print(\"here\")\n",
    "                #self.__getitem__(idx + 1)\n",
    "                #image = Image.new('RGB', (224, 224))  # Create a blank image\n",
    "            \n",
    "            image = Image.open(image_path)\n",
    "            age = row['age']\n",
    "            \n",
    "            if(row['age']<=0):\n",
    "                age=35\n",
    "            label = {\n",
    "                'age': self.getAgeLabel(age),\n",
    "                'gender': self.gender_mapping.get(row['gender'], 0),  # -1 for unknown\n",
    "                'ethnicity': self.ethnicity_mapping.get(row['ethnicity'], 0),\n",
    "                'age1':age,\n",
    "                'name' : row['nameNum']\n",
    "            \n",
    "            }\n",
    "            #print(row['name'])\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "        # else\n",
    "        else:\n",
    "            if self.num_pairs == None:\n",
    "                print(\"Num of Pairs not specified.Setting to 50.\")\n",
    "                self.num_pairs = 50\n",
    "            row = self.data.iloc[idx]\n",
    "            image_path = '/home/csgrad/byalavar/FHE/celebSet/CELEBTEST/CELEBTEST/'+row['name']+'/' + row['filename']  # Assuming images are in a folder named 'images'\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(image_path)\n",
    "            except Exception as e:\n",
    "                # Handle the error, for example, you can return a placeholder image\n",
    "                print(\"here\")\n",
    "                #self.__getitem__(idx + 1)\n",
    "                #image = Image.new('RGB', (224, 224))  # Create a blank image\n",
    "            \n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            #print(row['name'])\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            genuine_images = []\n",
    "            imposter_images = []\n",
    "            if idx + self.num_pairs < len(self.data):\n",
    "                st = max(0, idx - self.num_pairs)\n",
    "                end = idx + self.num_pairs\n",
    "            else:\n",
    "                st = idx - self.num_pairs + 1\n",
    "                end = len(self.data)\n",
    "            for i in range(st, end):\n",
    "                row_pair = self.data.iloc[i]\n",
    "                \n",
    "                if row_pair['nameNum'] == row['nameNum']:\n",
    "\n",
    "                    image_path_pair = '/home/csgrad/byalavar/FHE/celebSet/CELEBTEST/CELEBTEST/' + row_pair['name'] + '/' + row_pair['filename']\n",
    "\n",
    "                    image_pair = Image.open(image_path_pair)\n",
    "                    #print(row['name'])\n",
    "                    if self.transform:\n",
    "                        image_pair = self.transform(image_pair)\n",
    "\n",
    "                    \n",
    "                    genuine_images.append(image_pair)\n",
    "                if len(genuine_images) == self.num_pairs:\n",
    "                    break\n",
    "                \n",
    "            i = idx\n",
    "            while True:\n",
    "                i = (i + 1000) % len(self.data)\n",
    "                row_pair = self.data.iloc[i]\n",
    "                \n",
    "                if row_pair['nameNum'] != row['nameNum']:\n",
    "\n",
    "                    image_path_pair = '/home/csgrad/byalavar/FHE/celebSet/CELEBTEST/CELEBTEST/' + row_pair['name'] + '/' + row_pair['filename']\n",
    "\n",
    "                    image_pair = Image.open(image_path_pair)\n",
    "                    #print(row['name'])\n",
    "                    if self.transform:\n",
    "                        image_pair = self.transform(image_pair)\n",
    "\n",
    "                    \n",
    "                    imposter_images.append(image_pair)\n",
    "                if len(imposter_images) == self.num_pairs:\n",
    "                    break\n",
    "            # print(images)\n",
    "            # print(len(genuine_images))\n",
    "            # print(len(imposter_images))\n",
    "            return image, genuine_images, imposter_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv'  # Replace with the actual path to your CSV file\n",
    "# df = pd.read_csv(csv_file)\n",
    "\n",
    "# # Create a list to store the indices of rows with missing files\n",
    "# rows_to_remove = []\n",
    "# count=0\n",
    "# # Iterate through the DataFrame and check if the files exist\n",
    "# for index, row in df.iterrows():\n",
    "#     image_path = '/home/csgrad/byalavar/FHE/celebSet/CELEBTEST/CELEBTEST/'+row['name']+'/' + row['filename'] \n",
    "#     if not os.path.exists(image_path):\n",
    "#         rows_to_remove.append(index)\n",
    "#         count=count+1\n",
    "# df = df.drop(rows_to_remove)\n",
    "# df.to_csv(csv_file, index=False)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor(),          # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainSet = CustomDataset('/home/csgrad/byalavar/FHE/HEAAN/FA_CVPR_Exp/celebSetIdentity.csv', transform=transform, get_pairs=True, num_pairs=25)\n",
    "# trainloader = DataLoader(trainSet, batch_size=16, shuffle=True, num_workers = 2)\n",
    "\n",
    "# testSet = CustomDataset('/home/csgrad/byalavar/FHE/HEAAN/FA_CVPR_Exp/celebSetIdentityTest.csv', transform=transform, get_pairs=False)\n",
    "# testloader = DataLoader(testSet, batch_size=16, shuffle=False)\n",
    "\n",
    "trainSet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv', get_pairs=False, transform=transform)\n",
    "trainloader = DataLoader(trainSet, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "testSet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/final_clebSET_test.csv', get_pairs=False, transform=transform)\n",
    "testloader = DataLoader(testSet, batch_size=32, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempIter = iter(testloader)\n",
    "# images,gen_pairs, imp_pairs = next(tempIter)\n",
    "# # imshow(images[0])\n",
    "# # print(labels['age'][0],labels['gender'][0],labels['ethnicity'][0])\n",
    "# # print(image_pairs)\n",
    "# imshow(images[0])\n",
    "# imshow(gen_pairs[0][0])\n",
    "# imshow(imp_pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import Conv2d, Linear\n",
    "from torch.nn import BatchNorm1d, BatchNorm2d\n",
    "from torch.nn import ReLU, Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.nn import PReLU\n",
    "import os\n",
    "\n",
    "def build_model(model_name='ir_50'):\n",
    "    if model_name == 'ir_101':\n",
    "        return IR_101(input_size=(112,112))\n",
    "    elif model_name == 'ir_50':\n",
    "        return IR_50(input_size=(112,112))\n",
    "    elif model_name == 'ir_se_50':\n",
    "        return IR_SE_50(input_size=(112,112))\n",
    "    elif model_name == 'ir_34':\n",
    "        return IR_34(input_size=(112,112))\n",
    "    elif model_name == 'ir_18':\n",
    "        return IR_18(input_size=(112,112))\n",
    "    else:\n",
    "        raise ValueError('not a correct model name', model_name)\n",
    "\n",
    "def initialize_weights(modules):\n",
    "    \"\"\" Weight initilize, conv2d and linear is initialized with kaiming_normal\n",
    "    \"\"\"\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight,\n",
    "                                    mode='fan_out',\n",
    "                                    nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight,\n",
    "                                    mode='fan_out',\n",
    "                                    nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class Flatten(Module):\n",
    "    \"\"\" Flat tensor\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class LinearBlock(Module):\n",
    "    \"\"\" Convolution block without no-linear activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_c, kernel, stride, padding, groups=groups, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNAP(Module):\n",
    "    \"\"\" Global Norm-Aware Pooling block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c):\n",
    "        super(GNAP, self).__init__()\n",
    "        self.bn1 = BatchNorm2d(in_c, affine=False)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.bn2 = BatchNorm1d(in_c, affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x_norm = torch.norm(x, 2, 1, True)\n",
    "        x_norm_mean = torch.mean(x_norm)\n",
    "        weight = x_norm_mean / x_norm\n",
    "        x = x * weight\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        feature = self.bn2(x)\n",
    "        return feature\n",
    "\n",
    "\n",
    "class GDC(Module):\n",
    "    \"\"\" Global Depthwise Convolution block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, embedding_size):\n",
    "        super(GDC, self).__init__()\n",
    "        self.conv_6_dw = LinearBlock(in_c, in_c,\n",
    "                                     groups=in_c,\n",
    "                                     kernel=(7, 7),\n",
    "                                     stride=(1, 1),\n",
    "                                     padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(in_c, embedding_size, bias=False)\n",
    "        self.bn = BatchNorm1d(embedding_size, affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_6_dw(x)\n",
    "        x = self.conv_6_flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEModule(Module):\n",
    "    \"\"\" SE block\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = Conv2d(channels, channels // reduction,\n",
    "                          kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight.data)\n",
    "\n",
    "        self.relu = ReLU(inplace=True)\n",
    "        self.fc2 = Conv2d(channels // reduction, channels,\n",
    "                          kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlockIR(Module):\n",
    "    \"\"\" BasicBlock for IRNet\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(BasicBlockIR, self).__init__()\n",
    "        if in_channel == depth:\n",
    "            self.shortcut_layer = MaxPool2d(1, stride)\n",
    "        else:\n",
    "            self.shortcut_layer = Sequential(\n",
    "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
    "                BatchNorm2d(depth))\n",
    "        self.res_layer = Sequential(\n",
    "            BatchNorm2d(in_channel),\n",
    "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n",
    "            BatchNorm2d(depth),\n",
    "            PReLU(depth),\n",
    "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False),\n",
    "            BatchNorm2d(depth))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut_layer(x)\n",
    "        res = self.res_layer(x)\n",
    "\n",
    "        return res + shortcut\n",
    "\n",
    "\n",
    "class BottleneckIR(Module):\n",
    "    \"\"\" BasicBlock with bottleneck for IRNet\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(BottleneckIR, self).__init__()\n",
    "        reduction_channel = depth // 4\n",
    "        if in_channel == depth:\n",
    "            self.shortcut_layer = MaxPool2d(1, stride)\n",
    "        else:\n",
    "            self.shortcut_layer = Sequential(\n",
    "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
    "                BatchNorm2d(depth))\n",
    "        self.res_layer = Sequential(\n",
    "            BatchNorm2d(in_channel),\n",
    "            Conv2d(in_channel, reduction_channel, (1, 1), (1, 1), 0, bias=False),\n",
    "            BatchNorm2d(reduction_channel),\n",
    "            PReLU(reduction_channel),\n",
    "            Conv2d(reduction_channel, reduction_channel, (3, 3), (1, 1), 1, bias=False),\n",
    "            BatchNorm2d(reduction_channel),\n",
    "            PReLU(reduction_channel),\n",
    "            Conv2d(reduction_channel, depth, (1, 1), stride, 0, bias=False),\n",
    "            BatchNorm2d(depth))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut_layer(x)\n",
    "        res = self.res_layer(x)\n",
    "\n",
    "        return res + shortcut\n",
    "\n",
    "\n",
    "class BasicBlockIRSE(BasicBlockIR):\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(BasicBlockIRSE, self).__init__(in_channel, depth, stride)\n",
    "        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\n",
    "\n",
    "\n",
    "class BottleneckIRSE(BottleneckIR):\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(BottleneckIRSE, self).__init__(in_channel, depth, stride)\n",
    "        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\n",
    "\n",
    "\n",
    "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
    "    '''A named tuple describing a ResNet block.'''\n",
    "\n",
    "\n",
    "def get_block(in_channel, depth, num_units, stride=2):\n",
    "\n",
    "    return [Bottleneck(in_channel, depth, stride)] +\\\n",
    "           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
    "\n",
    "\n",
    "def get_blocks(num_layers):\n",
    "    if num_layers == 18:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=2),\n",
    "            get_block(in_channel=64, depth=128, num_units=2),\n",
    "            get_block(in_channel=128, depth=256, num_units=2),\n",
    "            get_block(in_channel=256, depth=512, num_units=2)\n",
    "        ]\n",
    "    elif num_layers == 34:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=4),\n",
    "            get_block(in_channel=128, depth=256, num_units=6),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 50:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=4),\n",
    "            get_block(in_channel=128, depth=256, num_units=14),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 100:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=13),\n",
    "            get_block(in_channel=128, depth=256, num_units=30),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 152:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=256, num_units=3),\n",
    "            get_block(in_channel=256, depth=512, num_units=8),\n",
    "            get_block(in_channel=512, depth=1024, num_units=36),\n",
    "            get_block(in_channel=1024, depth=2048, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 200:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=256, num_units=3),\n",
    "            get_block(in_channel=256, depth=512, num_units=24),\n",
    "            get_block(in_channel=512, depth=1024, num_units=36),\n",
    "            get_block(in_channel=1024, depth=2048, num_units=3)\n",
    "        ]\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "class Backbone(Module):\n",
    "    def __init__(self, input_size, num_layers, mode='ir'):\n",
    "        \"\"\" Args:\n",
    "            input_size: input_size of backbone\n",
    "            num_layers: num_layers of backbone\n",
    "            mode: support ir or irse\n",
    "        \"\"\"\n",
    "        super(Backbone, self).__init__()\n",
    "        assert input_size[0] in [112, 224], \\\n",
    "            \"input_size should be [112, 112] or [224, 224]\"\n",
    "        assert num_layers in [18, 34, 50, 100, 152, 200], \\\n",
    "            \"num_layers should be 18, 34, 50, 100 or 152\"\n",
    "        assert mode in ['ir', 'ir_se'], \\\n",
    "            \"mode should be ir or ir_se\"\n",
    "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
    "                                      BatchNorm2d(64), PReLU(64))\n",
    "        blocks = get_blocks(num_layers)\n",
    "        if num_layers <= 100:\n",
    "            if mode == 'ir':\n",
    "                unit_module = BasicBlockIR\n",
    "            elif mode == 'ir_se':\n",
    "                unit_module = BasicBlockIRSE\n",
    "            output_channel = 512\n",
    "        else:\n",
    "            if mode == 'ir':\n",
    "                unit_module = BottleneckIR\n",
    "            elif mode == 'ir_se':\n",
    "                unit_module = BottleneckIRSE\n",
    "            output_channel = 2048\n",
    "\n",
    "        if input_size[0] == 112:\n",
    "            self.output_layer = Sequential(BatchNorm2d(output_channel),\n",
    "                                        Dropout(0.4), Flatten(),\n",
    "                                        Linear(output_channel * 7 * 7, 512),\n",
    "                                        BatchNorm1d(512, affine=False))\n",
    "        else:\n",
    "            self.output_layer = Sequential(\n",
    "                BatchNorm2d(output_channel), Dropout(0.4), Flatten(),\n",
    "                Linear(output_channel * 14 * 14, 512),\n",
    "                BatchNorm1d(512, affine=False))\n",
    "\n",
    "        modules = []\n",
    "        for block in blocks:\n",
    "            for bottleneck in block:\n",
    "                modules.append(\n",
    "                    unit_module(bottleneck.in_channel, bottleneck.depth,\n",
    "                                bottleneck.stride))\n",
    "        self.body = Sequential(*modules)\n",
    "\n",
    "        initialize_weights(self.modules())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # current code only supports one extra image\n",
    "        # it comes with a extra dimension for number of extra image. We will just squeeze it out for now\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        for idx, module in enumerate(self.body):\n",
    "            x = module(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        norm = torch.norm(x, 2, 1, True)\n",
    "        output = torch.div(x, norm)\n",
    "\n",
    "        return output, norm\n",
    "\n",
    "\n",
    "\n",
    "def IR_18(input_size):\n",
    "    \"\"\" Constructs a ir-18 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 18, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_34(input_size):\n",
    "    \"\"\" Constructs a ir-34 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 34, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_50(input_size):\n",
    "    \"\"\" Constructs a ir-50 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 50, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_101(input_size):\n",
    "    \"\"\" Constructs a ir-101 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 100, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_152(input_size):\n",
    "    \"\"\" Constructs a ir-152 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 152, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_200(input_size):\n",
    "    \"\"\" Constructs a ir-200 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 200, 'ir')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_50(input_size):\n",
    "    \"\"\" Constructs a ir_se-50 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 50, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_101(input_size):\n",
    "    \"\"\" Constructs a ir_se-101 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 100, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_152(input_size):\n",
    "    \"\"\" Constructs a ir_se-152 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 152, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def IR_SE_200(input_size):\n",
    "    \"\"\" Constructs a ir_se-200 model.\n",
    "    \"\"\"\n",
    "    model = Backbone(input_size, 200, 'ir_se')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "adaface_models = {\n",
    "    'ir_18':\"/home/csgrad/byalavar/FHE/HEAAN/FA_CVPR_Exp/adaface_ir18_webface4m.ckpt\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_pretrained_model(architecture='ir_18'):\n",
    "    # load model and pretrained statedict\n",
    "    assert architecture in adaface_models.keys()\n",
    "    model = build_model(architecture)\n",
    "    statedict = torch.load(adaface_models[architecture])['state_dict']\n",
    "    model_statedict = {key[6:]:val for key, val in statedict.items() if key.startswith('model.')}\n",
    "    model.load_state_dict(model_statedict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def to_input(pil_rgb_image):\n",
    "    np_img = np.array(pil_rgb_image)\n",
    "    brg_img = ((np_img[:,:,::-1] / 255.) - 0.5) / 0.5\n",
    "    tensor = torch.tensor([brg_img.transpose(2,0,1)]).float()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaFaceModel = load_pretrained_model('ir_18')\n",
    "\n",
    "# print(images.shape)\n",
    "# bgr_image = images[:, [2, 1, 0], :, :]\n",
    "# print(bgr_image.shape)\n",
    "\n",
    "\n",
    "#feature, _ = adaFaceModel(bgr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet.data['age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_age_value(original_age_value):\n",
    "    return (original_age_value - trainSet.data['age'].min())/(trainSet.data['age'].max() - trainSet.data['age'].min())\n",
    "\n",
    "\n",
    "def get_original_age_value(normalized_age_value):\n",
    "    return normalized_age_value * (trainSet.data['age'].max()  - trainSet.data['age'].min()) + trainSet.data['age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import MSELoss\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "class CompressionModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompressionModel1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "class CompressionModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompressionModel2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        x = self.model2(x)\n",
    "        return x\n",
    "\n",
    "class OgId(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OgId, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,80)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class CompId(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompId, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,80)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CompressionLoss(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(CompressionLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, img, genuine_imgs, imposter_imgs, genuine_sim, imposter_sim):\n",
    "        # img -> base image embedding\n",
    "        # genuine_imgs, imposter_imgs -> Compressed embeddings of dimension 128\n",
    "        # genuine_sim, imposter_sim -> cosine smilairty scores of uncompressed embeddings of dimension 512\n",
    "\n",
    "\n",
    "        # Compute cosine similarity loss\n",
    "        genuine_loss = 0\n",
    "        for i in range(len(genuine_imgs)):\n",
    "            sim_score = F.cosine_similarity(img, genuine_imgs[i])\n",
    "            genuine_loss += torch.mean(torch.abs(sim_score - genuine_sim[i].view(-1)))\n",
    "\n",
    "        imp_loss = 0\n",
    "        for i in range(len(imposter_imgs)):\n",
    "            imp_sim_score = F.cosine_similarity(img, imposter_imgs[i])\n",
    "            imp_loss += torch.mean(torch.abs(imp_sim_score - imposter_sim[i].view(-1)))\n",
    "\n",
    "        \n",
    "        # Compute Covariance Loss\n",
    "        # matrix -> Concatenates compressed genuine and imposter embeddings\n",
    "        \n",
    "        matrix = torch.cat([genuine_imgs[0], imposter_imgs[0]], dim=0)\n",
    "        for i in range(1, len(genuine_imgs)):\n",
    "            matrix = torch.cat([matrix, genuine_imgs[i]], dim = 0)\n",
    "            matrix = torch.cat([matrix, imposter_imgs[i]], dim = 0)\n",
    "\n",
    "\n",
    "        mean_matrix = torch.mean(matrix, dim = 0)\n",
    "        mx = torch.matmul(mean_matrix.t(), mean_matrix)\n",
    "        vx = torch.matmul(matrix.t(), matrix) / self.batch_size # Dividing by batch size as done here https://github.com/human-analysis/hers-encrypted-image-search/blob/master/deep_mds%2B%2B/nntools/tensorflow/networks/deepmds.py#L94\n",
    "        cov_matrix = mx - vx\n",
    "        diag = torch.diag(cov_matrix.diag())\n",
    "        cov_loss = torch.mean(torch.abs(cov_matrix - diag))\n",
    "        \n",
    "        # Compute Supervise Loss - Not used for ArcFace\n",
    "        # # g_dist -> Concatenates compressed genuine embeddings\n",
    "        # g_dist = torch.cat([genuine_imgs[0], genuine_imgs[1]], dim = 0)\n",
    "        # for i in range(2, len(genuine_imgs)):\n",
    "        #     g_dist = torch.cat([g_dist, genuine_imgs[i]], dim = 0)\n",
    "\n",
    "        # # g_dist -> Concatenates compressed imposter embeddings\n",
    "        # imp_dist = torch.cat([imposter_imgs[0], imposter_imgs[1]], dim = 0)\n",
    "        # for i in range(2, len(imposter_imgs)):\n",
    "        #     imp_dist = torch.cat([imp_dist, imposter_imgs[i]], dim = 0)\n",
    "\n",
    "        # gloss = (torch.mean(g_dist) + 1.0) * 0.5\n",
    "        # iloss = (torch.mean(imp_dist) + 1.0) * 0.5\n",
    "\n",
    "        # lda_term = iloss / gloss\n",
    "        # return genuine_loss + imp_loss + 10 * cov_loss \n",
    "        return 10*genuine_loss + imp_loss + cov_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class faceAnalytics(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1=nn.Linear(128,64)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.layer2=nn.Linear(64,32)\n",
    "        #self.layer3=nn.Linear(1024,512)\n",
    "        # self.layer4=nn.Linear(128,64)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        self.genderOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "        self.ageOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,4)\n",
    "        )\n",
    "        self.ethnicityOut = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "        # self.identity = nn.Sequential(nn.Linear(32,64), nn.ReLU(), nn.Linear(64,80))\n",
    "        # self.maxVal = 0\n",
    "        # self.min=0\n",
    "        \n",
    "    \n",
    "    def writeResult(self,result):\n",
    "       output_directory=\"\"\n",
    "       file_name = \"resultAge.txt\"\n",
    "\n",
    "       with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in result:\n",
    "            file.write(f\"{value}\\n\")\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        #print(\"Input\",x[0])\n",
    "        # x=self.layer1(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "        # x=self.layer2(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "\n",
    "        gender=self.genderOut(x)\n",
    "        age=self.ageOut(x)\n",
    "        ethn = self.ethnicityOut(x)\n",
    "        # identity = self.identity(x)\n",
    "        return gender,age,ethn\n",
    "    \n",
    "    \n",
    "    def are_values_in_same_class(self,value1, value2):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(15, 22),(22,40),(40,60),(60,80)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        for class_range in class_ranges:\n",
    "            if class_range[0] <= value1 < class_range[1] and class_range[0] <= value2 < class_range[1]:\n",
    "                return True\n",
    "    \n",
    "        return False    \n",
    "    \n",
    "    def trainAE(self, trainloader, testloader, adaFace, aeModel,OgIdLoss, CompIdLoss, device, episodes, completed_eps=0):\n",
    "        input_dim = 512\n",
    "        compressed_dim = 128\n",
    "        model = aeModel\n",
    "        # Define optimizer and loss function\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        MSELoss = nn.MSELoss().to(device)\n",
    "        celoss = nn.CrossEntropyLoss().to(device)\n",
    "        compLoss = CompIdLoss\n",
    "        ogLoss = OgIdLoss  \n",
    "        model.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "            cnt = 0\n",
    "\n",
    "            for img, data in trainloader:\n",
    "                cnt += 1\n",
    "                if cnt%50 == 0:\n",
    "                    print(\"Image number = \" + str(cnt))\n",
    "                img = img.to(device=device)\n",
    "                label = data['name'].to(device=device)\n",
    "                input = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings,_ = adaFace(input)\n",
    "                encoded, decoded = model(embeddings)\n",
    "                x1 = compLoss(encoded) \n",
    "                x2 = ogLoss(decoded)\n",
    "                loss = celoss(x1, label) + celoss(x2, label) + 0.5*MSELoss(embeddings, decoded)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (ep+1) % 5 == 0:\n",
    "                torch.save(model, f'AeModel_{ep+1+completed_eps}.pt')\n",
    "            print(f\"Epoch [{ep+completed_eps + 1}/{episodes+completed_eps}], Loss: {total_loss}\")\n",
    "        torch.save(model, f'AeModel_{episodes+completed_eps}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "\n",
    "    def trainCombinedModel(self, trainloader, testloader, adaFace, combinedModel, compressionLoss, device, episodes, completed_eps=0):\n",
    "        input_dim = 512\n",
    "        compressed_dim = 128\n",
    "        model = combinedModel\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=4e-5)\n",
    "        criterion = compressionLoss\n",
    "        model.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "            cnt = 0\n",
    "            for img, gen_pairs, imp_pairs in trainloader:\n",
    "\n",
    "                # print(img.shape)\n",
    "                # print(image_pairs[0].shape)\n",
    "                # print(img)\n",
    "                # print(len(image_pairs))\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt%50 == 0:\n",
    "                    print(\"Image number = \" + str(cnt))\n",
    "                \n",
    "\n",
    "                img = img.to(device=device)\n",
    "                input = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings,_ = adaFace(input)\n",
    "                compressed_img = model(embeddings)\n",
    "                gen_sim_scores = []\n",
    "                gen_embedding_pairs = []\n",
    "                for img_pair in gen_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compressed_img2 = model(embedding_pair)\n",
    "                    gen_embedding_pairs.append(compressed_img2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    gen_sim_scores.append(sim_score)\n",
    "                \n",
    "                imp_sim_scores = []\n",
    "                imp_embedding_pairs = []\n",
    "                for img_pair in imp_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compressed_img2 = model(embedding_pair)\n",
    "                    imp_embedding_pairs.append(compressed_img2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    imp_sim_scores.append(sim_score)\n",
    "                \n",
    "                loss = criterion(compressed_img, gen_embedding_pairs, imp_embedding_pairs, gen_sim_scores, imp_sim_scores)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (ep+1) % 5 == 0:\n",
    "                torch.save(model, f'CombinedModel_{ep+1+completed_eps}.pt')\n",
    "            print(f\"Epoch [{ep+completed_eps + 1}/{episodes+completed_eps}], Loss: {total_loss}\")\n",
    "        torch.save(model, f'CombinedModel_{episodes+completed_eps}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def trainCompressionModel2(self, trainloader, testloader, adaFace, compressionModel1, compressionModel2, compressionLoss, device, episodes, completed_eps = 0):\n",
    "        \n",
    "        # Define optimizer and loss function\n",
    "        compressionModel1.eval()\n",
    "        optimizer = optim.Adam(compressionModel2.parameters(), lr=0.001, weight_decay=4e-5)\n",
    "        criterion = compressionLoss\n",
    "        compressionModel2.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "            cnt = 0\n",
    "            for img, gen_pairs, imp_pairs in trainloader:\n",
    "\n",
    "                # print(img.shape)\n",
    "                # print(image_pairs[0].shape)\n",
    "                # print(img)\n",
    "                # print(len(image_pairs))\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt%50 == 0:\n",
    "                    print(\"Image number = \" + str(cnt))\n",
    "                \n",
    "\n",
    "                img = img.to(device=device)\n",
    "                input = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings,_ = adaFace(input)\n",
    "                compressed_img = compressionModel1(embeddings)\n",
    "                compressed_img2 = compressionModel2(compressed_img)\n",
    "                gen_sim_scores = []\n",
    "                gen_embedding_pairs = []\n",
    "                for img_pair in gen_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compression_pair1 = compressionModel1(embedding_pair)\n",
    "                    compressed_pair2 = compressionModel2(compression_pair1)\n",
    "                    gen_embedding_pairs.append(compressed_pair2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    gen_sim_scores.append(sim_score)\n",
    "                \n",
    "                imp_sim_scores = []\n",
    "                imp_embedding_pairs = []\n",
    "                for img_pair in imp_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compression_pair1 = compressionModel1(embedding_pair)\n",
    "                    compressed_pair2 = compressionModel2(compression_pair1)\n",
    "                    imp_embedding_pairs.append(compressed_pair2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    imp_sim_scores.append(sim_score)\n",
    "                \n",
    "                loss = criterion(compressed_img2, gen_embedding_pairs, imp_embedding_pairs, gen_sim_scores, imp_sim_scores)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (ep + 1) % 5 == 0:\n",
    "                torch.save(compressionModel2, f'CompressionModel2_{ep+1+completed_eps}.pt')\n",
    "            print(f\"Epoch [{completed_eps + ep + 1}/{completed_eps + episodes}], Loss: {total_loss}\")\n",
    "        torch.save(compressionModel2, f'CompressionModel2_{completed_eps + episodes}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def trainCompressionModel1(self, trainloader, testloader, adaFace, compressionModel1, compressionLoss, device, episodes, completed_eps = 0):\n",
    "        input_dim = 512\n",
    "        compressed_dim = 128\n",
    "        model = compressionModel1\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        criterion = compressionLoss\n",
    "        model.train()\n",
    "        for ep in range(episodes):\n",
    "            total_loss = 0\n",
    "            print(\"Starting \" + str(ep))\n",
    "            cnt = 0\n",
    "            for img, gen_pairs, imp_pairs in trainloader:\n",
    "\n",
    "                # print(img.shape)\n",
    "                # print(image_pairs[0].shape)\n",
    "                # print(img)\n",
    "                # print(len(image_pairs))\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt%50 == 0:\n",
    "                    print(\"Image number = \" + str(cnt))\n",
    "                \n",
    "\n",
    "                img = img.to(device=device)\n",
    "                input = img[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings,_ = adaFace(input)\n",
    "                compressed_img = model(embeddings)\n",
    "                gen_sim_scores = []\n",
    "                gen_embedding_pairs = []\n",
    "                for img_pair in gen_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compressed_img2 = model(embedding_pair)\n",
    "                    gen_embedding_pairs.append(compressed_img2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    gen_sim_scores.append(sim_score)\n",
    "                \n",
    "                imp_sim_scores = []\n",
    "                imp_embedding_pairs = []\n",
    "                for img_pair in imp_pairs:\n",
    "\n",
    "                    img_pair = img_pair.to(device=device)\n",
    "                    input_pair = img_pair[:, [2, 1, 0], :, :]\n",
    "                    #print(inputs.shape)\n",
    "                    embedding_pair,_ = adaFace(input_pair)\n",
    "                    compressed_img2 = model(embedding_pair)\n",
    "                    imp_embedding_pairs.append(compressed_img2)\n",
    "                    sim_score = F.cosine_similarity(embeddings, embedding_pair)\n",
    "                    imp_sim_scores.append(sim_score)\n",
    "                \n",
    "                loss = criterion(compressed_img, gen_embedding_pairs, imp_embedding_pairs, gen_sim_scores, imp_sim_scores)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (ep+1) % 5 == 0:\n",
    "                torch.save(model, f'CompressionModel1_{ep+1+completed_eps}.pt')\n",
    "            print(f\"Epoch [{ep + 1 + completed_eps}/{completed_eps + episodes}], Loss: {total_loss}\")\n",
    "        torch.save(model, f'CompressionModel1_{episodes+completed_eps}.pt')\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "\n",
    "    def trainModel(self,trainLoader,testLoader,adaFace, compressionModel, device,episodes):\n",
    "        self.train()\n",
    "        #maxVal = 0\n",
    "        learningRate=0.01\n",
    "        gender_loss = nn.CrossEntropyLoss() \n",
    "        age_loss = nn.CrossEntropyLoss() \n",
    "        ethn_loss = nn.CrossEntropyLoss()\n",
    "        identityLoss = nn.CrossEntropyLoss()\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=learningRate)\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=learningRate,\n",
    "        momentum=0.9, weight_decay=5e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=episodes)\n",
    "        trainingAcc = []\n",
    "        model = compressionModel\n",
    "        for e in range(0,episodes):\n",
    "            total_training_loss =0\n",
    "            genderAcc=0\n",
    "            ethnAcc = 0\n",
    "            ageAcc=0\n",
    "            identAcc=0\n",
    "            count=0\n",
    "            totalGenderLoss=0\n",
    "\n",
    "\n",
    "            for i,data in enumerate(trainLoader):\n",
    "\n",
    "                #print(inputs.shape)\n",
    "                #inputs = inputs.type(torch.double)\n",
    "                \n",
    "                #gender_label = data[\"gender\"].to(device=device)\n",
    "                #race_label = data[\"race\"].to(device=device)\n",
    "            # print(i)\n",
    "                inputs = data[0].to(device=device)\n",
    "                #inputs = inputs.type(torch.double)\n",
    "                age_label = (data[1][\"age\"]).to(device=device)\n",
    "            \n",
    "                gender_label = data[1][\"gender\"].to(device=device)\n",
    "                ethn_label = data[1][\"ethnicity\"].to(device=device)\n",
    "\n",
    "                ident_label = data[1][\"name\"].to(device=device)\n",
    "                \n",
    "                #print(torch.max(ethn_label),torch.min(ethn_label))\n",
    "                #print(torch.max(gender_label),torch.min(gender_label))\n",
    "\n",
    "                #print(count)\n",
    "                #print(inputs.shape)\n",
    "                inputs = inputs[:, [2, 1, 0], :, :]\n",
    "                #print(inputs.shape)\n",
    "                embeddings,_ = adaFace(inputs)\n",
    "                encoded, _ = model(embeddings)\n",
    "                gender,age,ethn = self(encoded)\n",
    "                ageLoss = age_loss(age,age_label)\n",
    "                loss = gender_loss(gender,gender_label) + ageLoss + ethn_loss(ethn,ethn_label)\n",
    "                # loss = gender_loss(gender,gender_label) + ageLoss + ethn_loss(ethn,ethn_label)  + identityLoss(ident,ident_label)\n",
    "                #print(gender)\n",
    "                #print(gender_label)\n",
    "                #totalGenderLoss = totalGenderLoss + loss.item()\n",
    "                loss.backward()\n",
    "                #print(\"Loss:\",loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                #total_training_loss = total_training_loss+loss.item()*512\n",
    "\n",
    "                predictedGender = torch.argmax(gender,dim=1)\n",
    "                predictedEthn = torch.argmax(ethn,dim=1)\n",
    "                predictedAge = torch.argmax(age,dim=1)\n",
    "                # predictedIdent = torch.argmax(ident,dim=1)\n",
    "                #print(predictedGender)\n",
    "                #print(gender_label)\n",
    "                for j in range(0,predictedGender.shape[0]):\n",
    "                    count=count+1\n",
    "                    #print(predictedGender[j].item(),gender_label[j])\n",
    "                    if(predictedGender[j].item()==gender_label[j].item()):\n",
    "                        genderAcc=genderAcc+1\n",
    "                \n",
    "                    if(predictedEthn[j].item()==ethn_label[j].item()):\n",
    "                        ethnAcc=ethnAcc+1\n",
    "    \n",
    "                    if(predictedAge[j].item()==age_label[j].item()):\n",
    "                        ageAcc=ageAcc+1\n",
    "\n",
    "                    # if(predictedIdent[j].item()==ident_label[j].item()):\n",
    "                    #     identAcc=identAcc+1\n",
    "    \n",
    "            genderAccuracy =  genderAcc/count\n",
    "            trainingAcc.append(genderAccuracy)\n",
    "            print(\"Gender Accuracy:\", genderAccuracy,\"Age Acc:\", ageAcc/count, \" ethnAcc:\", ethnAcc/count)\n",
    "            #print(\"total training loss:\",total_training_loss/16595,\"\\n\")\n",
    "            #print(\"\\n\")\n",
    "            scheduler.step()\n",
    "            #print(\"max observed value: \", maxVal)\n",
    "            if(e%2==0):\n",
    "                self.test(testLoader,adaFace, compressionModel, device)\n",
    "            \n",
    "        return trainingAcc\n",
    "\n",
    "    def are_values_in_same_class(self,value1, value2):\n",
    "\n",
    "    # Define class ranges\n",
    "        class_ranges = [(15, 22),(22,40),(40,60),(60,80)]\n",
    "    \n",
    "    # Check if both values fall into the same class range\n",
    "        for class_range in class_ranges:\n",
    "            if class_range[0] <= value1 < class_range[1] and class_range[0] <= value2 < class_range[1]:\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def test(self,testLoader,adaFace, compressionModel, device):\n",
    "\n",
    "         self.eval()\n",
    "         age_loss = nn.L1Loss()\n",
    "\n",
    "         totalAgeError = 0\n",
    "         genderAccuracy = 0\n",
    "         tempAcc=0\n",
    "         count=0\n",
    "         totalGenderLoss=0\n",
    "         maxAge = 0\n",
    "         minAge = 0\n",
    "         ageAccuracy = 0\n",
    "         total_training_loss =0\n",
    "         tempAcc=0\n",
    "         count=0\n",
    "         totalGenderLoss=0\n",
    "\n",
    "         genderAcc=0\n",
    "         ethnAcc = 0\n",
    "         identAcc = 0\n",
    "\n",
    "         model = compressionModel\n",
    "        \n",
    "         for i,data in enumerate(testLoader):\n",
    "\n",
    "            #print(inputs.shape)\n",
    "            #inputs = inputs.type(torch.double)\n",
    "            \n",
    "            #gender_label = data[\"gender\"].to(device=device)\n",
    "            #race_label = data[\"race\"].to(device=device)\n",
    "            \n",
    "   \n",
    "            #inputs = inputs.type(torch.double)\n",
    "            inputs = data[0].to(device=device)\n",
    "            age_label = data[1][\"age\"].to(device=device)\n",
    "            gender_label = data[1][\"gender\"].to(device=device)\n",
    "            ethn_label = data[1][\"ethnicity\"].to(device=device)\n",
    "            ident_label = data[1]['name'].to(device=device)\n",
    "            #print(count)\n",
    "            inputs = inputs[:, [2, 1, 0], :, :]\n",
    "            embeddings,_ = adaFace(inputs)\n",
    "            compressedEm, _ = model(embeddings)\n",
    "            gender,age,ethn = self(compressedEm)\n",
    "         \n",
    "\n",
    "            predictedGender = torch.argmax(gender,dim=1)\n",
    "            predictedEthn = torch.argmax(ethn,dim=1)\n",
    "            predictedAge = torch.argmax(age,dim=1)\n",
    "            # predictedIdent = torch.argmax(ident,dim=1)\n",
    "            #age = get_original_age_value(age)\n",
    "\n",
    "            for j in range(0,predictedGender.shape[0]):\n",
    "                count=count+1\n",
    "                #print(predictedGender[j].item(),gender_label[j])\n",
    "                if(predictedGender[j].item()==gender_label[j]):\n",
    "                    genderAcc=genderAcc+1\n",
    "                if(predictedEthn[j].item()==ethn_label[j]):\n",
    "                    ethnAcc=ethnAcc+1\n",
    "                if(predictedAge[j].item()==age_label[j]):\n",
    "                   ageAccuracy = ageAccuracy +1\n",
    "                \n",
    "                # if(predictedIdent[j].item()==ident_label[j].item()):\n",
    "                #     identAcc=identAcc+1\n",
    "\n",
    "         genderAccuracy =  genderAcc/count\n",
    "         \n",
    "         print(\"Test Gender Accuracy:\", genderAccuracy,\"Test Age Accuracy:\", ageAccuracy/count, \"Test ethnicity Accuracy:\", ethnAcc/count)\n",
    "\n",
    "         return genderAccuracy,totalAgeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "FAmodel=faceAnalytics()\n",
    "# compressionModel1 = CompressionModel1()\n",
    "# compressionModel2 = CompressionModel2()\n",
    "# compressionLoss = CompressionLoss(batch_size)\n",
    "# combinedModel = CombinedModel(compressionModel1, compressionModel2)\n",
    "aeModel = Autoencoder()\n",
    "#model=torch.load(\"arcFaceCelebSetBase.pt\")\n",
    "FAmodel.to(device)\n",
    "adaFaceModel.to(device)\n",
    "adaFaceModel.eval()\n",
    "# compressionModel1.to(device)\n",
    "# compressionModel2.to(device)\n",
    "# combinedModel.to(device)\n",
    "aeModel.to(device)\n",
    "# compressionLoss = DataParallel(compressionLoss)\n",
    "# compressionLoss.to(device)\n",
    "#torch.save(model,\"bestFaceAn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [1/20], Loss: 18630.683947086334\n",
      "Starting 1\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [2/20], Loss: 16629.32578611374\n",
      "Starting 2\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [3/20], Loss: 15211.09567117691\n",
      "Starting 3\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [4/20], Loss: 14279.437047958374\n",
      "Starting 4\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [5/20], Loss: 13618.91488981247\n",
      "Starting 5\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [6/20], Loss: 13121.941080570221\n",
      "Starting 6\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [7/20], Loss: 12737.83532333374\n",
      "Starting 7\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [8/20], Loss: 12416.52683877945\n",
      "Starting 8\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [9/20], Loss: 12145.823643684387\n",
      "Starting 9\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [10/20], Loss: 11911.165961742401\n",
      "Starting 10\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [11/20], Loss: 11715.245296478271\n",
      "Starting 11\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [12/20], Loss: 11548.957551956177\n",
      "Starting 12\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [13/20], Loss: 11404.508037090302\n",
      "Starting 13\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [14/20], Loss: 11278.11402797699\n",
      "Starting 14\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [15/20], Loss: 11163.671092510223\n",
      "Starting 15\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [16/20], Loss: 11060.02625465393\n",
      "Starting 16\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [17/20], Loss: 10969.15516281128\n",
      "Starting 17\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [18/20], Loss: 10887.094640254974\n",
      "Starting 18\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [19/20], Loss: 10809.476818561554\n",
      "Starting 19\n",
      "Image number = 50\n",
      "Image number = 100\n",
      "Image number = 150\n",
      "Image number = 200\n",
      "Image number = 250\n",
      "Image number = 300\n",
      "Image number = 350\n",
      "Image number = 400\n",
      "Image number = 450\n",
      "Image number = 500\n",
      "Image number = 550\n",
      "Image number = 600\n",
      "Image number = 650\n",
      "Image number = 700\n",
      "Image number = 750\n",
      "Image number = 800\n",
      "Image number = 850\n",
      "Image number = 900\n",
      "Image number = 950\n",
      "Image number = 1000\n",
      "Image number = 1050\n",
      "Image number = 1100\n",
      "Image number = 1150\n",
      "Image number = 1200\n",
      "Image number = 1250\n",
      "Image number = 1300\n",
      "Image number = 1350\n",
      "Image number = 1400\n",
      "Image number = 1450\n",
      "Image number = 1500\n",
      "Image number = 1550\n",
      "Image number = 1600\n",
      "Image number = 1650\n",
      "Image number = 1700\n",
      "Image number = 1750\n",
      "Image number = 1800\n",
      "Image number = 1850\n",
      "Image number = 1900\n",
      "Image number = 1950\n",
      "Image number = 2000\n",
      "Image number = 2050\n",
      "Image number = 2100\n",
      "Image number = 2150\n",
      "Image number = 2200\n",
      "Epoch [20/20], Loss: 10738.157445907593\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "ogLoss = OgId()\n",
    "compLoss = CompId()\n",
    "ogLoss.to(device)\n",
    "compLoss.to(device)\n",
    "FAmodel.trainAE(trainloader, testloader, adaFaceModel, aeModel, ogLoss, compLoss, device, 20, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FAmodel=faceAnalytics()\n",
    "# FAmodel.to(device)\n",
    "FAmodel.trainCompressionModel1(trainloader,testloader,adaFaceModel,compressionModel1, compressionLoss, device, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FAmodel=faceAnalytics()\n",
    "# FAmodel.to(device)\n",
    "FAmodel.trainCompressionModel2(trainloader,testloader,adaFaceModel,compressionModel1, compressionModel2, compressionLoss, device, 10,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FAmodel=faceAnalytics()\n",
    "FAmodel.to(device)\n",
    "compressionModel1.eval()\n",
    "compressionModel2.eval()\n",
    "FAmodel.trainCombinedModel(trainloader, testloader, adaFaceModel, combinedModel, compressionLoss, device, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "processedTensor = torch.zeros((len(testSet), 128))\n",
    "identityLabel = torch.zeros((len(testSet)))\n",
    "count = 0\n",
    "\n",
    "for i,data in enumerate(testloader):\n",
    "    \n",
    "    inputs = data[0].to(device=device)\n",
    "    iden_label = data[1]['name'].to(device=device)\n",
    "    inputs = inputs[:, [2, 1, 0], :, :]    \n",
    "    embeddings,_ = adaFaceModel(inputs)\n",
    "    encoded, _ = aeModel(embeddings)\n",
    "    # compressed_img1 = compressionModel1(embeddings)\n",
    "    # compressed_img2 = compressionModel2(compressed_img1)\n",
    "    # compressed = combinedModel(embeddings)\n",
    "    processedTensor[count : count + embeddings.shape[0]] = torch.tensor(encoded)\n",
    "    identityLabel[count:count + iden_label.shape[0]] = torch.tensor(iden_label)\n",
    "\n",
    "\n",
    "    count = count + embeddings.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8325\n"
     ]
    }
   ],
   "source": [
    "identityAccuracy = 0\n",
    "\n",
    "count = 0\n",
    "\n",
    "while(count < processedTensor.shape[0]):\n",
    "\n",
    "    currentFace = processedTensor[count : count + 1]\n",
    "    cosine_similarity = nn.functional.cosine_similarity(currentFace, processedTensor)\n",
    "    if(identityLabel[torch.topk(cosine_similarity, k = 2)[1][1]].item() == identityLabel[count].item()):\n",
    "        identityAccuracy = identityAccuracy +1\n",
    "    count = count + 1\n",
    "    # print(count)\n",
    "print(identityAccuracy/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAmodel.trainCompressionModel(trainloader,testloader,adaFaceModel,compressionModel, compressionLoss, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetDet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/celebSET_final_v1.csv', get_pairs=False, transform=transform)\n",
    "trainloaderDet = DataLoader(trainSetDet, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "testSetDet = CustomDataset('/home/csgrad/byalavar/FHE/celebSet/final_clebSET_test.csv', get_pairs=False, transform=transform)\n",
    "testloaderDet = DataLoader(testSetDet, batch_size=16, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Accuracy: 0.9623919084766325 Age Acc: 0.6096150319513606  ethnAcc: 0.9649875156935491\n",
      "Test Gender Accuracy: 0.934375 Test Age Accuracy: 0.571875 Test ethnicity Accuracy: 0.925625\n",
      "Gender Accuracy: 0.969106631494308 Age Acc: 0.5989786849863872  ethnAcc: 0.9704608613466124\n",
      "Gender Accuracy: 0.971490640296802 Age Acc: 0.5989786849863872  ethnAcc: 0.97325396041699\n",
      "Test Gender Accuracy: 0.954375 Test Age Accuracy: 0.571875 Test ethnicity Accuracy: 0.921875\n",
      "Gender Accuracy: 0.9732398538560284 Age Acc: 0.5989786849863872  ethnAcc: 0.9745235509035253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb Cell 27\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m FAmodel\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m aeModel\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m trainingAcc \u001b[39m=\u001b[39m FAmodel\u001b[39m.\u001b[39;49mtrainModel(trainloader,testloader,adaFaceModel,aeModel, device,\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=369'>370</a>\u001b[0m loss \u001b[39m=\u001b[39m gender_loss(gender,gender_label) \u001b[39m+\u001b[39m ageLoss \u001b[39m+\u001b[39m ethn_loss(ethn,ethn_label)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=370'>371</a>\u001b[0m \u001b[39m# loss = gender_loss(gender,gender_label) + ageLoss + ethn_loss(ethn,ethn_label)  + identityLoss(ident,ident_label)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=371'>372</a>\u001b[0m \u001b[39m#print(gender)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=372'>373</a>\u001b[0m \u001b[39m#print(gender_label)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=373'>374</a>\u001b[0m \u001b[39m#totalGenderLoss = totalGenderLoss + loss.item()\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=374'>375</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=375'>376</a>\u001b[0m \u001b[39m#print(\"Loss:\",loss)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btrail-02.cse.buffalo.edu/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/CompressionAdaFaceCelebSet.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=376'>377</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/face_analytics2/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/face_analytics2/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aeModel = torch.load('/home/csgrad/kaushik3/PolyProtect/PolyProtect_HEAAN/HEAAN/code/CompressionCode/AeModel_20_bs32_82acc_best.pt')\n",
    "aeModel.to(device)\n",
    "FAmodel=faceAnalytics()\n",
    "FAmodel.to(device)\n",
    "aeModel.eval()\n",
    "trainingAcc = FAmodel.trainModel(trainloader,testloader,adaFaceModel,aeModel, device,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAmodel.test(testloaderDet,adaFaceModel, compressionModel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"arcFaceCelebSetBase.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"negGenderBaseWiki.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainingAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"model92.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "input1=[]\n",
    "for i,data in enumerate(val_dataloader):\n",
    "    \n",
    "    if(count==0):\n",
    "     inputs=resnet(data[\"image\"].to(device))\n",
    "\n",
    " \n",
    "     input1 = polyprotect(0,inputs[0])\n",
    "\n",
    "     break\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    print(param[0])\n",
    "    print(torch.dot(input1,param[1]))\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory=\"\"\n",
    "file_name = \"input1.txt\"\n",
    "\n",
    "with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in input1:\n",
    "            file.write(f\"{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=faceAnalytics()\n",
    "model=torch.load(\"/home/csgrad/byalavar/FHE/HEAAN/modelUsing0.pt\")\n",
    "model.to(device)\n",
    "model.test(dataloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"modelUsing0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    count=count+1\n",
    "    if(count==2):\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=nn.Linear(4,2)\n",
    "input1=torch.rand((1,4))\n",
    "print(\"input1\",input1)\n",
    "for param in a.parameters():\n",
    "    print(\"param\",param)\n",
    "print(a(input1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=faceAnalytics()\n",
    "model=torch.load(\"modelUsing0.pt\")\n",
    "model.to(device)\n",
    "count=0\n",
    "ageBias=[]\n",
    "for param in model.parameters(): \n",
    "    print(param.shape)\n",
    "    if(count==7):\n",
    "       ageBias = param.tolist()\n",
    "    count=count+1\n",
    "print(len(ageBias),len(ageBias[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists with shape (512, 128)\n",
    "\n",
    "\n",
    "# Define the directory where you want to save the text files\n",
    "output_directory = \"ageWeights\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Write each list to a separate text file\n",
    "\n",
    "\n",
    "# Write each list to a separate text file\n",
    "for i, sublist in enumerate(ageWeights):\n",
    "    # Define the file name with leading zeros\n",
    "    file_name = f\"{i:03d}.txt\"\n",
    "\n",
    "    # Write each value in the sublist on a new line\n",
    "    with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in sublist:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "print(\"Files saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "layer1Bias=[]\n",
    "for param in model.parameters(): \n",
    "    print(param.shape)\n",
    "    if(count==2):\n",
    "       layer1Bias = param.tolist()\n",
    "       break\n",
    "    count=count+1\n",
    "print(len(layer1Bias),len(layer1Bias[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory=\"\"\n",
    "file_name = \"ageBias.txt\"\n",
    "\n",
    "with open(os.path.join(output_directory, file_name), \"w\") as file:\n",
    "        for value in ageBias:\n",
    "            file.write(f\"{value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
