{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukNC_taEjokm",
        "outputId": "7e156131-a4e2-4324-9343-96ff6a40d972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "TfOPNj28mbII"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYxTYj_pmUBt",
        "outputId": "1b9dcf73-7d8a-4dee-9e25-a5dc4c40b609"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mk-minchul/AdaFace.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0famaMAmbwU",
        "outputId": "c1d90db3-b1b9-4806-e723-4dbd01b137d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AdaFace' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRCyNkZpp0Mt",
        "outputId": "ccaef691-ede4-4fbc-85a8-5ca6f4a0c9a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/AdaFace')\n",
        "sys.path.append('/content/vgg_face2')"
      ],
      "metadata": {
        "id": "L4PGeCKOmwpO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import net\n",
        "import torch\n",
        "import os\n",
        "from face_alignment import align\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "adaface_models = {\n",
        "    'ir_50':\"/content/drive/MyDrive/Pre-trained models/adaface_ir50_ms1mv2.ckpt\",\n",
        "}\n",
        "\n",
        "def load_pretrained_model(architecture='ir_50'):\n",
        "    # load model and pretrained statedict\n",
        "    assert architecture in adaface_models.keys()\n",
        "    model = net.build_model(architecture)\n",
        "    statedict = torch.load(adaface_models[architecture])['state_dict']\n",
        "    model_statedict = {key[6:]:val for key, val in statedict.items() if key.startswith('model.')}\n",
        "    model.load_state_dict(model_statedict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def to_input(pil_rgb_image):\n",
        "    np_img = np.array(pil_rgb_image)\n",
        "    brg_img = ((np_img[:,:,::-1] / 255.) - 0.5) / 0.5\n",
        "    # tensor = torch.tensor([brg_img.transpose(2,0,1)]).float()\n",
        "    return torch.tensor(brg_img).float()\n"
      ],
      "metadata": {
        "id": "Isj8aJvrm4s5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = load_pretrained_model('ir_50')\n",
        "# feature, norm = model(torch.randn(2,3,112,112))\n",
        "\n",
        "# test_image_path = '/content/drive/MyDrive/Dataset/dataset/train'\n",
        "# all_features = []\n",
        "# for fname in sorted(os.listdir(test_image_path)):\n",
        "#     path = os.path.join(test_image_path, fname)\n",
        "#     features = []\n",
        "#     for img in sorted(os.listdir(path)):\n",
        "#       img_path = os.path.join(path, img)\n",
        "#       print(img_path)\n",
        "#       aligned_rgb_img = align.get_aligned_face(img_path)\n",
        "#       bgr_tensor_input = to_input(aligned_rgb_img)\n",
        "#       feature, _ = model(bgr_tensor_input)\n",
        "#       features.append(feature)\n",
        "#     all_features.append(features)"
      ],
      "metadata": {
        "id": "BiU9huxZm9gZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_pairs\n",
        "\n",
        "lfw_pairs_train = fetch_lfw_pairs(subset='train')"
      ],
      "metadata": {
        "id": "TX1AHRbHwaZg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lfw_pairs_train.pairs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUkhp7H6qCnl",
        "outputId": "227c7ac4-34d9-4d5f-892e-1780016be89d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2200, 2, 62, 47)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torchvision import transforms\n",
        "# from adafaces import AdaFace\n",
        "adafaces_model = load_pretrained_model('ir_50')\n",
        "\n",
        "# Load LFW dataset\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=70, color=True)\n",
        "X, y = lfw_people.images, lfw_people.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_embeddings = []\n",
        "X_test_embeddings = []\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  img = torch.Tensor(X_train[i])\n",
        "  img = img.permute(2,0,1).unsqueeze(0)\n",
        "  output_tensor = F.interpolate(img, size=(112, 112), mode='bilinear', align_corners=False)\n",
        "  # print(output_tensor.shape)\n",
        "  output_tensor = output_tensor.permute(0, 2, 3, 1)\n",
        "  # print(output_tensor.shape)\n",
        "  input_img = to_input(output_tensor).permute(0, 3, 1, 2)\n",
        "  img1, _ = adafaces_model(input_img)\n",
        "  # print(img1.view(-1).shape)\n",
        "  # print(len(img1.tolist()))\n",
        "  X_train_embeddings.append(img1.view(-1).tolist())\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "\n",
        "  img = torch.Tensor(X_train[i])\n",
        "  img = img.permute(2,0,1).unsqueeze(0)\n",
        "  output_tensor = F.interpolate(img, size=(112, 112), mode='bilinear', align_corners=False)\n",
        "  # print(output_tensor.shape)\n",
        "  output_tensor = output_tensor.permute(0, 2, 3, 1)\n",
        "  # print(output_tensor.shape)\n",
        "  input_img = to_input(output_tensor).permute(0, 3, 1, 2)\n",
        "  img1, _ = adafaces_model(input_img)\n",
        "\n",
        "  X_test_embeddings.append(img1.view(-1).tolist())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xr_0aUg-wrzq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "X_train_new = []\n",
        "X_test_new = []\n",
        "y_train_new = []\n",
        "y_test_new = []\n",
        "\n",
        "for i in range(0,len(X_train_embeddings)):\n",
        "  for j in range(i, len(X_train_embeddings)):\n",
        "    X_train_new.append((X_train_embeddings[i], X_train_embeddings[j]))\n",
        "    y_train_new.append(cosine_similarity(X_train_embeddings[i].reshape(1,-1), X_train_embeddings[j].reshape(1,-1)))\n",
        "\n",
        "\n",
        "for i in range(0,len(X_test_embeddings)):\n",
        "  for j in range(i, len(X_test_embeddings)):\n",
        "    X_test_new.append((X_test_embeddings[i], X_test_embeddings[j]))\n",
        "    y_test_new.append(cosine_similarity(X_test_embeddings[i].reshape(1,-1), X_test_embeddings[j].reshape(1,-1)))\n"
      ],
      "metadata": {
        "id": "11pq0y6r_viZ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Embeddings(Dataset):\n",
        "  def __init__(self, embeddings, similarity):\n",
        "    self.imgs = embeddings\n",
        "    self.similarity = similarity\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.similarity)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img1 = torch.FloatTensor(self.imgs[idx][0])\n",
        "    img2 = torch.FloatTensor(self.imgs[idx][1])\n",
        "    sim = torch.FloatTensor(self.similarity[idx])\n",
        "\n",
        "    return img1, img2, sim"
      ],
      "metadata": {
        "id": "JiaB3a5IBDiH"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Embeddings(X_train_new, y_train_new)\n",
        "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "\n",
        "test_data = Embeddings(X_test_new, y_test_new)\n",
        "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "IPDj47UcDiJk"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = set(y_train)\n",
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn0xybRRL1H4",
        "outputId": "bd0dcb27-e816-475c-8ac3-e079e1ceae7f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import MSELoss\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "# # Convert to PyTorch tensors\n",
        "# X_train_embeddings, X_test_embeddings = torch.tensor(X_train_embeddings).float(), torch.tensor(X_test_embeddings).float()\n",
        "# y_train, y_test = torch.tensor(y_train), torch.tensor(y_test)\n",
        "\n",
        "# Use AdaFace for face embedding\n",
        "\n",
        "class CompressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, compressed_dim):\n",
        "        super(CompressionModel, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, compressed_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # self.linear = nn.Linear(compressed_dim, 1030)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        # y = self.linear(encoded)\n",
        "        # y = F.softmax(y, dim = 0)\n",
        "        return encoded\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Loss, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, img1, img2, true_sim):\n",
        "        # Compute cosine similarity loss\n",
        "        sim_score = F.cosine_similarity(img1, img2)\n",
        "        # print(true_sim.view(-1).shape)\n",
        "        loss = MSELoss()(sim_score, true_sim.view(-1))\n",
        "        # print(loss)\n",
        "        return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "esBQ5TnN0rx3"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 512\n",
        "compressed_dim = 128\n",
        "model = CompressionModel(input_dim, compressed_dim)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = Loss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for img1, img2, sim_score in train_dataloader:\n",
        "      # print(X_train[i].unsqueeze(0).shape)\n",
        "\n",
        "      # print(X_train_embeddings[i].shape)\n",
        "      # Forward pass\n",
        "      compressed_img1 = model(img1)\n",
        "      compressed_img2 = model(img2)\n",
        "\n",
        "      loss = criterion(compressed_img1, compressed_img2, sim_score)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "        # for name, param in model.named_parameters():\n",
        "        #   if param.requires_grad:\n",
        "        #     print(name, param.grad)\n",
        "\n",
        "\n",
        "\n",
        "    # for name, param in model.named_parameters():\n",
        "    #   if param.requires_grad:\n",
        "    #     print(name, param.grad)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Yfn_sChFISL",
        "outputId": "cf112b1f-1cb1-46b7-c3a8-37c03b53bdbf"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.2518103592821378e-06\n",
            "Epoch [2/10], Loss: 3.008335028196682e-07\n",
            "Epoch [3/10], Loss: 2.3758208174146783e-07\n",
            "Epoch [4/10], Loss: 1.9882635706727422e-07\n",
            "Epoch [5/10], Loss: 1.7292707164004786e-07\n",
            "Epoch [6/10], Loss: 1.5902935630602438e-07\n",
            "Epoch [7/10], Loss: 1.4498030767020817e-07\n",
            "Epoch [8/10], Loss: 1.4571598843087646e-07\n",
            "Epoch [9/10], Loss: 1.5026597131038207e-07\n",
            "Epoch [10/10], Loss: 1.396859835799645e-07\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_score = []\n",
        "pred_score = []\n",
        "with torch.no_grad():\n",
        "  for img1, img2, sim_score in test_dataloader:\n",
        "    compressed_img1 = model(img1)\n",
        "    compressed_img2 = model(img2)\n",
        "    true_score.extend(sim_score.numpy())\n",
        "    pred_score.extend(F.cosine_similarity(compressed_img1, compressed_img2).numpy())\n",
        "\n",
        "\n",
        "true_score = [(x+1)/2 for x in true_score]\n",
        "pred_score = [(x+1)/2 for x in pred_score]\n"
      ],
      "metadata": {
        "id": "bOPo0DEnFMJF"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(true_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZEX4mzKUHJk",
        "outputId": "19a53288-7eb2-4dc5-b40b-5722980dc0f9"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = sum(x-y for x,y in zip(true_score,pred_score))\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwD8dQyjTy_p",
        "outputId": "e68363aa-97aa-4410-8bff-acb7698b447f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.00740111]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, _ = roc_curve(true_score, pred_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize =(8,6))\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.title('ROC curve')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "x49x2wlFRvqA",
        "outputId": "a35ba548-d4b6-4011-a53b-5eabe0dbc5de"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-ad6f6bbce275>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \"\"\"\n\u001b[0;32m--> 992\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: unknown format is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSBwZzI7SwON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}